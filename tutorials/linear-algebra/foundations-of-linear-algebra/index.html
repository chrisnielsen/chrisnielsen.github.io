<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>Chris Nielsen | Foundations of Linear Algebra</title>
  <meta name="description" content="This is the personal website of Chris Nielsen">

  <!-- Fonts and Icons -->
  <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons" />

  <!-- CSS Files -->
  <link rel="stylesheet" href="/assets/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/academicons.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/tutorials/linear-algebra/foundations-of-linear-algebra/">
</head>
<body>
  <!-- Header -->
  <nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">
      
        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Chris</span> Nielsen</a>
      
      <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/cv/">
                    Curriculum Vitae
                    
                  </a>
              </li>
            
          
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/music/">
                    Music
                    
                  </a>
              </li>
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/projects/">
                    Projects
                    
                  </a>
              </li>
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/publications/">
                    Publications
                    
                  </a>
              </li>
            
          
            
              <li class="nav-item ">
                  <a class="nav-link" href="/writing/">
                    Writing
                    
                  </a>
              </li>
            
          
            
          
        </ul>
      </div>
    </div>
  </nav>

  <!-- Scrolling Progress Bar -->
  <progress id="progress" value="0">
    <div class="progress-container">
      <span class="progress-bar"></span>
    </div>
  </progress>

  <!-- Content -->
  <div class="content">
    
  <h1>Foundations of Linear Algebra</h1>
  <h6></h6>


<p><br /></p>
<h3 id="table-of-contents"><strong>Table of Contents</strong></h3>
<!-- MarkdownTOC depth=4 -->
<ul>
  <li><a href="#introducing-vectors-and-matrices">Introducing Vectors and Matrices</a>
    <ul>
      <li><a href="#what-is-a-vector?">What is a Vector?</a></li>
      <li><a href="#what-is-a-matrix?">What is a Matrix?</a></li>
    </ul>
  </li>
  <li><a href="#important-definitions">Important Definitions</a>
    <ul>
      <li><a href="#important-theorems">Important Theorems</a></li>
      <li><a href="#rank">Rank</a></li>
      <li><a href="#reduced-row-echelon-form">Reduced Row Echelon Form</a></li>
      <li><a href="#linear-independence-and-basis">Linear Independence and Basis</a></li>
      <li><a href="#pseudoinverse">Pseudoinverse</a></li>
      <li><a href="#projections">Projections</a></li>
    </ul>
  </li>
  <li><a href="#four-fundamental-subspaces">Four Fundamental Subspaces</a>
    <ul>
      <li><a href="#finding-basis-vectors-for-the-four-fundamental-subspaces">Finding Basis Vectors for the Four Fundamental Subspaces</a></li>
    </ul>
  </li>
  <li><a href="#solving-the-equation-$$\boldsymbol{ax}-=-\boldsymbol{b}$$-">Solving the Equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> </a>
    <ul>
      <li><a href="#interpreting-the-product-$$\boldsymbol{ax}$$">Interpreting the Product <script type="math/tex">\boldsymbol{Ax}</script></a></li>
      <li><a href="#interpreting-the-equation-$$\boldsymbol{ax}-=-\boldsymbol{b}$$-">Interpreting the Equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> </a></li>
      <li><a href="#interpreting-the-matrix-product-$$\boldsymbol{a}-=-\boldsymbol{bc}$$-">Interpreting the Matrix Product <script type="math/tex">\boldsymbol{A} = \boldsymbol{BC}</script> </a></li>
      <li><a href="#interpreting-the-matrix-inverse-">Interpreting the Matrix Inverse </a></li>
      <li><a href="#when-can-we-solve-$$\boldsymbol{ax}-=-\boldsymbol{b}$$?">When can we solve <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>?</a></li>
      <li><a href="#how-can-we-solve-$$\boldsymbol{ax}-=-\boldsymbol{b}$$?">How can we solve <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>?</a></li>
      <li><a href="#least-squares">Least Squares</a></li>
    </ul>
  </li>
  <li><a href="#determinants">Determinants</a></li>
  <li><a href="#eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</a>
    <ul>
      <li><a href="#solving-difference-equations">Solving Difference Equations</a></li>
      <li><a href="#solving-differential-equations">Solving Differential Equations</a></li>
      <li><a href="#markov-chains">Markov Chains</a></li>
      <li><a href="#symmetric-matrices">Symmetric Matrices</a></li>
    </ul>
  </li>
  <li><a href="#special-matrices">Special Matrices</a>
    <ul>
      <li><a href="#positive-definite-matrices">Positive Definite Matrices</a></li>
      <li><a href="#gram-matrices">Gram Matrices</a></li>
      <li><a href="#similar-matrices">Similar Matrices</a></li>
      <li><a href="#orthogonal-matrices">Orthogonal Matrices</a></li>
      <li><a href="#graph-matrices">Graph Matrices</a></li>
    </ul>
  </li>
  <li><a href="#change-of-basis">Change of Basis</a>
    <ul>
      <li><a href="#fourier-basis">Fourier Basis</a></li>
    </ul>
  </li>
  <li><a href="#matrix-factorizations">Matrix Factorizations</a>
    <ul>
      <li><a href="#cr">CR</a></li>
      <li><a href="#lu">LU</a></li>
      <li><a href="#qr">QR</a></li>
      <li><a href="#svd">SVD</a>
<!-- /MarkdownTOC --></li>
    </ul>
  </li>
</ul>

<hr />
<p><br /></p>

<p>The purpose of this post is to encapsulate the foundational linear algebra knowledge that I gained from working through the 18.06 course on MIT OCW (located <a href="https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/index.htm">here</a>). Gilbert Strang is an amazing instructor and I highly recommend this course for anyone wanting to explore the depths of linear algebra. This post was written to serve as a reference for the fundamental concepts of linear algebra.</p>

<p><a name="introducing-vectors-and-matrices"></a></p>

<p><br /></p>

<hr />
<h2 id="introducing-vectors-and-matrices">Introducing Vectors and Matrices</h2>
<hr />

<p>In this section, we will discuss definitions for vectors and matrices which are the foundational mathematical objects used in linear algebra.</p>

<p><a name="what-is-a-vector?"></a></p>

<p><br /></p>

<hr />
<h4 id="what-is-a-vector">What is a Vector?</h4>
<hr />

<p>A source of confusion when learning about vectors is the abundance of differing definitions. For example, in high school physics, a vector is introduced as a quantity that has length and direction. While in computer science, a vector is defined as an ordered list of numbers.</p>

<p>So… what <em>really</em> is a vector? For the purpose of this post and our development of linear algebra, it is useful to define a vector in its most general form, namely: <strong>a vector is simply an element of a vector space</strong>.</p>

<p>A vector space is defined as a set of objects <script type="math/tex">V</script> (elements of <script type="math/tex">V</script> are called vectors), and a field <script type="math/tex">\boldsymbol{\mathbb{F}}</script> (elements of <script type="math/tex">\boldsymbol{\mathbb{F}}</script> are called scalars). In general, there are several axioms that must be satisfied for <script type="math/tex">V</script> and <script type="math/tex">\boldsymbol{\mathbb{F}}</script> to be considered a vector space. Intuitively, these axioms largely boil down to the requirement that any linear combination of vectors in <script type="math/tex">V</script> (using scalar coefficients from <script type="math/tex">\boldsymbol{\mathbb{F}}</script>) must also be in <script type="math/tex">V</script> (i.e. if <script type="math/tex">{\boldsymbol{v}_1},{\boldsymbol{v}_2} \in V</script>, then for <script type="math/tex">{c_1},{c_2} \in \boldsymbol{\mathbb{F}}</script>, we have <script type="math/tex">{c_1}{\boldsymbol{v}_1} + {c_2}{\boldsymbol{v}_2} \in V</script>).</p>

<p>The advantage of a vector space is that we can now generalize the concept of a vector beyond the specific definitions given in high school physics and computer science. For illustration, the following are examples of vector spaces:</p>

<ul>
  <li><strong>Real Coordinate Space <script type="math/tex">{\mathbb{R}^n}</script>:</strong>: The real coordinate space <script type="math/tex">{\mathbb{R}^n}</script> over the field of real numbers is probably the most common and widely used vector space, where the vectors are n-tuples of real numbers. For example, a vector in this space could be <script type="math/tex">\left[ {\begin{array}{*{20}{c}}1\\2\\3\end{array}} \right] \in {\mathbb{R}^3}</script>. In computer science, when we think about a “vector” as an ordered list of real numbers, we are really thinking about a coordinate vector in <script type="math/tex">{\mathbb{R}^n}</script>.</li>
  <li><strong>Complex Matrices <script type="math/tex">{\mathbb{C}^{m \times n}}</script>:</strong> The vector space <script type="math/tex">{\mathbb{C}^{m \times n}}</script> over the field of complex numbers consists of all <script type="math/tex">m \times n</script> matrices that have complex values. Notice how for this vector space, the vectors are actually matrices. For example, a vector in this space could be <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}i&2\\{3i}&{4i + 2}\end{array}} \right] \in {\mathbb{C}^{2 \times 2}} %]]></script>.</li>
  <li><strong>Polynomials <script type="math/tex">{P_n}</script>:</strong> The vector space <script type="math/tex">{P_n}</script> over the field of real numbers consists of all polynomials of degree <script type="math/tex">n</script> and smaller. For example, a vector in this space could be <script type="math/tex">{x^3} + 2x + 5 \in {P_3}</script>.</li>
  <li><strong>Solutions to Linear Homogeneous Differential Equations:</strong> Suppose that <script type="math/tex">{f_1},{f_2}</script> are distinct solutions to the equation <script type="math/tex">f'' = f</script>, such that <script type="math/tex">{f''_1} = {f_1}</script> and <script type="math/tex">{f''_2} = {f_2}</script>. Due to the linearity of the differentiation operator, it follows that any linear combination of <script type="math/tex">{f_1},{f_2}</script> will also be a solution to <script type="math/tex">f'' = f</script>. To see this, we note that <script type="math/tex">{\left( {{c_1}{f_1} + {c_2}{f_2}} \right)^{\prime \prime }} = {c_1}{f_1}^{\prime \prime } + {c_2}{f_2}^{\prime \prime } = {c_1}{f_1} + {c_2}{f_2}</script>. Therefore, the solutions to the differential equation <script type="math/tex">f'' = f</script> form a vector space over the field of real numbers and the vectors in this space are functions. For example, a vector in this space could be <script type="math/tex">f\left( t \right) = \sin \left( t \right)</script>.</li>
  <li><strong>Column Space of a Matrix in <script type="math/tex">{\mathbb{R}^{m \times n}}</script>:</strong> The column space of a matrix in <script type="math/tex">{\mathbb{R}^{m \times n}}</script> over the field of real numbers is defined by the set of all linear combinations of the columns in the matrix. Since the column vectors are in <script type="math/tex">{\mathbb{R}^m}</script>, it follows that the column space will be a subspace of <script type="math/tex">{\mathbb{R}^m}</script>, where the subspace dimension is equal to the number of independent columns in the matrix. The elements of this vector space are vectors in <script type="math/tex">{\mathbb{R}^m}</script>. For example, a vector in the column space of <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}1&3\\2&2\end{array}} \right] %]]></script> could be <script type="math/tex">4\left[ {\begin{array}{*{20}{c}}1\\2\end{array}} \right] + 2\left[ {\begin{array}{*{20}{c}}3\\2\end{array}} \right]</script>.</li>
</ul>

<p>To summarize, a vector space is defined by a set of vectors <script type="math/tex">V</script> and a field <script type="math/tex">\boldsymbol{\mathbb{F}}</script>. For <script type="math/tex">\left( {V,\boldsymbol{\mathbb{F}}} \right)</script> to be a valid vector space, it must be closed under linear combination. This means that if <script type="math/tex">{\boldsymbol{v}_1},{\boldsymbol{v}_2} \in V</script>, then for <script type="math/tex">{c_1},{c_2} \in \boldsymbol{\mathbb{F}}</script>, we have <script type="math/tex">{c_1}{\boldsymbol{v}_1} + {c_2}{\boldsymbol{v}_2} \in V</script>. You can check that this result holds for the example vector spaces shown above. Thus, our answer to “what is a vector” is simply this: a vector is an element of a vector space.</p>

<p>It is important to note that the most general vector space does not include a notion of distance. To include a metric of distance, we must upgrade our general vector space into a normed vector space. If we go further still, and define an inner product on our space to measure how orthogonal two vectors are (e.g. the inner product  for <script type="math/tex">{\mathbb{R}^n}</script> is the dot product), then we upgrade our normed vector space into an inner product space. A great resource to see the relationships between mathematical spaces is <a href="https://en.wikipedia.org/wiki/Space_(mathematics)">here</a>. For this post, we will be using the term <em>vector space</em> quite loosely and in most cases will assume that the vector space is an inner product space (i.e. has norm and inner product defined for the space).</p>

<p>The advantage of working with the generalized concept of a vector space is that any results proven for the general case can be applied equally well to any vector space regardless of whether it is composed vectors that are n-tuples, matrices, polynomials, or functions. If we can show that our data forms a valid vector space, then we can leverage the mathematics developed on generalized vector spaces to analyze our data. For example, consider the natural language processing application of determining word embedding vectors. For this task, it is assumed that words in a language form a vector space such that more similar words are closer in space, and the relationship between words can be represented by the additive properties of the vector space (e.g. king – man + woman = queen).  The goal of word embedding models such as word2vec is to learn a representation of words as coordinate vectors in <script type="math/tex">{\mathbb{R}^n}</script>. Once we have a coordinate vector for each word, we can apply the properties of the <script type="math/tex">{\mathbb{R}^n}</script> vector space to geometrically visualize words as coordinates in space so that we can calculate distances between words, linear combinations of words, and the inner product between words.</p>

<p>The real power of linear algebra comes from being able to use it to computationally solve equations and process data on a computer. Since computers work with discrete numbers in memory, this raises a challenge when working with vector spaces whose vectors are objects such as continuous functions. For example, suppose that we are trying to solve the equation <script type="math/tex">{x_1}\left( {{t^2} + 4t + 1} \right) + {x_2}\left( {2{t^2} + 5t + 2} \right) + {x_3}\left( {4{t^2} + 6t + 3} \right) = 3{t^2} + 2t + 1</script>. In this case, we are trying to find whether a vector in <script type="math/tex">{P_2}</script> can be written as a linear combination of three other vectors from <script type="math/tex">{P_2}</script>. To solve this problem on a computer, we must first answer an important question: how can we represent a polynomial vector from <script type="math/tex">{P_2}</script>  (e.g. <script type="math/tex">{t^2} + 4t + 1</script>) on a computer? This brings us to one of the most powerful attributes of vector spaces. Recall that any linear combination of vectors in a vector space is also contained in the vector space. Intuitively it makes sense that we should be able to have a minimal set of vectors whose linear combinations can describe every other vector in the space. As an informal proof of this, consider a set containing all the vectors in the vector space. Now follow an iterative pattern of: 1) taking each vector and checking whether it can be written as a linear combination of the other vectors in the set, 2) if a linear combination exists, then remove the vector from the set. If we continue this process, eventually we will get down to a minimal set of vectors whose linear combinations describe all vectors in the space. This minimal set is called a basis for the space, each vector in the basis is called a basis vector, and the number of vectors in the basis is called the dimension of the vector space. For example, a basis for <script type="math/tex">{P_2}</script> could be the set <script type="math/tex">\left\{ {1,t,{t^2}} \right\}</script> which has dimension 3 and whose linear combinations could be used to describe any polynomial of degree 2 and smaller. Suppose that we want to represent the polynomial <script type="math/tex">{t^2} + 4t + 1</script> using this basis. We can write down the corresponding coefficients as an ordered list of numbers <script type="math/tex">\left[ {\begin{array}{*{20}{c}}1\\4\\1\end{array}} \right]</script>, such that we can express the polynomial as a linear combination of the basis vectors <script type="math/tex">1 * {t^2} + 4 * t + 1 * 1 = {t^2} + 4t + 1</script>. Hence, to represent the polynomial vector <script type="math/tex">{t^2} + 4t + 1</script>, we only need to know the corresponding coefficients of the basis vectors. This is critical because we can represent this list of coefficients easily on a computer. Therefore, instead of working directly with the polynomial vector, we can work with the coefficients defined for the basis we choose. Furthermore, notice that this ordered list of coefficients is itself a vector in <script type="math/tex">{\mathbb{R}^3}</script>, since  <script type="math/tex">\left[ {\begin{array}{*{20}{c}}1\\4\\1\end{array}} \right] \in {\mathbb{R}^3}</script>.</p>

<p>In general, we can describe any vector in a vector space as a linear combination of basis vectors. Therefore, for a given basis, each vector in the space is uniquely identified by a coordinate vector whose elements are the basis vector coefficients in the linear combination. Hence, we can use coordinate vectors to represent any general vector (e.g. function, polynomial, etc.) on a computer and apply linear algebra operations to these coordinate vectors.</p>

<p>In summary, to use vectors for computational linear algebra we can think about the following steps:</p>
<ol>
  <li>Define a vector space that represents the objects we are analyzing (e.g. tabular data as coordinates, polynomials, differentiable functions, language words, matrices, etc.)</li>
  <li>Determine an appropriate basis, such that all basis vectors are independent (i.e. a minimal set), and every vector in the space can be written as a linear combination of the basis vectors</li>
  <li>Perform a “coordinate transform” where we represent vectors in the space using coordinate vectors whose elements are the linear combination coefficients of the basis vectors (typically the coordinate vectors are in <script type="math/tex">{\mathbb{R}^n}</script>)</li>
  <li>Perform computational linear algebra procedures using the coordinate vector representation such as solving linear equations, change of basis (e.g. Fourier transform), or linear transformations</li>
</ol>

<p><a name="what-is-a-matrix?"></a></p>

<p><br /></p>

<hr />
<h4 id="what-is-a-matrix">What is a Matrix?</h4>
<hr />

<p>In its most general form, a matrix is simply a rectangular array of mathematical objects such as numbers or symbols. Depending on the context, matrices can represent different things, just like how the number “3” can mean different things in different contexts. The strength of matrices is that we can use results from matrix theory to utilize the structure of the matrix to help solve the specific problem at hand (e.g. solving a system of equations). Such techniques from matrix theory include eigenvectors/eigenvalues, different factorizations (QR, SVD, LR, etc.), positive definiteness, four fundamental subspaces, etc. Once we pose our problem in matrix form, we gain access to these powerful analytical tools from matrix theory to help understand and manipulate the structure of our data. Some examples of the data that matrices represent includes the following:</p>

<ul>
  <li><strong>Linear Equation Coefficients:</strong> Suppose that we have a system of linear equations. We can write coefficients of the equations in a matrix <script type="math/tex">\boldsymbol{A}</script> which enables us to write the entire system as <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>. When thinking about systems of linear equations, it is especially useful to interpret the matrix in terms of its four fundamental subspaces.</li>
  <li><strong>Linear Transformations:</strong> Suppose we have a linear transformation <script type="math/tex">f:V \to W</script>from a vector space <script type="math/tex">V</script> to a vector space <script type="math/tex">W</script>. If <script type="math/tex">\left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script> is a basis for <script type="math/tex">V</script>, then every vector <script type="math/tex">\boldsymbol{v} \in V</script> is uniquely determined by the coordinates <script type="math/tex">{c_1}, \ldots ,{c_n}</script> in <script type="math/tex">{\mathbb{R}^n}</script> such that <script type="math/tex">\boldsymbol{v} = {c_1}{\boldsymbol{v}_1} +  \cdots {c_n}{\boldsymbol{v}_n}</script>. Due to the linearity of <script type="math/tex">f</script>, we can write <script type="math/tex">f\left( \boldsymbol{v} \right) = f\left( {{c_1}{\boldsymbol{v}_1} +  \cdots {c_n}{\boldsymbol{v}_n}} \right) = {c_1}f\left( {{\boldsymbol{v}_1}} \right) +  \cdots  + {c_n}f\left( {{\boldsymbol{v}_n}} \right)</script>. This implies that the function <script type="math/tex">f</script> is completely determined by the transformed basis vectors <script type="math/tex">f\left( {{\boldsymbol{v}_1}} \right), \ldots ,f\left( {{\boldsymbol{v}_n}} \right)</script>. Now let <script type="math/tex">\left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_m}} \right\}</script> be a basis for <script type="math/tex">W</script>. Since<script type="math/tex">f\left( {{\boldsymbol{v}_j}} \right) \in W</script>, we can write each transformed vector in terms of the basis vectors of <script type="math/tex">W</script> as <script type="math/tex">f\left( {{\boldsymbol{v}_j}} \right) = {a_{1,j}}{\boldsymbol{w}_1} +  \cdots {a_{m,j}}{\boldsymbol{w}_m}</script>. Hence, the function <script type="math/tex">f</script> is completely determined by the coordinates <script type="math/tex">{a_{i,j}}</script>. We can arrange these coordinates in a matrix such that<script type="math/tex">% <![CDATA[
\boldsymbol{A} = \left[ {\begin{array}{*{20}{c}}{{a_{1,1}}}& \cdots &{{a_{n,1}}}\\ \vdots & \ddots & \vdots \\{{a_{m,1}}}& \cdots &{{a_{m,n}}}\end{array}} \right] %]]></script>. Therefore, the matrix <script type="math/tex">\boldsymbol{A}</script> completely determines the transformation <script type="math/tex">f:V \to W</script> using <script type="math/tex">\left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script> and <script type="math/tex">\left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_m}} \right\}</script> as basis. <strong>Important insight:</strong> we can interpret each column of a general matrix <script type="math/tex">\boldsymbol{A}</script> as expressing the coordinates for a transformed input basis vector in terms of the output basis vectors. Therefore, for the transformation <script type="math/tex">\boldsymbol{w = Av}</script>, we can interpret <script type="math/tex">\boldsymbol{v}</script> as representing an input coordinate vector relative to the basis <script type="math/tex">\left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script>, and <script type="math/tex">\boldsymbol{w}</script> as representing an output transformed coordinate vector relative to the basis <script type="math/tex">\left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_m}} \right\}</script>. The choice of basis for <script type="math/tex">V</script> and <script type="math/tex">W</script> is arbitrary but will change the matrix used to represent the linear transformation. For example, the <script type="math/tex">{k^{th}}</script> column of <script type="math/tex">\boldsymbol{A}</script> contains the coordinates <script type="math/tex">\left[ {\begin{array}{*{20}{c}}{{a_{1,k}}}\\ \vdots \\{{a_{m,k}}}\end{array}} \right]</script> which is the transformed input basis vector <script type="math/tex">f\left( {{\boldsymbol{v}_k}} \right)</script> written as a coordinate vector in terms of the output basis vectors as <script type="math/tex">f\left( {{\boldsymbol{v}_k}} \right) = {a_{1,k}}{\boldsymbol{w}_1} +  \cdots {a_{m,k}}{\boldsymbol{w}_m}</script>. For example, let us compute the linear transformation <script type="math/tex">% <![CDATA[
\boldsymbol{Ax} = \left[ {\begin{array}{*{20}{c}}1&3\\2&3\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{{x_1}}\\{{x_2}}\end{array}} \right] %]]></script> where <script type="math/tex">\left[ {\begin{array}{*{20}{c}}{{x_1}}\\{{x_2}}\end{array}} \right]</script> is a coordinate vector relative to the input basis. To visualize: 1) start by drawing a 2D Cartesian coordinate plane, 2) plot the columns of <script type="math/tex">\boldsymbol{A}</script> as coordinate vectors in this plane (these are the coordinates of the input basis vectors written in terms of the output basis vectors), 3) plot the linear combination of these vectors where <script type="math/tex">\left[ {\begin{array}{*{20}{c}}{{x_1}}\\{{x_2}}\end{array}} \right]</script> are the scalar coefficients to get the transformed output as a coordinate vector relative to the output basis.</li>
  <li><strong>Change of Basis:</strong> Suppose that we have a basis for our vector space <script type="math/tex">{B_{old}} = \left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script>. Now suppose that we have a vector <script type="math/tex">\boldsymbol{v} = {c_1}{\boldsymbol{w}_1} +  \cdots {c_n}{\boldsymbol{w}_n}</script> that has been written in terms of a new basis <script type="math/tex">{B_{new}} = \left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_n}} \right\}</script>. How can we write the coordinates for <script type="math/tex">\boldsymbol{v}</script> in terms of our old basis <script type="math/tex">{B_{old}} = \left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script>? Suppose that we can write each new basis vector in terms of the old basis vectors such that <script type="math/tex">{\boldsymbol{w}_j} = {a_{1,j}}{\boldsymbol{v}_1} +  \cdots {a_{n,j}}{\boldsymbol{v}_n}</script>. Then we can expand the vector as <script type="math/tex">\boldsymbol{v} = {c_1}\left( {{a_{1,1}}{\boldsymbol{v}_1} +  \cdots {a_{n,1}}{\boldsymbol{v}_n}} \right) +  \cdots {c_n}\left( {{a_{1,n}}{\boldsymbol{v}_1} +  \cdots {a_{n,n}}{\boldsymbol{v}_n}} \right)</script>. Rearranging, we can put the entries <script type="math/tex">{a_{i,j}}</script> into a matrix <script type="math/tex">\boldsymbol{A}</script> to write<script type="math/tex">% <![CDATA[
\boldsymbol{A}\left[ {\begin{array}{*{20}{c}}{{c_1}}\\ \vdots \\{{c_n}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}{{a_{1,1}}}& \cdots &{{a_{n,1}}}\\ \vdots & \ddots & \vdots \\{{a_{n,1}}}& \cdots &{{a_{n,n}}}\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{{c_1}}\\ \vdots \\{{c_n}}\end{array}} \right] %]]></script>. Therefore, the matrix <script type="math/tex">\boldsymbol{A}</script> takes a coordinate vector relative to the basis <script type="math/tex">{B_{new}} = \left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_n}} \right\}</script> and produces a new coordinate vector relative to the original basis <script type="math/tex">{B_{old}} = \left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script>. Each column of <script type="math/tex">\boldsymbol{A}</script> provides the coordinates for each basis vector in <script type="math/tex">{B_{new}} = \left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_n}} \right\}</script> written using the old basis vectors <script type="math/tex">{B_{old}} = \left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script>. Note that to represent a change of basis, <script type="math/tex">\boldsymbol{A}</script> must be a square invertible matrix. Therefore, we see that the matrix <script type="math/tex">\boldsymbol{A}</script> will change the basis of a coordinate vector relative to the “new” basis into a coordinate vector relative to the “old” basis. Likewise, the inverse <script type="math/tex">{\boldsymbol{A}^{ - 1}}</script> will change the basis of a coordinate vector relative to the “old” basis into a coordinate vector relative to the “new” basis. More generally, suppose that we have a linear transformation <script type="math/tex">\boldsymbol{T}:V \to W</script> which maps from a vector space <script type="math/tex">V</script> to a vector space <script type="math/tex">W</script> using <script type="math/tex">\left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script> and <script type="math/tex">\left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_m}} \right\}</script> as basis for <script type="math/tex">V</script> and <script type="math/tex">W</script> respectively. Suppose that would like to represent the input and output coordinate vectors relative to other basis <script type="math/tex">\left\{ {\boldsymbol{v}_1^*, \ldots ,\boldsymbol{v}_n^*} \right\}</script> and <script type="math/tex">\left\{ {\boldsymbol{w}_1^*, \ldots ,\boldsymbol{w}_m^*} \right\}</script>. We can achieve this by first utilizing two change of basis matrices <script type="math/tex">{\boldsymbol{P}_{{\boldsymbol{v}^*} \to \boldsymbol{v}}}</script> and <script type="math/tex">{\boldsymbol{Q}_{\boldsymbol{w} \to {\boldsymbol{w}^*}}}</script>, where <script type="math/tex">{\boldsymbol{P}_{{\boldsymbol{v}^*} \to \boldsymbol{v}}}</script> performs a change of basis from <script type="math/tex">\left\{ {\boldsymbol{v}_1^*, \ldots ,\boldsymbol{v}_n^*} \right\}</script> to <script type="math/tex">\left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script>, and <script type="math/tex">{\boldsymbol{Q}_{\boldsymbol{w} \to {\boldsymbol{w}^*}}}</script> performs a change of basis from <script type="math/tex">\left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_m}} \right\}</script> to <script type="math/tex">\left\{ {\boldsymbol{w}_1^*, \ldots ,\boldsymbol{w}_m^*} \right\}</script>. Then a coordinate vector <script type="math/tex">\boldsymbol{x}</script> written relative to <script type="math/tex">\left\{ {\boldsymbol{v}_1^*, \ldots ,\boldsymbol{v}_n^*} \right\}</script> can be transformed into a coordinate vector <script type="math/tex">\boldsymbol{y}</script> written relative to <script type="math/tex">\left\{ {\boldsymbol{w}_1^*, \ldots ,\boldsymbol{w}_m^*} \right\}</script> by the following <script type="math/tex">\boldsymbol{y = }{\boldsymbol{Q}_{\boldsymbol{w} \to {\boldsymbol{w}^*}}}\boldsymbol{T}{\boldsymbol{P}_{{\boldsymbol{v}^*} \to \boldsymbol{v}}}\boldsymbol{x}</script>.</li>
  <li><strong>Storing Data:</strong> We can use a matrix to store data such as pixel data from images. Matrix factorization methods can be used to perform dimensionality reduction.</li>
  <li><strong>Difference Equations</strong> For some classes of difference equations and recurrence equations, the state transitions between steps can be represented using a matrix. General solutions to such equations can be found through eigenvalue analysis.</li>
  <li><strong>Differential Equations:</strong> The relationship between the derivatives in some linear differential equations can be represented using a matrix. Eigenvalue analysis can be used to find solutions to these equations.</li>
  <li><strong>Markov Chains:</strong> The state transition probabilities can be represented in a matrix. Eigenvalue analysis can be used to identify the long-term state probabilities</li>
  <li><strong>Graphs:</strong> We can encode the structure of a graph in an adjacency matrix.</li>
</ul>

<p>Now that we have seen some of the contexts in which matrices are used, an important question arises: how should we visualize the action of a matrix? Here are some principals to guide our intuition:</p>
<ul>
  <li><strong>SVD:</strong> One of the most informative ways to visualize the action of a matrix is to consider the singular value decomposition (SVD) theorem. This theorem states that any matrix can be written as a sequence of three operations: 1) rotation, 2) scaling, 3) rotation. Hence, we can interpret any linear transformation represented by a matrix as a sequence of these three composite transformations. For example, consider the transformation of a hypercube. The action of any matrix will first rotate the hypercube, then scale the hypercube which produces a stretched parallelotope, and then rotate the stretched parallelotope. Similarly, the action of any matrix on a hypersphere will first rotate the hypersphere, then scale it which will produce an ellipsoid, then rotate it again, producing a rotated (and potentially flattened) ellipsoid.</li>
  <li><strong>Determinants:</strong> The determinant of a matrix tells us how much the linear transformation represented by a matrix changes volumes. As shown above with the SVD, the action of any matrix will take a hypersphere and produce a rotated ellipsoid. The ratio of the volume of the resulting ellipsoid to the original hypersphere is given by the determinant. Note that if the determinant is zero, it means that at least one of the dimensions of the ellipsoid has been collapsed. The sign of the determinant tells us whether the transformation preserves or reverses orientation.</li>
  <li><strong>Four Fundamental Subspaces:</strong> Every <script type="math/tex">m\,\, \times \,\,n</script> matrix <script type="math/tex">\boldsymbol{A}</script> has four fundamental subspaces: 1) the column space defined as the subspace of <script type="math/tex">{\mathbb{R}^m}</script> spanned by the columns of the matrix, 2) the row space defined as the subspace of <script type="math/tex">{\mathbb{R}^n}</script> spanned by the rows of the matrix, 3) the nullspace defined as the subspace of <script type="math/tex">{\mathbb{R}^n}</script> orthogonal to the row space, and 4) the left nullspace defined as the subspace of <script type="math/tex">{\mathbb{R}^m}</script> orthogonal to the column space. Hence the combination of the column space and left nullspace is the complete <script type="math/tex">{\mathbb{R}^m}</script> and the combination of the row space and the nullspace is the complete <script type="math/tex">{\mathbb{R}^n}</script>. We can visualize each of these subspaces as being a linear manifold inside their respective parent space.</li>
  <li><strong>Linear Transformations:</strong> See the section on linear transformations described above for more detail. Essentially, we can interpret each column of a general matrix <script type="math/tex">\boldsymbol{A}</script> as expressing the coordinates for a transformed input basis vector in terms of the output basis vectors. Therefore, for the transformation <script type="math/tex">\boldsymbol{w = Av}</script>, we can interpret <script type="math/tex">\boldsymbol{v}</script> as representing an input coordinate vector relative to the basis <script type="math/tex">\left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script>, and <script type="math/tex">\boldsymbol{w}</script> as representing an output transformed coordinate vector relative to the basis <script type="math/tex">\left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_m}} \right\}</script>. The choice of basis for <script type="math/tex">V</script> and <script type="math/tex">W</script> is arbitrary but will change the matrix used to represent the linear transformation.</li>
</ul>

<p><a name="important-definitions"></a></p>

<p><br /></p>

<hr />
<h2 id="important-definitions">Important Definitions</h2>
<hr />

<p>In this section, we will examine the definitions of several important concepts in linear algebra.</p>

<p><a name="important-theorems"></a></p>

<p><br /></p>

<hr />
<h4 id="important-theorems">Important Theorems</h4>
<hr />

<p>Gilbert Strang proposes that there are six great theorems of linear algebra (as seen <a href="https://math.mit.edu/~gs/linearalgebra/linearalgebra5_6Great.pdf">here</a>). Some of these theorems are difficult to prove, so I will list the theorems without proof:</p>
<ul>
  <li><strong>Dimension Theorem:</strong> All bases for a vector space have the same number of vectors.</li>
  <li><strong>Counting Theorem:</strong> Dimension of column space + dimension of nullspace = number of columns.</li>
  <li><strong>Rank Theorem:</strong> Dimension of column space = dimension of row space. This is the rank.</li>
  <li><strong>Fundamental Theorem:</strong> The row space and nullspace of an <script type="math/tex">m\,\, \times \,\,n</script> matrix <script type="math/tex">\boldsymbol{A}</script> are orthogonal complements in <script type="math/tex">{\mathbb{R}^n}</script>.</li>
  <li><strong>SVD:</strong> There are orthonormal basis (<script type="math/tex">{\boldsymbol{u}_1}, \ldots ,{\boldsymbol{u}_r}</script> for the column space and <script type="math/tex">{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_r}</script> for the row space) such that <script type="math/tex">\boldsymbol{A}{\boldsymbol{v}_i} = {\sigma _i}{\boldsymbol{u}_i}</script>.</li>
  <li><strong>Spectral Theorem:</strong> If <script type="math/tex">{\boldsymbol{A}^T} = \boldsymbol{A}</script> then there are orthonormal <script type="math/tex">{\boldsymbol{q}_1}, \ldots ,{\boldsymbol{q}_n}</script> so that <script type="math/tex">\boldsymbol{A}{\boldsymbol{q}_i} = {\lambda _i}{\boldsymbol{q}_i}</script> and <script type="math/tex">\boldsymbol{A} = \boldsymbol{Q}\Lambda {\boldsymbol{Q}^T}</script>.</li>
</ul>

<p><a name="rank"></a></p>

<p><br /></p>

<hr />
<h4 id="rank">Rank</h4>
<hr />

<p>The rank of a matrix <script type="math/tex">\boldsymbol{A}</script> is defined as the number of linearly independent columns of <script type="math/tex">\boldsymbol{A}</script>. In other words, the rank is the dimension of the column space of <script type="math/tex">\boldsymbol{A}</script>. To compute the rank of a matrix <script type="math/tex">\boldsymbol{A}</script>, an effective strategy is to first transform <script type="math/tex">\boldsymbol{A}</script> into its reduced row echelon form <script type="math/tex">\boldsymbol{R}</script> and determine the number of pivot columns which corresponds to the rank. An interesting and nonobvious fact is that the column rank (number of independent columns) equals the row rank (number of independent rows) for any matrix <script type="math/tex">\boldsymbol{A}</script>.</p>

<p>How can we show that the column rank equals the row rank for any matrix? Here are three different ways to see this:</p>

<ol>
  <li><strong>Simplest Proof:</strong> The simplest proof to show that the row rank is equal to the column rank is the following. Suppose that we have any <script type="math/tex">m</script> by <script type="math/tex">n</script> matrix <script type="math/tex">\boldsymbol{A}</script>, with column rank <script type="math/tex">c</script> and row rank <script type="math/tex">r</script>. The fact that the column rank equals <script type="math/tex">c</script> tells us that we can describe the <script type="math/tex">n</script> columns of <script type="math/tex">\boldsymbol{A}</script> using linear combinations of <script type="math/tex">c</script> vectors. If we put the <script type="math/tex">c</script> vectors in an <script type="math/tex">m</script> by <script type="math/tex">c</script> matrix <script type="math/tex">\boldsymbol{C}</script>, and put the corresponding combination coefficients in a  <script type="math/tex">c</script> by <script type="math/tex">n</script> matrix <script type="math/tex">\boldsymbol{R}</script>, then we can write <script type="math/tex">\boldsymbol{A}</script> as <script type="math/tex">\boldsymbol{A = CR}</script>. The key insight for this proof is the fact that we can interpret the matrix multiplication <script type="math/tex">\boldsymbol{CR}</script> as a linear combination of the columns of <script type="math/tex">\boldsymbol{C}</script>, OR we can interpret the matrix multiplication as a linear combination of the rows <script type="math/tex">\boldsymbol{R}</script>. Therefore, since <script type="math/tex">\boldsymbol{R}</script> is a <script type="math/tex">c</script> by <script type="math/tex">n</script> matrix, it follows that the rows of <script type="math/tex">\boldsymbol{A}</script> can be written as a linear combination of the rows in <script type="math/tex">\boldsymbol{R}</script>. Hence, the row rank of <script type="math/tex">\boldsymbol{A}</script> must be less than or equal to the number of rows in <script type="math/tex">\boldsymbol{R}</script>, such that <script type="math/tex">r \le c</script>. We can repeat this process in the reverse order using the fact that the row rank equals <script type="math/tex">r</script> tells us that we can describe the <script type="math/tex">m</script> rows of <script type="math/tex">\boldsymbol{A}</script> using linear combinations of <script type="math/tex">r</script> vectors. We can put these <script type="math/tex">r</script> vectors in the rows of <script type="math/tex">\boldsymbol{R}</script> and repeat the proof process described above to show that <script type="math/tex">c \le r</script>. Therefore, we can equate these expressions to show that <script type="math/tex">r = c</script>.</li>
  <li><strong>Information Theory Interpretation:</strong> Another intuitive “hand wavy” interpretation of why the dimensionality of the row space equals the column space can be framed in terms of information capacity. In this way, the rank tells you the information capacity of the channels (i.e. row space or column space) provided by the matrix. Simply put, if I give you an ordered list of rows in a matrix, you have all the information necessary to tell me what the columns are (here we are implicitly using the relationship between the rows and columns imposed by the definition of a matrix). Therefore, the columns are completely described by the information contained in the rows. Therefore, the column rank should be less than or equal to the row rank. Similarly, if I give you an ordered list of columns in a matrix, you have all the information necessary to tell me what the rows are. Therefore, the rows are completely described by the information contained in the columns. Therefore, the row rank should be less than or equal to the column rank. Hence, the column rank should equal the row rank. Continuing with this information theory interpretation, suppose that we have an <script type="math/tex">m</script> by <script type="math/tex">n</script> matrix <script type="math/tex">\boldsymbol{A}</script>, where <script type="math/tex">m > n</script>. In this case, the vectors in the column space are in <script type="math/tex">{\mathbb{R}^m}</script> which have higher dimensionality than the vectors in the row space which are in <script type="math/tex">{\mathbb{R}^n}</script>. How is it possible for the row space and the column space to have the same information capacity if the vectors in their respective spaces have different dimensions? The key answer to this question is that the information capacity of the row space and column space depend on the dimensionality of the subspace, not the parent space. Hence, even though  <script type="math/tex">m > n</script>, the subspace dimensionality for the row space and column space are equal. This leads to a key insight into vector spaces, the information capacity of the space does not depend on whether the actual vectors in the space are in <script type="math/tex">{\mathbb{R}^2}</script>, <script type="math/tex">{\mathbb{R}^{100}}</script>, or some other abstract vector quantity. The information capacity depends on the subspace dimensionality. However, with this said, the subspace dimensionality has an upper bound described by the dimensionality of the parent space. Hence, a subspace of <script type="math/tex">{\mathbb{R}^2}</script> can have a maximum dimension of 2, while a subspace of <script type="math/tex">{\mathbb{R}^{100}}</script> can have a maximum dimension of 100.</li>
  <li><strong>Specific 2 by 2 Example:</strong> Suppose we ask the question, is it possible for a matrix to have more linearly independent columns than rows. Let us take the example of a 2 by 2 matrix <script type="math/tex">% <![CDATA[
\boldsymbol{A} = \left[ {\begin{array}{*{20}{c}}a&b\\c&d\end{array}} \right] %]]></script>. Suppose that there exist scalars such that <script type="math/tex">{x_1}\left[ {\begin{array}{*{20}{c}}a\\c\end{array}} \right] + {x_2}\left[ {\begin{array}{*{20}{c}}b\\d\end{array}} \right] = 0</script>. Solving for these scalers gives <script type="math/tex">{x_2}\left( {\frac{{ad - bc}}{a}} \right) = 0</script> which implies that <script type="math/tex">ad - bc = 0</script>. Now turning our attention to the rows, the question we are asking is whether we can find <script type="math/tex">{y_1},\,{y_2}</script> such that <script type="math/tex">% <![CDATA[
{y_1}\left[ {\begin{array}{*{20}{c}}a&b\end{array}} \right] + {y_2}\left[ {\begin{array}{*{20}{c}}c&d\end{array}} \right] = 0 %]]></script>. We find that setting <script type="math/tex">{y_1} = d</script> and <script type="math/tex">{y_2} =  - b</script> will solve this equation. Hence, if the columns are not linearly independent, then the rows cannot be linearly independent.</li>
</ol>

<p><a name="reduced-row-echelon-form"></a></p>

<p><br /></p>

<hr />
<h4 id="reduced-row-echelon-form">Reduced Row Echelon Form</h4>
<hr />

<p>For any <script type="math/tex">m</script> by <script type="math/tex">n</script> matrix <script type="math/tex">\boldsymbol{A}</script> with rank <script type="math/tex">r</script>, we can transform <script type="math/tex">\boldsymbol{A}</script> into what is called the <em>reduced row echelon form</em> such that <script type="math/tex">{\boldsymbol{E}_K} \cdots {\boldsymbol{E}_1}\boldsymbol{A}{\boldsymbol{Q}_1} \cdots {\boldsymbol{Q}_L}\boldsymbol{ = R}</script>, where <script type="math/tex">{\boldsymbol{E}_K} \cdots {\boldsymbol{E}_1}</script> represent a series of invertible elementary row operations (i.e. swap rows, scale rows, add multiple of rows), and <script type="math/tex">{\boldsymbol{Q}_1} \cdots {\boldsymbol{Q}_L}</script> represent a series of column permutations. The reduced row echelon matrix takes the form <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}&{{\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{0}_{m - r,r}}}&{{\boldsymbol{0}_{m - r,n - r}}}\end{array}} \right] %]]></script>. Where <script type="math/tex">{\boldsymbol{I}_{r,r}}</script> is an <script type="math/tex">r\,\, \times \,\,r</script> identity matrix corresponding to the pivot columns, and <script type="math/tex">{\boldsymbol{F}_{r,n - r}}</script> is an <script type="math/tex">r</script> by <script type="math/tex">n - r</script> matrix containing the free variable columns. The reduced row echelon matrix plays an important role in understanding the four fundamental subspaces for <script type="math/tex">\boldsymbol{A}</script>.</p>

<p><a name="linear-independence-and-basis"></a></p>

<p><br /></p>

<hr />
<h4 id="linear-independence-and-basis">Linear Independence and Basis</h4>
<hr />

<p>What is linear independence? Suppose that we have vectors <script type="math/tex">{\boldsymbol{x}_1},{\boldsymbol{x}_2}, \ldots ,{\boldsymbol{x}_n}</script>. We say that these vectors are independent if no linear combination gives the zero vector. In other words, it is impossible to pick some nontrivial (i.e. not all zero) coefficients to make the linear combination equal the zero vector <script type="math/tex">{c_1}{\boldsymbol{x}_1}\boldsymbol{ + }{c_2}{\boldsymbol{x}_2}\boldsymbol{ + } \cdots \boldsymbol{ + }{c_n}{\boldsymbol{x}_n} \ne \boldsymbol{0}</script>.</p>

<p>How can we determine whether a set of vectors <script type="math/tex">{\boldsymbol{x}_1},{\boldsymbol{x}_2}, \ldots ,{\boldsymbol{x}_n}</script> is independent? One way is to construct a matrix <script type="math/tex">\boldsymbol{A}</script> with <script type="math/tex">{\boldsymbol{x}_1},{\boldsymbol{x}_2}, \ldots ,{\boldsymbol{x}_n}</script> as the columns, and then determine whether there are any nonzero entries in the nullspace. We can achieve this by computing the reduced row echelon form of <script type="math/tex">\boldsymbol{A}</script> and determining whether we have any free variables.</p>

<p>We say that vectors <script type="math/tex">{\boldsymbol{x}_1},{\boldsymbol{x}_2}, \ldots ,{\boldsymbol{x}_l}</script> span a space if the space consists of all linear combinations of those vectors. In other words, if <script type="math/tex">{\boldsymbol{x}_1},{\boldsymbol{x}_2}, \ldots ,{\boldsymbol{x}_l}</script> span a space <script type="math/tex">S</script>, then <script type="math/tex">S</script> is the smallest space containing all linear combinations of those vectors. A basis for a vector space is a sequence of vectors <script type="math/tex">{\boldsymbol{x}_1},{\boldsymbol{x}_2}, \ldots ,{\boldsymbol{x}_d}</script> with two properties: 1) <script type="math/tex">{\boldsymbol{x}_1},{\boldsymbol{x}_2}, \ldots ,{\boldsymbol{x}_d}</script> are independent, and 2) <script type="math/tex">{\boldsymbol{x}_1},{\boldsymbol{x}_2}, \ldots ,{\boldsymbol{x}_d}</script> span the vector space. The basis of a space provides everything we need to know about that space.</p>

<p>How can we test whether a set of vectors <script type="math/tex">{\boldsymbol{x}_1},{\boldsymbol{x}_2}, \ldots ,{\boldsymbol{x}_d}</script> are a basis for <script type="math/tex">{\mathbb{R}^d}</script>? We can construct a matrix <script type="math/tex">\boldsymbol{A}</script> with <script type="math/tex">{\boldsymbol{x}_1},{\boldsymbol{x}_2}, \ldots ,{\boldsymbol{x}_d}</script> as the columns and compute the reduced row echelon matrix. If all columns are independent, then we will have no free variables so the column rank <script type="math/tex">r = d</script>. To see this, remember that any matrix can be written in reduced row echelon form as <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}&{{\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{0}_{m - r,r}}}&{{\boldsymbol{0}_{m - r,n - r}}}\end{array}} \right] %]]></script>. We clearly see that any column in <script type="math/tex">\left[ {\begin{array}{*{20}{c}}{{\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{0}_{m - r,n - r}}}\end{array}} \right]</script> can be written as a linear combination of the columns in <script type="math/tex">\left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}\\{{\boldsymbol{0}_{m - r,r}}}\end{array}} \right]</script> since we have the identity matrix <script type="math/tex">{\boldsymbol{I}_{r,r}}</script> up top. Hence, we only have <script type="math/tex">r</script> linearly independent columns. Note also, that we only have <script type="math/tex">r</script> linearly independent rows, since rows filled with zeros are not independent.</p>

<p>Now, to ensure that the vectors span the complete space, we can look at whether <script type="math/tex">\boldsymbol{Ay = b}</script> is solvable for every <script type="math/tex">\boldsymbol{b}</script> in <script type="math/tex">{\mathbb{R}^d}</script>. To be solvable for every <script type="math/tex">\boldsymbol{b}</script>, we cannot have a row of zeros at the bottom of the reduced row echelon matrix. Therefore, to be a basis, the reduced row echelon form must be the identity matrix.</p>

<p>Suppose we now want to test whether a set of vectors <script type="math/tex">{\boldsymbol{x}_1},{\boldsymbol{x}_2}, \ldots ,{\boldsymbol{x}_d}</script> are a basis for a subspace <script type="math/tex">{\mathbb{R}^n}</script>. We can repeat each of the steps described above, the only difference is that when examining whether these vectors span the subspace, we are only interested in determining whether <script type="math/tex">\boldsymbol{Ay = b}</script> is solvable for <script type="math/tex">\boldsymbol{b}</script> in the subspace. For a given subspace, every basis for that space has the same number of vectors. We call this number the dimension of the subspace.</p>

<p><a name="pseudoinverse"></a></p>

<p><br /></p>

<hr />
<h4 id="pseudoinverse">Pseudoinverse</h4>
<hr />

<p>If we have a matrix <script type="math/tex">\boldsymbol{A}</script> that has full column rank but not full row rank, then we know that <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> is invertible since the nullspace is empty (except for the zero vector). To see why, recall that full column rank implies that all columns are linearly independent. Therefore, by the definition of linear independence, <script type="math/tex">\boldsymbol{Ax = 0}</script> only has the trivial solution and the nullspace only contains the zero vector. Furthermore, we see that the nullspace of <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> is equal to the nullspace of <script type="math/tex">\boldsymbol{A}</script> since <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{Ax} = \boldsymbol{0} \Rightarrow {\boldsymbol{x}^T}{\boldsymbol{A}^T}\boldsymbol{Ax} = 0 \Rightarrow {\left\| {\boldsymbol{Ax}} \right\|^2} = 0</script>. Hence it follows that the nullspace of <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> is empty and <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> is invertible since it is square and must have full rank for the nullspace of <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> to be empty.  Therefore, we see that <script type="math/tex">{\left( {{\boldsymbol{A}^T}\boldsymbol{A}} \right)^{ - 1}}{\boldsymbol{A}^T}</script> is a left inverse of <script type="math/tex">\boldsymbol{A}</script> since <script type="math/tex">{\left( {{\boldsymbol{A}^T}\boldsymbol{A}} \right)^{ - 1}}{\boldsymbol{A}^T}\boldsymbol{A} = \boldsymbol{I}</script>.</p>

<p>Similarly, if we have a matrix <script type="math/tex">\boldsymbol{A}</script> that has full row rank but not full column rank, then we know that <script type="math/tex">\boldsymbol{A}{\boldsymbol{A}^T}</script> is invertible Therefore, we see that <script type="math/tex">{\boldsymbol{A}^T}{\left( {\boldsymbol{A}{\boldsymbol{A}^T}} \right)^{ - 1}}</script> is a right inverse of <script type="math/tex">\boldsymbol{A}</script> since<script type="math/tex">\boldsymbol{A}{\boldsymbol{A}^T}{\left( {\boldsymbol{A}{\boldsymbol{A}^T}} \right)^{ - 1}} = \boldsymbol{I}</script>.</p>

<p>Now suppose that a matrix does not have full row rank nor full column rank. Since the column space and the row space have the same dimension (column rank equals row rank) there should be a mapping between the two. The pseudoinverse is the invertible transformation from the row space to the column space.</p>

<p>If <script type="math/tex">\boldsymbol{x} \ne \boldsymbol{y}</script> are in the row space, then <script type="math/tex">\boldsymbol{Ax} \ne \boldsymbol{Ay}</script>. To see this, suppose that <script type="math/tex">\boldsymbol{Ax = Ay}</script>, then it follows that <script type="math/tex">\boldsymbol{A}\left( {\boldsymbol{x} - \boldsymbol{y}} \right) = \boldsymbol{0}</script> which implies that <script type="math/tex">\boldsymbol{x} - \boldsymbol{y}</script> is in the nullspace. However, vectors in the nullspace are orthogonal to vectors in the row space, so <script type="math/tex">\boldsymbol{x} - \boldsymbol{y} = \boldsymbol{0}</script>. However, we know that <script type="math/tex">\boldsymbol{x} \ne \boldsymbol{y}</script>, thus, <script type="math/tex">\boldsymbol{Ax} \ne \boldsymbol{Ay}</script>.</p>

<p>How can we find the pseudoinverse <script type="math/tex">{\boldsymbol{A}^ + }</script>? We can compute the SVD which gives <script type="math/tex">\boldsymbol{A} = \boldsymbol{U}\Sigma {\boldsymbol{V}^T}</script>. By taking the reciprocals of the nonzero elements of <script type="math/tex">\Sigma</script> we get <script type="math/tex">{\Sigma ^ + }</script> which enables the computation of the pseudoinverse as <script type="math/tex">{\boldsymbol{A}^ + } = \boldsymbol{V}{\Sigma ^ + }{\boldsymbol{U}^T}</script>.</p>

<p>Let’s dig into why the formula for the pseudoinverse <script type="math/tex">{\boldsymbol{A}^ + } = \boldsymbol{V}{\Sigma ^ + }{\boldsymbol{U}^T}</script> makes sense. Suppose that the <script type="math/tex">m\,\, \times \,\,n</script> matrix <script type="math/tex">\boldsymbol{A}</script> has rank <script type="math/tex">r</script>. We can write the pseudoinverse as a sum of rank 1 matrices as <script type="math/tex">{\boldsymbol{A}^ + } = \frac{1}{{{\sigma _1}}}{\boldsymbol{v}_1}\boldsymbol{u}_1^T +  \cdots  + \frac{1}{{{\sigma _r}}}{\boldsymbol{v}_r}\boldsymbol{u}_r^T + 0{\boldsymbol{v}_{r + 1}}\boldsymbol{u}_{r + 1}^T +  \cdots  + 0{\boldsymbol{v}_M}\boldsymbol{u}_M^T</script> where <script type="math/tex">M = \min \left\{ {m,n} \right\}</script>. Suppose that we have a vector in <script type="math/tex">{\mathbb{R}^m}</script> which we can write as <script type="math/tex">\boldsymbol{x} = {c_1}{\boldsymbol{u}_1} +  \cdots  + {c_m}{\boldsymbol{u}_m}</script> (since the columns of <script type="math/tex">\boldsymbol{U}</script>provide an orthonormal basis for <script type="math/tex">{\mathbb{R}^m}</script>). Multiplying by the pseudoinverse and using orthonormality to simplify we get:</p>

<script type="math/tex; mode=display">\begin{array}{l}{\boldsymbol{A}^ + }\boldsymbol{x} = \left( {\frac{1}{{{\sigma _1}}}{\boldsymbol{v}_1}\boldsymbol{u}_1^T +  \cdots  + \frac{1}{{{\sigma _r}}}{\boldsymbol{v}_r}\boldsymbol{u}_r^T + 0{\boldsymbol{v}_{r + 1}}\boldsymbol{u}_{r + 1}^T +  \cdots  + 0{\boldsymbol{v}_M}\boldsymbol{u}_M^T} \right)\left( {{c_1}{\boldsymbol{u}_1} +  \cdots  + {c_m}{\boldsymbol{u}_m}} \right)\\ = \frac{{{c_1}}}{{{\sigma _1}}}{\boldsymbol{v}_1} +  \cdots  + \frac{{{c_r}}}{{{\sigma _r}}}{\boldsymbol{v}_r}\end{array}</script>

<p>Since <script type="math/tex">{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_r}</script> provides an orthonormal basis for the row space, we can interpret the action of the pseudoinverse <script type="math/tex">{\boldsymbol{A}^ + }\boldsymbol{x}</script> as a two-step procedure: 1) computing the projection of <script type="math/tex">\boldsymbol{x}</script> onto the column space of <script type="math/tex">\boldsymbol{A}</script>, and 2) mapping this column space projection to a vector in the row space. Hence, the pseudoinverse finds the vector in the row space <script type="math/tex">\boldsymbol{v}</script> which maps to the vector <script type="math/tex">{\boldsymbol{x}^*} = \boldsymbol{Av}</script> in the column space such that <script type="math/tex">{\boldsymbol{x}^*}</script> is closest to <script type="math/tex">\boldsymbol{x}</script>. To see why, consider computing <script type="math/tex">\boldsymbol{A}{\boldsymbol{A}^ + }\boldsymbol{x}</script>. If we expand <script type="math/tex">\boldsymbol{A}</script> using its rank-1 SVD representation we can write</p>

<script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{A}{\boldsymbol{A}^ + }\boldsymbol{x = }\left( {{\sigma _1}{\boldsymbol{u}_1}\boldsymbol{v}_1^T +  \cdots  + {\sigma _r}{\boldsymbol{u}_r}\boldsymbol{v}_r^T + 0{\boldsymbol{u}_{r + 1}}\boldsymbol{v}_{r + 1}^T +  \cdots  + 0{\boldsymbol{u}_M}\boldsymbol{v}_M^T} \right)\left( {\frac{{{c_1}}}{{{\sigma _1}}}{\boldsymbol{v}_1} +  \cdots  + \frac{{{c_r}}}{{{\sigma _r}}}{\boldsymbol{v}_r}} \right)\\ = {c_1}{\boldsymbol{u}_1} +  \cdots  + {c_r}{\boldsymbol{u}_r}\end{array}</script>

<p>Notice that the vector <script type="math/tex">{c_1}{\boldsymbol{u}_1} +  \cdots  + {c_r}{\boldsymbol{u}_r}</script> is the projection of <script type="math/tex">\boldsymbol{x} = {c_1}{\boldsymbol{u}_1} +  \cdots  + {c_m}{\boldsymbol{u}_m}</script> onto the column space of <script type="math/tex">\boldsymbol{A}</script>. To summarize: The pseudoinverse in enabled by the SVD theorem which tells us that there exist basis vectors for the row and column space of <script type="math/tex">\boldsymbol{A}</script> such that <script type="math/tex">\boldsymbol{A}{\boldsymbol{v}_k} = {\sigma _k}{\boldsymbol{u}_k}</script>. The pseudoinverse <script type="math/tex">{\boldsymbol{A}^ + }\boldsymbol{x}</script> provides the coordinate vector in the row space of <script type="math/tex">\boldsymbol{A}</script> (relative to the basis <script type="math/tex">{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_r}</script>) that maps to the projection of <script type="math/tex">\boldsymbol{x}</script> onto the column space (relative to the basis <script type="math/tex">{\boldsymbol{u}_1}, \ldots ,{\boldsymbol{u}_r}</script>).</p>

<p><a name="projections"></a></p>

<p><br /></p>

<hr />
<h4 id="projections">Projections</h4>
<hr />

<p>We call the closest point to <script type="math/tex">\boldsymbol{b}</script> along the line <script type="math/tex">c\boldsymbol{a}</script> the projection of <script type="math/tex">\boldsymbol{b}</script> onto <script type="math/tex">\boldsymbol{a}</script>. To find the projection of a vector <script type="math/tex">\boldsymbol{b}</script> onto a vector <script type="math/tex">\boldsymbol{a}</script>, we can compute the error term as being the difference between <script type="math/tex">\boldsymbol{b}</script> and the projected point written as <script type="math/tex">\boldsymbol{e = b} - c\boldsymbol{a}</script>. At the closest point, the error term will be orthogonal to <script type="math/tex">\boldsymbol{a}</script>. To see why, think about the error term as being composed of two components: 1) a component orthogonal to <script type="math/tex">\boldsymbol{a}</script>, and 2) a component parallel to <script type="math/tex">\boldsymbol{a}</script>. To minimize the error term, we need to minimize the component parallel to <script type="math/tex">\boldsymbol{a}</script>. When the parallel component is 0, then what we have left is the orthogonal component. Therefore, at the closest point, the error term must be orthogonal to <script type="math/tex">\boldsymbol{a}</script> which gives us the equation <script type="math/tex">{\boldsymbol{a}^T}\boldsymbol{e} = {\boldsymbol{a}^T}\left( {\boldsymbol{b} - c\boldsymbol{a}} \right) = 0</script>. Solving for <script type="math/tex">c</script> we get <script type="math/tex">c = \frac{{{\boldsymbol{a}^T}\boldsymbol{b}}}{{{\boldsymbol{a}^T}\boldsymbol{a}}}</script>. Hence the projection of <script type="math/tex">\boldsymbol{b}</script> onto <script type="math/tex">\boldsymbol{a}</script> is <script type="math/tex">c\boldsymbol{a = a}\frac{{{\boldsymbol{a}^T}\boldsymbol{b}}}{{{\boldsymbol{a}^T}\boldsymbol{a}}} = \frac{{\boldsymbol{a}{\boldsymbol{a}^T}}}{{{\boldsymbol{a}^T}\boldsymbol{a}}}\boldsymbol{b}</script>. Therefore, we can think about the projection as being performed by a matrix where <script type="math/tex">\boldsymbol{P} = \frac{{\boldsymbol{a}{\boldsymbol{a}^T}}}{{{\boldsymbol{a}^T}\boldsymbol{a}}}</script>. We can multiply the matrix <script type="math/tex">\boldsymbol{b}</script> by <script type="math/tex">\boldsymbol{P}</script> to get the projection onto <script type="math/tex">\boldsymbol{a}</script>.</p>

<p>Examining the projection matrix, we see that the column space of <script type="math/tex">\boldsymbol{P}</script> is the combination of columns of <script type="math/tex">\boldsymbol{P} = \frac{{\boldsymbol{a}{\boldsymbol{a}^T}}}{{{\boldsymbol{a}^T}\boldsymbol{a}}}</script>. Notice that <script type="math/tex">\boldsymbol{a}{\boldsymbol{a}^T}</script> is rank 1 since <script type="math/tex">\boldsymbol{a}</script> is a vector. Therefore, the columns of <script type="math/tex">\boldsymbol{P}</script> are multiples of <script type="math/tex">\boldsymbol{a}</script>. So the column space of <script type="math/tex">\boldsymbol{P}</script>  can be visualized as multiples of the vector <script type="math/tex">\boldsymbol{a}</script>. We also see that <script type="math/tex">{\boldsymbol{P}^T} = \boldsymbol{P}</script>, so the matrix is symmetric. Furthermore, since the projection matrix will project onto <script type="math/tex">\boldsymbol{a}</script>, then it follows that if we project again onto <script type="math/tex">\boldsymbol{a}</script> then we will get the same point, so <script type="math/tex">{\boldsymbol{P}^2} = \boldsymbol{P}</script>.</p>

<p>For many equations, <script type="math/tex">\boldsymbol{Ax = b}</script> there does not exist a solution. Therefore, our goal is to find the solution to <script type="math/tex">\boldsymbol{A\hat x = p}</script>, where <script type="math/tex">\boldsymbol{p}</script> is a projection of <script type="math/tex">\boldsymbol{b}</script> onto the column space of <script type="math/tex">\boldsymbol{A}</script>. We can compute the projection of a point <script type="math/tex">\boldsymbol{b}</script> onto the column space of <script type="math/tex">\boldsymbol{A}</script> by first computing the error measured as the distance between <script type="math/tex">\boldsymbol{b}</script> and the projected point <script type="math/tex">\boldsymbol{p}</script>. We can compute the projected point as <script type="math/tex">\boldsymbol{A\hat x = p}</script>. Now for the error vector <script type="math/tex">\boldsymbol{e = b} - \boldsymbol{p}</script> to be orthogonal to the columns of <script type="math/tex">\boldsymbol{A}</script>, the error vector must be in the left nullspace of <script type="math/tex">\boldsymbol{A}</script> such that <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{e = 0}</script>.  We can expand this equation as <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A\hat x = }{\boldsymbol{A}^T}\boldsymbol{b}</script>. Therefore, the solution is <script type="math/tex">\boldsymbol{\hat x} = {\left( {{\boldsymbol{A}^T}\boldsymbol{A}} \right)^{ - 1}}{\boldsymbol{A}^T}\boldsymbol{b}</script> and we can write the projection vector as <script type="math/tex">\boldsymbol{p = A\hat x = A}{\left( {{\boldsymbol{A}^T}\boldsymbol{A}} \right)^{ - 1}}{\boldsymbol{A}^T}\boldsymbol{b}</script>. Therefore, the projection matrix is <script type="math/tex">\boldsymbol{P = A}{\left( {{\boldsymbol{A}^T}\boldsymbol{A}} \right)^{ - 1}}{\boldsymbol{A}^T}</script>. Notice that if <script type="math/tex">\boldsymbol{A}</script> is a square invertible matrix, then the projection matrix becomes <script type="math/tex">\boldsymbol{P = A}{\left( {{\boldsymbol{A}^T}\boldsymbol{A}} \right)^{ - 1}}{\boldsymbol{A}^T} = \boldsymbol{A}{\boldsymbol{A}^{ - 1}}{\boldsymbol{A}^{ - T}}{\boldsymbol{A}^T} = \boldsymbol{I}</script>. We see that <script type="math/tex">{\boldsymbol{P}^T} = \boldsymbol{P}</script> and <script type="math/tex">{\boldsymbol{P}^2} = \boldsymbol{P}</script>. Note that if <script type="math/tex">\boldsymbol{b}</script> is in the column space then <script type="math/tex">\boldsymbol{Pb = b}</script>, while if <script type="math/tex">\boldsymbol{b}</script> is orthogonal to the column space then <script type="math/tex">\boldsymbol{Pb = 0}</script>.</p>

<p>If <script type="math/tex">\boldsymbol{P}</script> is a projection matrix onto the column space, then <script type="math/tex">\boldsymbol{I} - \boldsymbol{P}</script> is a projection matrix onto the left nullspace. This is because we can view any vector <script type="math/tex">\boldsymbol{b}</script> as being a combination of a component in the column space and a component in the left nullspace. Therefore, since <script type="math/tex">\boldsymbol{Pb}</script> gives us the column space component of <script type="math/tex">\boldsymbol{b}</script>, then <script type="math/tex">\boldsymbol{b} - \boldsymbol{Pb = }\left( {\boldsymbol{I} - \boldsymbol{P}} \right)\boldsymbol{b}</script> gives us the projection onto the left nullspace.</p>

<p>In least squares the goal is to minimize <script type="math/tex">{\left\| {\boldsymbol{Ax} - \boldsymbol{b}} \right\|^2} = {\left\| \boldsymbol{e} \right\|^2}</script>. We can look at the least squares problem in two ways. The first way is to consider the errors produced by each equation which is contained in the individual elements of <script type="math/tex">\boldsymbol{e}</script>. For example, <script type="math/tex">{\left\| \boldsymbol{e} \right\|^2} = e_1^2 +  \cdots  + e_m^2</script>. These errors correspond to the squared difference between the prediction line and the training data. We can use calculus to minimize these errors by setting the partial derivatives to 0.</p>

<p>The second way to consider the column space as representing a space for the parameters. Each point in the column space represents a set of output predictions given the input data. Therefore, we can take our training data and project the desired outputs onto the column space to find the set of output predictions which are closest. Then we can determine the set of parameters which produce these predictions in the column space. The set of equations to solve to determine these parameters are called the normal equations written as <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{Ax = }{\boldsymbol{A}^T}\boldsymbol{b}</script>.</p>

<p>When is <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> invertible? If <script type="math/tex">\boldsymbol{A}</script> has independent columns, then <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> is invertible. To show this, suppose that <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{Ax = 0}</script>. This implies that <script type="math/tex">\boldsymbol{Ax}</script> must be orthogonal to the columns of <script type="math/tex">\boldsymbol{A}</script>. But by definition, <script type="math/tex">\boldsymbol{Ax}</script> is a combination of the columns of <script type="math/tex">\boldsymbol{A}</script>. Therefore, we must have <script type="math/tex">\boldsymbol{Ax = 0}</script>. Since we postulate that the columns of <script type="math/tex">\boldsymbol{A}</script> are independent, then <script type="math/tex">\boldsymbol{x = 0}</script> which implies that <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> is invertible. Another way to see this is to take the dot product with <script type="math/tex">\boldsymbol{x}</script> such that <script type="math/tex">{\boldsymbol{x}^T}{\boldsymbol{A}^T}\boldsymbol{Ax = }{\left\| {\boldsymbol{Ax}} \right\|^2}\boldsymbol{ = 0} \Rightarrow \boldsymbol{Ax = 0}</script>.</p>

<p><a name="four-fundamental-subspaces"></a></p>

<p><br /></p>

<hr />
<h2 id="four-fundamental-subspaces">Four Fundamental Subspaces</h2>
<hr />

<p>A subspace is a vector space that is contained inside of another vector space. For example, a line in <script type="math/tex">{\mathbb{R}^2}</script> that passes through the origin is a vector space since any linear combination of vectors along this line will also be on the line. Furthermore, a line in <script type="math/tex">{\mathbb{R}^2}</script> is a subspace of the <script type="math/tex">{\mathbb{R}^2}</script> vector space since every vector on the line is also in <script type="math/tex">{\mathbb{R}^2}</script>. The total possible subspaces of <script type="math/tex">{\mathbb{R}^2}</script> are: 1) all of <script type="math/tex">{\mathbb{R}^2}</script>,  2) any line in <script type="math/tex">{\mathbb{R}^2}</script> that passes through the origin, 3) the zero vector. Two subspaces <script type="math/tex">S</script> and <script type="math/tex">T</script> are said to be orthogonal if every vector in <script type="math/tex">S</script> is orthogonal to every vector in  <script type="math/tex">T</script>.</p>

<p>We have four fundamental subspaces associated with any <script type="math/tex">m</script> by <script type="math/tex">n</script> matrix <script type="math/tex">\boldsymbol{A}</script>:</p>
<ol>
  <li><strong>The column space <script type="math/tex">C\left( \boldsymbol{A} \right)</script>:</strong> is a subspace of <script type="math/tex">{\mathbb{R}^m}</script> and contains all linear combinations of the columns of <script type="math/tex">\boldsymbol{A}</script>. The <script type="math/tex">r</script> pivot columns from the reduced row echelon matrix for <script type="math/tex">\boldsymbol{A}</script> form a basis for <script type="math/tex">C\left( \boldsymbol{A} \right)</script>. Therefore, the dimension of the subspace is <script type="math/tex">\dim \,\,C\left( \boldsymbol{A} \right) = r</script>. All vectors in <script type="math/tex">C\left( \boldsymbol{A} \right)</script> are orthogonal to vectors in the left nullspace <script type="math/tex">N\left( {{\boldsymbol{A}^T}} \right)</script>. As an example, suppose we have a 3 by 3 matrix <script type="math/tex">\boldsymbol{A}</script> where each of the column vectors is in <script type="math/tex">{\mathbb{R}^3}</script>. We can interpret each column vector as describing a line in <script type="math/tex">{\mathbb{R}^3}</script> that passes through the origin. Therefore, each column vector is itself a subspace of <script type="math/tex">{\mathbb{R}^3}</script>. If we consider the linear combination of all columns of <script type="math/tex">\boldsymbol{A}</script> then we get a subspace of <script type="math/tex">{\mathbb{R}^3}</script> that we call the column space of <script type="math/tex">\boldsymbol{A}</script>, written as <script type="math/tex">C\left( \boldsymbol{A} \right)</script>.</li>
  <li><strong>The nullspace <script type="math/tex">N\left( \boldsymbol{A} \right)</script>:</strong> is a subspace of <script type="math/tex">{\mathbb{R}^n}</script> and contains the vectors that are the solutions to the equation<script type="math/tex">\boldsymbol{Ax = 0}</script>. From the reduced row echelon form of <script type="math/tex">\boldsymbol{A}</script>, written as <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}&{{\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{0}_{m - r,r}}}&{{\boldsymbol{0}_{m - r,n - r}}}\end{array}} \right] %]]></script>, we can solve for a basis of the nullspace as the columns of <script type="math/tex">\boldsymbol{M} = \left[ {\begin{array}{*{20}{c}}{ - {\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{I}_{n - r,n - r}}}\end{array}} \right]</script> where <script type="math/tex">{\boldsymbol{F}_{r,n - r}}</script> are the free variable coefficients in the reduced row echelon form of <script type="math/tex">\boldsymbol{A}</script>. Hence, we see that the dimension of the null space is equal to the number of free variables <script type="math/tex">\dim \,\,N\left( \boldsymbol{A} \right) = n - r</script>. To illustrate why the dimension of the nullspace of <script type="math/tex">\boldsymbol{A}</script> must be <script type="math/tex">n - r</script>, consider the following argument. If the rank of <script type="math/tex">\boldsymbol{A}</script> is <script type="math/tex">r</script>, then there are <script type="math/tex">r</script> linearly independent rows of <script type="math/tex">\boldsymbol{A}</script>. Hence, to solve <script type="math/tex">\boldsymbol{Ax} = 0</script>, we have <script type="math/tex">r</script> equations (one for each linearly independent row), and <script type="math/tex">n</script> variables in <script type="math/tex">\boldsymbol{x}</script>. Now consider the thought experiment of assigning arbitrary values to the first <script type="math/tex">n - r</script> variables in <script type="math/tex">\boldsymbol{x}</script>. Under these conditions, we have <script type="math/tex">r</script> variables remaining in <script type="math/tex">\boldsymbol{x}</script> that can be used to solve the system uniquely. Hence, we have <script type="math/tex">n - r</script>degrees of freedom in the nullspace which amounts to having dimensionality <script type="math/tex">n - r</script>.</li>
  <li><strong>The row space <script type="math/tex">C\left( {{\boldsymbol{A}^T}} \right)</script>:</strong> is a subspace of <script type="math/tex">{\mathbb{R}^n}</script> and contains all linear combinations of the rows of <script type="math/tex">\boldsymbol{A}</script>. The reduced row echelon form of <script type="math/tex">\boldsymbol{A}</script> can be written as <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}&{{\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{0}_{m - r,r}}}&{{\boldsymbol{0}_{m - r,n - r}}}\end{array}} \right] %]]></script>. Therefore, we see that there are <script type="math/tex">r</script> nonzero rows. Recall that we computed <script type="math/tex">\boldsymbol{R}</script> by performing invertible row and column operations on <script type="math/tex">\boldsymbol{A}</script> such that <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{AQ = R}</script>. Hence we see that <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{A} = \boldsymbol{R}{\boldsymbol{Q}^{ - 1}}</script> where <script type="math/tex">\boldsymbol{Q = }{\boldsymbol{Q}^{ - 1}}</script> is a column permutation operation matrix (i.e. the inverse of swapping two rows is to apply the same operation again and swap them back) and only affects the order of the elements in rows of <script type="math/tex">\boldsymbol{R}</script>. Hence, we see that by just performing elementary row operations <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}</script> we produced the matrix <script type="math/tex">\boldsymbol{R}</script> which has <script type="math/tex">m - r</script> rows filled with zeros. Therefore, we see that <script type="math/tex">\boldsymbol{R}</script> has <script type="math/tex">r</script> independent rows and thus, <script type="math/tex">\boldsymbol{A}</script> has <script type="math/tex">r</script> independent rows. Hence, the first <script type="math/tex">r</script> rows of <script type="math/tex">\boldsymbol{R}</script> form a basis for the row space of <script type="math/tex">\boldsymbol{A}</script>.</li>
  <li><strong>The left nullspace <script type="math/tex">N\left( {{\boldsymbol{A}^T}} \right)</script>:</strong> is a subspace of <script type="math/tex">{\mathbb{R}^m}</script> and contains the vectors that are the solution to the equation <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{x} = \boldsymbol{xA = 0}</script>. Since the rank of <script type="math/tex">{\boldsymbol{A}^T}</script> is <script type="math/tex">r</script>, then the number of free columns of <script type="math/tex">{\boldsymbol{A}^T}</script> must be <script type="math/tex">m - r</script>. Therefore, the dimension of the left nullspace must be <script type="math/tex">\dim \,\,N\left( {{\boldsymbol{A}^T}} \right) = m - r</script>. We can write the reduced row echelon matrix as <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{AQ = R}</script>. Rearranging, we see that<script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{A = R}{\boldsymbol{Q}^{ - 1}}</script>. Since the reduced row echelon matrix has the form <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}&{{\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{0}_{m - r,r}}}&{{\boldsymbol{0}_{m - r,n - r}}}\end{array}} \right] %]]></script> and <script type="math/tex">\boldsymbol{Q}</script> is a column permutation matrix, <script type="math/tex">\boldsymbol{Q}</script> will have no effect on the <script type="math/tex">m - r</script> bottom rows filled with zeros. We can interpret <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{A}</script> as a linear combination of the rows of <script type="math/tex">\boldsymbol{A}</script> where the combination coefficients are the values in <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}</script>. Hence, the coefficients that produce rows filled with zeros are the last <script type="math/tex">m - r</script> rows of <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}</script>. Additionally, we know the rows of <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}</script> are independent because the elementary matrices are invertible. Therefore, the last <script type="math/tex">m - r</script> rows of <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}</script> form a basis for the left nullspace.</li>
</ol>

<p><a name="finding-basis-vectors-for-the-four-fundamental-subspaces"></a></p>

<p><br /></p>

<hr />
<h4 id="finding-basis-vectors-for-the-four-fundamental-subspaces">Finding Basis Vectors for the Four Fundamental Subspaces</h4>
<hr />

<p>Suppose we have an <script type="math/tex">m</script> by <script type="math/tex">n</script> matrix <script type="math/tex">\boldsymbol{A}</script>. Our goal in this section is to demonstrate how we can find a set of basis vectors for each of the four fundamental subspaces.</p>

<p><strong>Column Space:</strong>
To determine a basis for the column space of <script type="math/tex">\boldsymbol{A}</script>, we can first calculate the reduced row echelon form <script type="math/tex">\boldsymbol{EA = R}</script>, where <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}&{{\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{0}_{m - r,r}}}&{{\boldsymbol{0}_{m - r,n - r}}}\end{array}} \right] %]]></script> and <script type="math/tex">\boldsymbol{E}</script> represents a series of invertible elementary row operations. Note that for simplicity we are assuming that no column permutations are needed. We see that the first <script type="math/tex">r</script> columns of <script type="math/tex">\boldsymbol{R}</script> form a basis for the column space of <script type="math/tex">\boldsymbol{R}</script> (due to the identity matrix in the top left). We can write each vector in the column space of <script type="math/tex">\boldsymbol{A}</script> as a linear combination of the columns of <script type="math/tex">\boldsymbol{A}</script>, written as <script type="math/tex">{c_1}{\boldsymbol{a}_1} +  \cdots  + {c_n}{\boldsymbol{a}_n}</script> where <script type="math/tex">{c_1}, \ldots ,{c_n}</script> are scalars and <script type="math/tex">{\boldsymbol{a}_1}, \ldots ,{\boldsymbol{a}_n}</script> are the columns of <script type="math/tex">\boldsymbol{A}</script>. Now using the relationship with the reduced row echelon matrix, we can write</p>

<script type="math/tex; mode=display">\begin{array}{l}{c_1}{\boldsymbol{a}_1} +  \cdots  + {c_n}{\boldsymbol{a}_n} = {c_1}{\boldsymbol{E}^{ - 1}}{\boldsymbol{r}_1} +  \cdots  + {c_n}{\boldsymbol{E}^{ - 1}}{\boldsymbol{r}_n}\\ = {\boldsymbol{E}^{ - 1}}\left( {{c_1}{\boldsymbol{r}_1} +  \cdots  + {c_n}{\boldsymbol{r}_n}} \right)\end{array}</script>

<p>Where <script type="math/tex">{\boldsymbol{r}_1}, \ldots ,{\boldsymbol{r}_n}</script> are the columns of <script type="math/tex">\boldsymbol{R}</script>. Notice that the combination <script type="math/tex">{c_1}{\boldsymbol{r}_1} +  \cdots  + {c_n}{\boldsymbol{r}_n}</script> is in the column space of <script type="math/tex">\boldsymbol{R}</script> and therefore can be written using the first  <script type="math/tex">r</script> columns as basis vectors as <script type="math/tex">{c_1}{\boldsymbol{r}_1} +  \cdots  + {c_n}{\boldsymbol{r}_n} = {d_1}{\boldsymbol{r}_1} +  \cdots  + {d_r}{\boldsymbol{r}_r}</script>, where <script type="math/tex">{d_1}, \ldots ,{d_r}</script> are scalars. Hence, we can pop this back into the equation above to get</p>

<script type="math/tex; mode=display">\begin{array}{l}{c_1}{\boldsymbol{a}_1} +  \cdots  + {c_n}{\boldsymbol{a}_n} = {\boldsymbol{E}^{ - 1}}\left( {{d_1}{\boldsymbol{r}_1} +  \cdots  + {d_r}{\boldsymbol{r}_r}} \right)\\ = {d_1}{\boldsymbol{E}^{ - 1}}{\boldsymbol{r}_1} +  \cdots  + {d_r}{\boldsymbol{E}^{ - 1}}{\boldsymbol{r}_r}\\ = {d_1}{\boldsymbol{a}_1} +  \cdots  + {d_r}{\boldsymbol{a}_r}\end{array}</script>

<p>Hence, we have shown that the columns of <script type="math/tex">\boldsymbol{A}</script> corresponding to the pivot columns of <script type="math/tex">\boldsymbol{R}</script> can be used to describe any vector in the column space of <script type="math/tex">\boldsymbol{A}</script> and hence span the column space.</p>

<p>To show that this is the minimal spanning set, we must show that all of the <script type="math/tex">{\boldsymbol{a}_1}, \ldots ,{\boldsymbol{a}_r}</script> are independent. We can show this by contradiction. Suppose that there did exist scalar coefficients such that <script type="math/tex">{c_1}{\boldsymbol{a}_1} +  \cdots  + {c_r}{\boldsymbol{a}_r} = 0</script>. This implies that <script type="math/tex">{c_1}\boldsymbol{E}{\boldsymbol{a}_1} +  \cdots  + {c_r}\boldsymbol{E}{\boldsymbol{a}_r} = {c_1}{\boldsymbol{r}_1} +  \cdots  + {c_r}{\boldsymbol{r}_r} = 0</script> which is impossible since <script type="math/tex">{\boldsymbol{r}_1}, \ldots ,{\boldsymbol{r}_r}</script> are all independent (they come from the identity matrix in <script type="math/tex">\boldsymbol{R}</script>).</p>

<p>Therefore, the columns of <script type="math/tex">\boldsymbol{A}</script> corresponding to the pivot columns of <script type="math/tex">\boldsymbol{R}</script> span the column space of <script type="math/tex">\boldsymbol{A}</script> and the set is minimal since all these vectors are independent. Hence, these vectors form a basis for the column space of <script type="math/tex">\boldsymbol{A}</script>.</p>

<p><strong>Row Space:</strong>
To determine a basis for the row space of <script type="math/tex">\boldsymbol{A}</script>, we can first calculate the reduced row echelon form <script type="math/tex">\boldsymbol{EA = R}</script>, where <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}&{{\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{0}_{m - r,r}}}&{{\boldsymbol{0}_{m - r,n - r}}}\end{array}} \right] %]]></script> and <script type="math/tex">\boldsymbol{E}</script> represents a series of invertible elementary row operations. Thus, we can write, <script type="math/tex">\boldsymbol{A} = {\boldsymbol{E}^{ - 1}}\boldsymbol{R}</script>. We can interpret this matrix product as a linear combination of the rows of <script type="math/tex">\boldsymbol{R}</script>. Since rows filled with zeros do not contribute to a basis, and the first <script type="math/tex">r</script> rows of <script type="math/tex">\boldsymbol{R}</script> are all independent (due to the identity matrix in the top left), we see that the first <script type="math/tex">r</script> rows of <script type="math/tex">\boldsymbol{R}</script> form a basis for the row space of <script type="math/tex">\boldsymbol{A}</script>.</p>

<p><strong>Nullspace:</strong>
To determine a basis for the null space of <script type="math/tex">\boldsymbol{A}</script>, we can first calculate the reduced row echelon form <script type="math/tex">\boldsymbol{EA = R}</script>, where <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}&{{\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{0}_{m - r,r}}}&{{\boldsymbol{0}_{m - r,n - r}}}\end{array}} \right] %]]></script> and <script type="math/tex">\boldsymbol{E}</script> represents a series of invertible elementary row operations. We can show that any solution <script type="math/tex">\boldsymbol{x}</script> to <script type="math/tex">\boldsymbol{Ax = 0}</script> will also be a be a solution to <script type="math/tex">\boldsymbol{Rx = 0}</script>.</p>

<script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{Ax = 0}\\ \Rightarrow \boldsymbol{EAx = E0 = 0}\\ \Rightarrow \boldsymbol{Rx = 0}\end{array}</script>

<p>Therefore, every element in the nullspace of <script type="math/tex">\boldsymbol{A}</script> is also in the nullspace of <script type="math/tex">\boldsymbol{R}</script>, such that <script type="math/tex">N\left( \boldsymbol{A} \right) \subseteq N\left( \boldsymbol{R} \right)</script>. We can also apply this logic in reverse:</p>

<script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{Rx = 0}\\ \Rightarrow {\boldsymbol{E}^{ - 1}}\boldsymbol{Rx = }{\boldsymbol{E}^{ - 1}}\boldsymbol{0 = 0}\\ \Rightarrow \boldsymbol{Ax = 0}\end{array}</script>

<p>Therefore, every element in the nullspace of <script type="math/tex">\boldsymbol{R}</script> is also in the nullspace of <script type="math/tex">\boldsymbol{A}</script>, such that <script type="math/tex">N\left( \boldsymbol{R} \right) \subseteq N\left( \boldsymbol{A} \right)</script>. Therefore, the nullspace of <script type="math/tex">\boldsymbol{R}</script> equals the nullspace of <script type="math/tex">\boldsymbol{A}</script>, <script type="math/tex">N\left( \boldsymbol{R} \right) = N\left( \boldsymbol{A} \right)</script>.</p>

<p>Hence, to find a basis for the nullspace of <script type="math/tex">\boldsymbol{A}</script>, we can instead find a basis for the nullspace of <script type="math/tex">\boldsymbol{R}</script>. We can view the pivot variables as being constrained for each equation in order to make the equation equal to 0. Therefore, the dimension of the nullspace is equal to the number of free variables which is <script type="math/tex">n - r</script>. We can find <script type="math/tex">n - r</script> independent solutions to the equation <script type="math/tex">\boldsymbol{RM = 0}</script> by noticing that <script type="math/tex">% <![CDATA[
\boldsymbol{RM = }\left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}&{{\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{0}_{m - r,r}}}&{{\boldsymbol{0}_{m - r,n - r}}}\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{ - {\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{I}_{n - r,n - r}}}\end{array}} \right] %]]></script>. Therefore, <script type="math/tex">\left[ {\begin{array}{*{20}{c}}{ - {\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{I}_{n - r,n - r}}}\end{array}} \right]</script> forms a basis for the nullspace because it contains <script type="math/tex">n - r</script> independent vectors (due to the identity matrix).</p>

<p><strong>Left Nullspace:</strong>
To determine a basis for the left nullspace of <script type="math/tex">\boldsymbol{A}</script>, we can first calculate the reduced row echelon form <script type="math/tex">\boldsymbol{EA = R}</script>, where <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}&{{\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{0}_{m - r,r}}}&{{\boldsymbol{0}_{m - r,n - r}}}\end{array}} \right] %]]></script> and <script type="math/tex">\boldsymbol{E}</script> represents a series of invertible elementary row operations. We know that the row rank is <script type="math/tex">r</script>, therefore, the number of free columns of <script type="math/tex">{\boldsymbol{A}^T}</script> must be <script type="math/tex">m - r</script> and the dimension of the left nullspace must be <script type="math/tex">m - r</script>. The left nullspace is defined as the solutions to <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{x} = \boldsymbol{0}</script> which is equivalent (by taking the transpose) to <script type="math/tex">{\boldsymbol{x}^T}\boldsymbol{A} = \boldsymbol{0}</script>. Notice that in the result of the multiplication <script type="math/tex">\boldsymbol{EA = R}</script>, the bottom <script type="math/tex">m - r</script> rows of <script type="math/tex">\boldsymbol{R}</script>are filled with zeros. This means that the bottom <script type="math/tex">m - r</script> rows of <script type="math/tex">\boldsymbol{E}</script> must be solutions to <script type="math/tex">{\boldsymbol{x}^T}\boldsymbol{A} = \boldsymbol{0}</script>. Since <script type="math/tex">\boldsymbol{E}</script> is invertible, all the rows must be independent, otherwise there would be a linear combination of the rows such that <script type="math/tex">{\boldsymbol{E}^T}\boldsymbol{y} = \boldsymbol{0}</script> for nonzero <script type="math/tex">\boldsymbol{y}</script> which is impossible since <script type="math/tex">\boldsymbol{E}</script> is invertible and there is no transformation to transform <script type="math/tex">\boldsymbol{0}</script> back into <script type="math/tex">\boldsymbol{y}</script>. Hence the bottom <script type="math/tex">m - r</script> rows of <script type="math/tex">\boldsymbol{E}</script> must form a basis for the left nullspace of <script type="math/tex">\boldsymbol{A}</script>.</p>

<p><a name="solving-the-equation-$$\boldsymbol{ax}-=-\boldsymbol{b}$$-"></a></p>

<p><br /></p>

<hr />
<h2 id="solving-the-equation-boldsymbolax--boldsymbolb">Solving the Equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script></h2>
<hr />

<p>One of the fundamental problems in linear algebra is how to find solutions to the equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>. In this section we will work through how to effectively visualize this problem and how to solve it.</p>

<p><a name="interpreting-the-product-$$\boldsymbol{ax}$$"></a></p>

<p><br /></p>

<hr />
<h4 id="interpreting-the-product-boldsymbolax">Interpreting the Product <script type="math/tex">\boldsymbol{Ax}</script></h4>
<hr />

<p>Suppose we have a 3 x 3 matrix <script type="math/tex">\boldsymbol{A}</script> defined as</p>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{A} = \left[ {\begin{array}{*{20}{c}}1&4&5\\3&2&5\\2&1&3\end{array}} \right] %]]></script>

<p>and a 3 x 1 vector <script type="math/tex">\boldsymbol{x}</script> defined as</p>

<script type="math/tex; mode=display">\boldsymbol{x = }\left[ {\begin{array}{*{20}{c}}{{x_1}}\\{{x_2}}\\{{x_3}}\end{array}} \right]</script>

<p>How should we interpret the matrix product <script type="math/tex">\boldsymbol{Ax}</script>? Let us consider a few powerful interpretations. Note that although our example is in <script type="math/tex">{\mathbb{R}^3}</script>, these interpretations apply for any n-dimensional coordinate space.</p>

<ol>
  <li><strong>Row Picture - Hyperplanes:</strong> The row picture originates from expanding the matrix product using the rows of <script type="math/tex">\boldsymbol{A}</script> as <script type="math/tex">% <![CDATA[
\boldsymbol{Ax} = \left[ {\begin{array}{*{20}{c}}1&4&5\\3&2&5\\2&1&3\end{array}} \right]\boldsymbol{x} = \left[ {\begin{array}{*{20}{c}}{\left[ {\begin{array}{*{20}{c}}1&4&5\end{array}} \right]\boldsymbol{x}}\\{\left[ {\begin{array}{*{20}{c}}3&2&5\end{array}} \right]\boldsymbol{x}}\\{\left[ {\begin{array}{*{20}{c}}2&1&3\end{array}} \right]\boldsymbol{x}}\end{array}} \right] %]]></script>. We see that the output elements are computed as the dot products between <script type="math/tex">\boldsymbol{x}</script> and the rows of <script type="math/tex">\boldsymbol{A}</script>.The key idea for interpretability is that we can visualize each row of <script type="math/tex">\boldsymbol{A}</script> as describing a normal vector for a hyperplane. Hence, when we take the dot product of <script type="math/tex">\boldsymbol{x}</script> with a row of <script type="math/tex">\boldsymbol{A}</script>, for example <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}1&4&5\end{array}} \right]\boldsymbol{x} %]]></script>, we are effectively computing the closest distance from the origin to the hyperplane with normal vector <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}1&4&5\end{array}} \right] %]]></script> that contains <script type="math/tex">\boldsymbol{x}</script> (note that these distances will be scaled if the rows of <script type="math/tex">\boldsymbol{A}</script> are not unit length). Therefore, for a given <script type="math/tex">\boldsymbol{x}</script>, the matrix product <script type="math/tex">\boldsymbol{Ax}</script> can be interpreted as computing the distances to hyperplanes containing <script type="math/tex">\boldsymbol{x}</script> and with normal vectors given by the rows of<script type="math/tex">\boldsymbol{A}</script>.</li>
  <li><strong>Row Picture – Change of Coordinate System:</strong> The row picture originates from expanding the matrix product using the rows of <script type="math/tex">\boldsymbol{A}</script> as <script type="math/tex">% <![CDATA[
\boldsymbol{Ax} = \left[ {\begin{array}{*{20}{c}}1&4&5\\3&2&5\\2&1&3\end{array}} \right]\boldsymbol{x} = \left[ {\begin{array}{*{20}{c}}{\left[ {\begin{array}{*{20}{c}}1&4&5\end{array}} \right]\boldsymbol{x}}\\{\left[ {\begin{array}{*{20}{c}}3&2&5\end{array}} \right]\boldsymbol{x}}\\{\left[ {\begin{array}{*{20}{c}}2&1&3\end{array}} \right]\boldsymbol{x}}\end{array}} \right] %]]></script>. We see that the output elements are computed as the dot products between <script type="math/tex">\boldsymbol{x}</script> and the rows of<script type="math/tex">\boldsymbol{A}</script>. The key idea for interpretability is that we can visualize each row of <script type="math/tex">\boldsymbol{A}</script> as describing a basis vector for a new coordinate system. Hence, when we take the dot product of <script type="math/tex">\boldsymbol{x}</script> with a row of <script type="math/tex">\boldsymbol{A}</script>, for example <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}1&4&5\end{array}} \right]\boldsymbol{x} %]]></script>, we are effectively computing the projection of <script type="math/tex">\boldsymbol{x}</script> onto the new basis vector <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}1&4&5\end{array}} \right] %]]></script>. Therefore, for a given <script type="math/tex">\boldsymbol{x}</script>, the matrix product <script type="math/tex">\boldsymbol{Ax}</script> can be interpreted as computing the projection of <script type="math/tex">\boldsymbol{x}</script> onto a new coordinate system described by the rows of <script type="math/tex">\boldsymbol{A}</script>.</li>
  <li><strong>Column Picture:</strong> The column picture originates from expanding the matrix product using the columns of <script type="math/tex">\boldsymbol{A}</script> as <script type="math/tex">% <![CDATA[
\boldsymbol{Ax} = \left[ {\begin{array}{*{20}{c}}1&4&5\\3&2&5\\2&1&3\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{{x_1}}\\{{x_2}}\\{{x_3}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}1\\3\\2\end{array}} \right]{x_1} + \left[ {\begin{array}{*{20}{c}}4\\2\\1\end{array}} \right]{x_2} + \left[ {\begin{array}{*{20}{c}}5\\5\\3\end{array}} \right]{x_3} %]]></script>. We see that the output vector can be interpreted as a linear combination of the columns of <script type="math/tex">\boldsymbol{A}</script> where the combination coefficients are the elements of <script type="math/tex">\boldsymbol{x}</script>. Therefore, for a given <script type="math/tex">\boldsymbol{x}</script>, the matrix <script type="math/tex">\boldsymbol{Ax}</script> is a linear combination of the columns.</li>
</ol>

<p><a name="interpreting-the-equation-$$\boldsymbol{ax}-=-\boldsymbol{b}$$-"></a></p>

<p><br /></p>

<hr />
<h4 id="interpreting-the-equation-boldsymbolax--boldsymbolb">Interpreting the Equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script></h4>
<hr />

<p>Suppose that we are trying to solve the equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>, where <script type="math/tex">\boldsymbol{A}</script> is an <script type="math/tex">m</script> by <script type="math/tex">n</script> matrix, and we have substantially more equations than unknowns so <script type="math/tex">m > n</script>. There will typically not exist a unique solution to this problem. What does it mean for there not to exist a unique solution? We can interpret this in two ways:</p>

<ol>
  <li><strong>Row-centric:</strong> we can write the matrix product <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>, as <script type="math/tex">\boldsymbol{Ax} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{A}_{1,:}}}\\ \vdots \\{{\boldsymbol{A}_{m,:}}}\end{array}} \right]\boldsymbol{x} = \left[ {\begin{array}{*{20}{c}}{{{\rm{b}}_1}}\\ \vdots \\{{b_m}}\end{array}} \right]</script>. Therefore, we can interpret the matrix product as a system <script type="math/tex">m</script> equations, each of the form <script type="math/tex">{\boldsymbol{A}_{k,:}}\boldsymbol{x} = {b_k}</script>, where <script type="math/tex">{\boldsymbol{A}_{k,:}}</script> is the <script type="math/tex">{k^{th}}</script> row of <script type="math/tex">\boldsymbol{A}</script>. Each of these equations can be viewed as a hyperplane in <script type="math/tex">{\mathbb{R}^n}</script>. If an <script type="math/tex">\boldsymbol{x}</script> satisfies the equation <script type="math/tex">{\boldsymbol{A}_{k,:}}\boldsymbol{x} = {b_k}</script>, then it implies that <script type="math/tex">\boldsymbol{x}</script> is located on the <script type="math/tex">{k^{th}}</script> hyperplane. To pick an <script type="math/tex">\boldsymbol{x}</script> which satisfies the equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>, it follows that <script type="math/tex">\boldsymbol{x}</script> must be on each of the <script type="math/tex">m</script> hyperplanes, which is equivalent to <script type="math/tex">\boldsymbol{x}</script> being the intersection point of all hyperplanes. In the situation where we have more equations than unknowns such that <script type="math/tex">m > n</script>, then we have more hyperplanes than the dimension of the space holding the hyperplanes and there will generally not exist an intersection point between these hyperplanes. To see why, consider the following example. Suppose that <script type="math/tex">\boldsymbol{x}</script> is a vector in <script type="math/tex">{\mathbb{R}^2}</script> such that we have 2 unknowns. If we have 1 equation, then any point on the hyperplane (in the case of <script type="math/tex">{\mathbb{R}^2}</script>, the generalized “hyperplane” is a line) described by that equation will be a solution to the system. If we have 2 linearly independent equations, then there will be a unique intersection point of the hyperplanes and a unique solution to the system. Suppose we now take this 2-equation system with a unique solution and want to add another equation to the system. For this new system to remain consistent, we must ensure that the new hyperplane contains the unique solution from the 2-equation system. The two normal vectors for the hyperplanes in the 2-equation system describe a complete basis for <script type="math/tex">{\mathbb{R}^2}</script>, therefore, any new hyperplane will have a normal vector that can be written as a linear combination of these basis vectors. Hence, any new equation will need to be linearly dependent on the existing equations from the 2-equation system to preserve consistency. Hence, if we have a new hyperplane that is not linearly dependent on the existing equations, we lose consistency and no longer have a solution. Another way to think about this situation is to consider that to add a new equation to the 2-equation system and preserve consistency, the new hyperplane must contain the unique solution to the 2-equation system, but is free to rotate around that point in space. A linear combination of the prior equations from the 2-equation system can be viewed as constructing a new hyperplane with a normal vector that is effectively a rotation along the axis between the two normal vectors from the 2-equation system.</li>
  <li><strong>Column-centric:</strong> we can write the matrix product <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>, as <script type="math/tex">\boldsymbol{Ax} = {\boldsymbol{A}_{:,1}}{x_1} +  \cdots  + {\boldsymbol{A}_{:,n}}{x_n} = \left[ {\begin{array}{*{20}{c}}{{{\rm{b}}_1}}\\ \vdots \\{{b_m}}\end{array}} \right]</script>. Therefore, we can interpret the matrix product as the linear combination of the columns of <script type="math/tex">\boldsymbol{A}</script>. Effectively the matrix product defines the column space of <script type="math/tex">\boldsymbol{A}</script>. A solution can be found if <script type="math/tex">\boldsymbol{b}</script> is in the column space of <script type="math/tex">\boldsymbol{A}</script>. When <script type="math/tex">m > n</script>, the column space is a lower dimensional linear manifold in the space <script type="math/tex">{\mathbb{R}^m}</script>. As such, it becomes likely that there is a component of <script type="math/tex">\boldsymbol{b}</script> which is not in the column space.</li>
</ol>

<p>Now suppose that we are given a 3 x 1 vector <script type="math/tex">\boldsymbol{b}</script> defined as</p>

<script type="math/tex; mode=display">\boldsymbol{b = }\left[ {\begin{array}{*{20}{c}}{{b_1}}\\{{b_2}}\\{{b_3}}\end{array}} \right]</script>

<p>And we are asked to find <script type="math/tex">\boldsymbol{x}</script> such that <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>. How should we interpret the matrix equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>? Let us consider a few powerful interpretations.</p>

<ol>
  <li><strong>Row Picture - Hyperplanes:</strong> If we expand the matrix equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> using the rows of <script type="math/tex">\boldsymbol{A}</script>, we can write an equation for each row, for example <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}1&4&5\end{array}} \right]\boldsymbol{x} = {b_1} %]]></script>. To satisfy this equation, <script type="math/tex">\boldsymbol{x}</script> must lie on a hyperplane with normal vector <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}1&4&5\end{array}} \right] %]]></script> that is a distance of <script type="math/tex">% <![CDATA[
\frac{{{b_1}}}{{\left\| {\left[ {\begin{array}{*{20}{c}}1&4&5\end{array}} \right]} \right\|}} %]]></script> from the origin. For <script type="math/tex">\boldsymbol{x}</script> to satisfy the matrix equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>, <script type="math/tex">\boldsymbol{x}</script> must simultaneously lie on each of the hyperplanes defined by the row equations. Therefore, we can visualize the solution space for <script type="math/tex">\boldsymbol{x}</script> as the set of points in the intersection of all these hyperplanes. We can visualize the effect of changing the components of <script type="math/tex">\boldsymbol{b}</script> as the process of moving the individual hyperplanes either closer or further from the origin along their respective normal vector axes defined by the rows of <script type="math/tex">\boldsymbol{A}</script>.</li>
  <li><strong>Column Picture:</strong> The column picture originates from expanding the matrix product using the columns of <script type="math/tex">\boldsymbol{A}</script> as<script type="math/tex">% <![CDATA[
\boldsymbol{Ax} = \left[ {\begin{array}{*{20}{c}}1&4&5\\3&2&5\\2&1&3\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{{x_1}}\\{{x_2}}\\{{x_3}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}1\\3\\2\end{array}} \right]{x_1} + \left[ {\begin{array}{*{20}{c}}4\\2\\1\end{array}} \right]{x_2} + \left[ {\begin{array}{*{20}{c}}5\\5\\3\end{array}} \right]{x_3} = \boldsymbol{b} %]]></script>. To satisfy this equation, we must be able to express <script type="math/tex">\boldsymbol{b}</script> as a linear combination of the columns of <script type="math/tex">\boldsymbol{A}</script>. Any coefficients <script type="math/tex">\boldsymbol{x}</script> that express <script type="math/tex">\boldsymbol{b}</script> as a linear combination of the columns of <script type="math/tex">\boldsymbol{A}</script> is a valid solution. Any <script type="math/tex">\boldsymbol{b}</script> that exists in the column space of <script type="math/tex">\boldsymbol{A}</script>, (i.e. the space defined by linear combinations of the columns) will have a solution <script type="math/tex">\boldsymbol{x}</script>.</li>
</ol>

<p><a name="interpreting-the-matrix-product-$$\boldsymbol{a}-=-\boldsymbol{bc}$$-"></a></p>

<p><br /></p>

<hr />
<h4 id="interpreting-the-matrix-product-boldsymbola--boldsymbolbc">Interpreting the Matrix Product <script type="math/tex">\boldsymbol{A} = \boldsymbol{BC}</script></h4>
<hr />

<p>Now suppose that we have two matrices <script type="math/tex">% <![CDATA[
\boldsymbol{B} = \left[ {\begin{array}{*{20}{c}}1&3\\5&2\end{array}} \right] %]]></script> and <script type="math/tex">% <![CDATA[
\boldsymbol{C} = \left[ {\begin{array}{*{20}{c}}4&7\\1&6\end{array}} \right] %]]></script>, and we want to calculate<script type="math/tex">\boldsymbol{A = BC}</script>. How can we interpret this multiplication? Here are a few different interpretation methods:</p>

<ol>
  <li><strong>Individual Elements:</strong> We can calculate the value of individual elements of <script type="math/tex">\boldsymbol{A}</script> by taking the dot product of a row of <script type="math/tex">\boldsymbol{B}</script> with a column of <script type="math/tex">\boldsymbol{C}</script>. For example, to calculate the <script type="math/tex">{\boldsymbol{A}_{1,2}}</script> (first row, second column), we could take the dot product of the first row of <script type="math/tex">\boldsymbol{B}</script> and the second column of <script type="math/tex">\boldsymbol{C}</script>, calculated as <script type="math/tex">% <![CDATA[
{\boldsymbol{A}_{1,2}} = {\boldsymbol{B}_{1,:}}{\boldsymbol{C}_{:,1}} = \left[ {\begin{array}{*{20}{c}}1&3\end{array}} \right]\left[ {\begin{array}{*{20}{c}}7\\6\end{array}} \right] = 25 %]]></script>.</li>
  <li><strong>Combination of Columns:</strong> We can calculate the value of a column of <script type="math/tex">\boldsymbol{A}</script> by taking a linear combination of the columns of <script type="math/tex">\boldsymbol{B}</script>, where the combination coefficients come from a column of <script type="math/tex">\boldsymbol{C}</script>. For example, we can calculate the second column of <script type="math/tex">\boldsymbol{A}</script> as follows: <script type="math/tex">{\boldsymbol{A}_{:,2}} = \boldsymbol{B}{\boldsymbol{C}_{:,2}} = \left[ {\begin{array}{*{20}{c}}1\\5\end{array}} \right]7 + \left[ {\begin{array}{*{20}{c}}3\\2\end{array}} \right]6 = \left[ {\begin{array}{*{20}{c}}{25}\\{47}\end{array}} \right]</script>.</li>
  <li><strong>Combination of Rows:</strong> We can calculate the value of a row of <script type="math/tex">\boldsymbol{A}</script> by taking a linear combination of the rows of <script type="math/tex">\boldsymbol{C}</script>, where the combination coefficients come from a row of <script type="math/tex">\boldsymbol{B}</script>. For example, we can calculate the second row of <script type="math/tex">\boldsymbol{A}</script> as follows:<script type="math/tex">% <![CDATA[
{\boldsymbol{A}_{2,:}} = {\boldsymbol{B}_{2,:}}\boldsymbol{C} = 5\left[ {\begin{array}{*{20}{c}}4&7\end{array}} \right] + 2\left[ {\begin{array}{*{20}{c}}1&6\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}{22}&{47}\end{array}} \right] %]]></script>.</li>
  <li><strong>Sum of Rank-1 Matrices:</strong> The matrix <script type="math/tex">\boldsymbol{A}</script> can be expressed as a sum of the outer products of the columns of <script type="math/tex">\boldsymbol{B}</script> and the rows of <script type="math/tex">\boldsymbol{C}</script>. Therefore, we can write: <script type="math/tex">% <![CDATA[
\boldsymbol{A} = \left[ {\begin{array}{*{20}{c}}1\\5\end{array}} \right]\left[ {\begin{array}{*{20}{c}}4&7\end{array}} \right] + \left[ {\begin{array}{*{20}{c}}3\\2\end{array}} \right]\left[ {\begin{array}{*{20}{c}}1&6\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}7&{25}\\{22}&{47}\end{array}} \right] %]]></script>.</li>
  <li><strong>Decomposition into Blocks:</strong> If we partition the matrix <script type="math/tex">\boldsymbol{A}</script> into blocks and cut the matrices <script type="math/tex">\boldsymbol{B}</script> and <script type="math/tex">\boldsymbol{C}</script> into appropriately sized blocks, then we can view the multiplication in terms of the multiplication of individual block components. For example, suppose that <script type="math/tex">\boldsymbol{B}</script> and <script type="math/tex">\boldsymbol{C}</script> were large matrices. if we partition the matrices such that <script type="math/tex">% <![CDATA[
\boldsymbol{A} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{A}_1}}&{{\boldsymbol{A}_2}}\\{{\boldsymbol{A}_3}}&{{\boldsymbol{A}_4}}\end{array}} \right] %]]></script>, <script type="math/tex">% <![CDATA[
\boldsymbol{B} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{B}_1}}&{{\boldsymbol{B}_2}}\\{{\boldsymbol{B}_3}}&{{\boldsymbol{B}_4}}\end{array}} \right] %]]></script>, <script type="math/tex">% <![CDATA[
\boldsymbol{C} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{C}_1}}&{{\boldsymbol{C}_2}}\\{{\boldsymbol{C}_3}}&{{\boldsymbol{C}_4}}\end{array}} \right] %]]></script>, then we can write the equation for an individual block of <script type="math/tex">\boldsymbol{A}</script> as <script type="math/tex">{\boldsymbol{A}_1} = {\boldsymbol{B}_1}{\boldsymbol{C}_1} + {\boldsymbol{B}_2}{\boldsymbol{C}_3}</script>.</li>
</ol>

<p><a name="interpreting-the-matrix-inverse-"></a></p>

<p><br /></p>

<hr />
<h4 id="interpreting-the-matrix-inverse">Interpreting the Matrix Inverse</h4>
<hr />

<p>What is a matrix inverse? We can view the matrix product <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{y}</script> as transforming the vector <script type="math/tex">\boldsymbol{x}</script> into <script type="math/tex">\boldsymbol{y}</script>. Hence, the inverse of this transformation, called <script type="math/tex">{\boldsymbol{A}^{ - 1}}</script>, should be able to turn <script type="math/tex">\boldsymbol{y}</script> into <script type="math/tex">\boldsymbol{x}</script>, such that <script type="math/tex">{\boldsymbol{A}^{ - 1}}\boldsymbol{y = x}</script>. Combining the equations, we see that <script type="math/tex">{\boldsymbol{A}^{ - 1}}\boldsymbol{Ax} = \boldsymbol{x}</script>. Since this equation must hold for any <script type="math/tex">\boldsymbol{x}</script>, it follows that <script type="math/tex">{\boldsymbol{A}^{ - 1}}\boldsymbol{A}</script> must be the identity matrix such that and <script type="math/tex">{\boldsymbol{A}^{ - 1}}\boldsymbol{A} = \boldsymbol{I}</script>.</p>

<p>Suppose that we are trying to find the inverse of a square matrix <script type="math/tex">\boldsymbol{A}</script>. From the definition of the inverse, we must solve for the matrix <script type="math/tex">{\boldsymbol{A}^{ - 1}}</script> such that <script type="math/tex">\boldsymbol{A}{\boldsymbol{A}^{ - 1}} = \boldsymbol{I}</script>. We can interpret this matrix multiplication using the interpretability methods in the section described above. Specifically, we can consider how the columns of <script type="math/tex">\boldsymbol{A}{\boldsymbol{A}^{ - 1}}</script> can be written as a linear combination of the columns of <script type="math/tex">\boldsymbol{A}</script>. As the columns of <script type="math/tex">\boldsymbol{I}</script> are all independent, they must span the entire space. As such, it follows that the columns of <script type="math/tex">\boldsymbol{A}</script> must also be independent and span the entire space. Another way to interpret this is as follows, for <script type="math/tex">\boldsymbol{A}</script> to have an inverse, it must have the columns of <script type="math/tex">\boldsymbol{I}</script> in its column space. Similarly, we can write the matrix product <script type="math/tex">{\boldsymbol{A}^{ - 1}}\boldsymbol{A}</script> as a linear combination of the rows of <script type="math/tex">\boldsymbol{A}</script>. Thus, in a likewise manner, for <script type="math/tex">\boldsymbol{A}</script> to have an inverse, its row space must contain the rows of <script type="math/tex">\boldsymbol{I}</script>.</p>

<p>Another way to see when an inverse does not exist is to consider whether there exists a nonzero <script type="math/tex">\boldsymbol{x}</script> such that <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{0}</script>. If such an <script type="math/tex">\boldsymbol{x}</script> exists, then the matrix <script type="math/tex">\boldsymbol{A}</script> is not invertible. To see why, suppose that an inverse <script type="math/tex">{\boldsymbol{A}^{ - 1}}</script> did exist. Then it would be possible to write <script type="math/tex">{\boldsymbol{A}^{ - 1}}\boldsymbol{Ax = x}</script>, but we know that <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{0}</script> which implies that <script type="math/tex">{\boldsymbol{A}^{ - 1}}\boldsymbol{0} = \boldsymbol{x}</script> which is impossible for nonzero <script type="math/tex">\boldsymbol{x}</script>, therefore <script type="math/tex">{\boldsymbol{A}^{ - 1}}</script> cannot exist. To visualize this intuitively, when <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{0}</script>, we can think of <script type="math/tex">\boldsymbol{A}</script> as mapping points from the space onto the zero vector. The zero vector is like a black hole, once a vector enters it, no transformation will be able to get the original vector back.</p>

<p>Given an <script type="math/tex">n</script> by <script type="math/tex">n</script> square matrix <script type="math/tex">\boldsymbol{A}</script>, how can we solve for the inverse? From the definition, our inverse is defined by the equation<script type="math/tex">\boldsymbol{A}{\boldsymbol{A}^{ - 1}} = \boldsymbol{I}</script>. Hence, we have <script type="math/tex">{n^2}</script> equations with <script type="math/tex">{n^2}</script> unknowns (i.e. the unknowns are all elements in <script type="math/tex">{\boldsymbol{A}^{ - 1}}</script>). We can solve these equations using Gauss-Jordan elimination. We can view each elimination step as applying an elementary operation applied to the left side of <script type="math/tex">\boldsymbol{A}</script> such that we eventually convert <script type="math/tex">\boldsymbol{A}</script> into <script type="math/tex">\boldsymbol{I}</script>, with steps given as <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{A = I}</script>. Therefore, we see that <script type="math/tex">{\boldsymbol{A}^{ - 1}} = {\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}</script>. Notice that <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1} = {\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{I}</script>, so we can set up an augmented matrix where on the left side we determine the row operations to transform <script type="math/tex">\boldsymbol{A}</script> into <script type="math/tex">\boldsymbol{I}</script>, and on the right side we apply the same operations to <script type="math/tex">\boldsymbol{I}</script> to determine the inverse matrix <script type="math/tex">{\boldsymbol{A}^{ - 1}} = {\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1} = {\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{I}</script>.</p>

<p>Suppose that <script type="math/tex">\boldsymbol{A}</script> is a square matrix that has left inverse <script type="math/tex">\boldsymbol{L}</script> and right inverse <script type="math/tex">\boldsymbol{R}</script>. To show that <script type="math/tex">\boldsymbol{L = R}</script>, consider the following proof. Using a clever rearrangement of parentheses, we can write <script type="math/tex">\boldsymbol{L} = \boldsymbol{LI} = \boldsymbol{L}\left( {\boldsymbol{AR}} \right) = \left( {\boldsymbol{LA}} \right)\boldsymbol{R} = \boldsymbol{IR} = \boldsymbol{R}</script>. Notice that this inequality only holds when <script type="math/tex">\boldsymbol{A}</script> is square, otherwise matrix multiplication is not possible due to the differing matrix sizes.</p>

<p><a name="when-can-we-solve-$$\boldsymbol{ax}-=-\boldsymbol{b}$$?"></a></p>

<p><br /></p>

<hr />
<h4 id="when-can-we-solve-boldsymbolax--boldsymbolb">When can we solve <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>?</h4>
<hr />

<p>An important question in linear algebra is to determine whether <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> has a solution, and how we can describe these solutions. A critical tool to understand the solution space is to write the matrix <script type="math/tex">\boldsymbol{A}</script> in reduced row echelon form. We can write the matrix <script type="math/tex">\boldsymbol{A}</script> in reduced row echelon form by left multiplying by a series of invertible elementary row operations <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{Ax = Rx = }{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{b}</script>, where <script type="math/tex">\boldsymbol{R} = {\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{A}</script> is the reduced row echelon form of <script type="math/tex">\boldsymbol{A}</script>.</p>

<p>To assess whether the equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> can be solved for a given <script type="math/tex">\boldsymbol{b}</script>, we can examine the rows of <script type="math/tex">\boldsymbol{R}</script>. If there exists a row of <script type="math/tex">\boldsymbol{R}</script> that is all zeros, and the corresponding element in <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{b}</script> is nonzero, then the equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> has no solutions. We interpret this using our intuition for the row picture: every row equation in <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> describes a hyperplane. When we apply elementary row operations to this equation, we transform the hyperplanes, but preserve the set of points that lie on the intersection between hyperplanes used in the transformation. For example, when adding two rows together, the resulting hyperplane will preserve the set of points that lie on the intersection of the two hyperplanes being added. Hence, to produce a row of zeros in <script type="math/tex">\boldsymbol{R}</script>, two rows must be combined that correspond to the same hyperplane normal vector. For there to be solutions, the distance to these hyperplanes from the origin (found in <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{b}</script>) must be the same for both planes, otherwise they will not intersect, and no solutions exist.</p>

<p>If the equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> is solvable, then we can write the general set of solutions as<script type="math/tex">{\boldsymbol{x}_{complete}} = \left\{ {{\boldsymbol{x}_p} + {\boldsymbol{x}_n},{\boldsymbol{x}_n} \in N\left( \boldsymbol{A} \right)} \right\}</script>, where <script type="math/tex">{\boldsymbol{x}_p}</script>is a particular solution such that <script type="math/tex">\boldsymbol{A}{\boldsymbol{x}_p} = \boldsymbol{b}</script>, and <script type="math/tex">{\boldsymbol{x}_n}</script> is a generic vector in the nullspace of <script type="math/tex">\boldsymbol{A}</script> such that <script type="math/tex">\boldsymbol{A}{\boldsymbol{x}_n} = \boldsymbol{0}</script>. To show that <script type="math/tex">{\boldsymbol{x}_{complete}}</script> contains all solutions to <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>, let us consider any arbitrary solution <script type="math/tex">{\boldsymbol{y}^*}</script>. If we calculate the matrix product of <script type="math/tex">\boldsymbol{A}</script> with the difference between <script type="math/tex">{\boldsymbol{y}^*}</script> and <script type="math/tex">{\boldsymbol{x}_p}</script>, we get <script type="math/tex">\boldsymbol{A}\left( {{\boldsymbol{y}^*} - {\boldsymbol{x}_p}} \right) = \boldsymbol{A}{\boldsymbol{y}^*} - \boldsymbol{A}{\boldsymbol{x}_p} = \boldsymbol{b} - \boldsymbol{b} = \boldsymbol{0}</script>. This implies that <script type="math/tex">{\boldsymbol{y}^*} - {\boldsymbol{x}_p}</script> is in the nullspace of <script type="math/tex">\boldsymbol{A}</script> such that <script type="math/tex">{\boldsymbol{y}^*} - {\boldsymbol{x}_p} \in N\left( \boldsymbol{A} \right)</script>. Since <script type="math/tex">{\boldsymbol{y}^*} - {\boldsymbol{x}_p} \in N\left( \boldsymbol{A} \right)</script>, we can let <script type="math/tex">{\boldsymbol{x}_n} = {\boldsymbol{y}^*} - {\boldsymbol{x}_p}</script> and write <script type="math/tex">{\boldsymbol{x}_p} + {\boldsymbol{x}_n} = {\boldsymbol{x}_p} + {\boldsymbol{y}^*} - {\boldsymbol{x}_p} = {\boldsymbol{y}^*}</script>. Therefore, <script type="math/tex">{\boldsymbol{y}^*} = {\boldsymbol{x}_p} + {\boldsymbol{x}_n} \in {\boldsymbol{x}_{complete}}</script> and <script type="math/tex">{\boldsymbol{x}_{complete}}</script> contains all solutions to <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>.  To gain intuition behind this, we can visualize the solution space as being the intersection of hyperplanes which is a linear manifold. The points on this linear manifold can be described by the combination of a point on the linear manifold (<script type="math/tex">{\boldsymbol{x}_p}</script>), and the direction vectors which are parallel to the linear manifold (the nullspace vectors <script type="math/tex">{\boldsymbol{x}_n}</script>).</p>

<p>Hence to find all solutions to <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>, we need to find: 1) a particular solution <script type="math/tex">{\boldsymbol{x}_p}</script>, and 2) a basis for the nullspace <script type="math/tex">N\left( \boldsymbol{A} \right)</script>. Since any particular solution will do, one way to find a particular solution is to take the reduced row echelon matrix <script type="math/tex">\boldsymbol{R}</script>, set the free variables in <script type="math/tex">\boldsymbol{x}</script> to 0, and solve for the pivot variables to satisfy <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>.</p>

<p>To find the nullspace, we need to find the solutions to the equation <script type="math/tex">\boldsymbol{Rx} = \boldsymbol{0}</script>. One way to do this is to capitalize upon the structure of the reduced row echelon matrix. We start by exchanging some columns in <script type="math/tex">\boldsymbol{R}</script> so that the pivot columns are clustered together on the left side of the matrix. We can write <script type="math/tex">\boldsymbol{R}</script> as a block matrix such that <script type="math/tex">% <![CDATA[
\boldsymbol{R = }\left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_r}}&\boldsymbol{F}\\\boldsymbol{0}&\boldsymbol{0}\end{array}} \right] %]]></script> where <script type="math/tex">\boldsymbol{F}</script>is the set of coefficients associated with the free variables and <script type="math/tex">{\boldsymbol{I}_r}</script> is an identity matrix of size <script type="math/tex">r</script> by <script type="math/tex">r</script> where <script type="math/tex">r</script> is the rank of <script type="math/tex">\boldsymbol{A}</script>. To find vectors for the nullspace, we need to solve the equation <script type="math/tex">\boldsymbol{Rx = 0}</script>. Using the block representation for <script type="math/tex">\boldsymbol{R}</script>, we can solve for a basis of the nullspace by recognizing that <script type="math/tex">% <![CDATA[
\boldsymbol{RN} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_r}}&\boldsymbol{F}\\\boldsymbol{0}&\boldsymbol{0}\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{ - \boldsymbol{F}}\\{{\boldsymbol{I}_{n - r}}}\end{array}} \right] = \boldsymbol{0} %]]></script> where <script type="math/tex">{\boldsymbol{I}_{n - r}}</script> is an identity matrix of size <script type="math/tex">n - r</script> by <script type="math/tex">n - r</script>. Therefore, the columns of the nullspace matrix <script type="math/tex">\boldsymbol{N} = \left[ {\begin{array}{*{20}{c}}{ - \boldsymbol{F}}\\{{\boldsymbol{I}_{n - r}}}\end{array}} \right]</script> provide a basis for the nullspace of <script type="math/tex">\boldsymbol{A}</script>. To summarize the operations done: we are trying to solve for the solutions to <script type="math/tex">\boldsymbol{Ax = 0}</script>. We can apply elementary row operations by left multiplying both sides of the equation with the elementary matrices such that <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{Ax} = {\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{0} = \boldsymbol{0}</script>. Therefore, we see that applying row operations (that are invertible) does not change the nullspace since <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{Ax} = \boldsymbol{0}</script>. Finally, we can right multiply the matrix with permutation matrix <script type="math/tex">\boldsymbol{Q}</script> to permute the columns, written as<script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{AQ}{\boldsymbol{Q}^{ - 1}}\boldsymbol{x} = \boldsymbol{R}{\boldsymbol{Q}^{ - 1}}\boldsymbol{x} = \boldsymbol{Ry} = \boldsymbol{0}</script>, where the final reduced row echelon matrix is <script type="math/tex">\boldsymbol{R} = {\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{AQ}</script>, and the permuted vector <script type="math/tex">\boldsymbol{x}</script> is written as <script type="math/tex">\boldsymbol{y} = {\boldsymbol{Q}^{ - 1}}\boldsymbol{x}</script>. The permutation ensures that the pivot variable columns are on the left side of <script type="math/tex">\boldsymbol{R}</script> and the free variable columns are on the right side of <script type="math/tex">\boldsymbol{R}</script>.</p>

<p>We can predict the type of solutions possible for the equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> by looking at the rank of <script type="math/tex">\boldsymbol{A}</script> which is an <script type="math/tex">m</script> by <script type="math/tex">n</script> matrix with rank <script type="math/tex">r</script>. Here we define <script type="math/tex">\boldsymbol{R}</script> to be the reduced row echelon form of <script type="math/tex">\boldsymbol{A}</script> with the pivot columns located to the left of the matrix. We have the following four potentials for the rank of <script type="math/tex">\boldsymbol{A}</script>.</p>

<ol>
  <li><script type="math/tex">r = m = n</script>: In this case, <script type="math/tex">\boldsymbol{R} = \boldsymbol{I}</script> since each row and column have a pivot. <script type="math/tex">\boldsymbol{A}</script> is invertible, and we have a unique solution to <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>. To see why <script type="math/tex">\boldsymbol{A}</script> is invertible. Recall that to construct <script type="math/tex">\boldsymbol{R}</script> we apply a series of invertible elementary row operations to <script type="math/tex">\boldsymbol{A}</script> such that <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{A} = \boldsymbol{R} = \boldsymbol{I}</script>. Therefore, we see that <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1} = {\boldsymbol{A}^{ - 1}}</script></li>
  <li><script type="math/tex">% <![CDATA[
r = n < m %]]></script>: In this case, <script type="math/tex">\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}\boldsymbol{I}\\\boldsymbol{0}\end{array}} \right]</script>. Since we have no free variables, the nullspace only has the zero vector. Therefore, depending on the value of <script type="math/tex">\boldsymbol{b}</script> we will either have 0 or 1 solution to the equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script></li>
  <li><script type="math/tex">% <![CDATA[
r = m < n %]]></script>: In this case, <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}\boldsymbol{I}&\boldsymbol{F}\end{array}} \right] %]]></script>. Each row has a pivot, so we will always have a solution. Furthermore, we have free variables, therefore, the nullspace is a linear manifold and we will have infinitely many solutions</li>
  <li><script type="math/tex">% <![CDATA[
r < m,\,r < n %]]></script>: In this case, <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}\boldsymbol{I}&\boldsymbol{F}\\\boldsymbol{0}&\boldsymbol{0}\end{array}} \right] %]]></script> and we have free variables. Therefore, the nullspace is a linear manifold. Depending on the value of <script type="math/tex">\boldsymbol{b}</script> we will either have 0 or infinitely many solutions to the equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script></li>
</ol>

<p>For <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> to be solvable, we have the following interpretations:</p>
<ol>
  <li>The vector <script type="math/tex">\boldsymbol{b}</script> must be in the column space of <script type="math/tex">\boldsymbol{A}</script>.</li>
  <li>We can write the reduced row echelon form of <script type="math/tex">\boldsymbol{A}</script> , denoted by <script type="math/tex">\boldsymbol{R}</script>, by left multiplying both sides of the equation with elementary row operations such that<script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{Ax} = \boldsymbol{Rx} = {\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{b}</script>. Hence, we have the transformed matrix equation <script type="math/tex">\boldsymbol{Rx} = {\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{b}</script>. If <script type="math/tex">\boldsymbol{R}</script> contains a row of zeros, then the matching entry in the transformed <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{b}</script> vector must also be 0. If not, then no solution exists. We can interpret this using our intuition from the row picture of matrix multiplication. Each row equation of <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> represents a hyperplane. When we perform an elementary row operation <script type="math/tex">{\boldsymbol{E}_k}</script> that adds a multiple of one row to another, we are transforming one of the hyperplanes into a new hyperplane while preserving the points that lie in the intersection of the hyperplanes represented by the two original rows. In other words, adding two rows together effectively constructs a new hyperplane that is rotated around the intersection set (in <script type="math/tex">{\mathbb{R}^2}</script> the intersection set would be a point, and in <script type="math/tex">{\mathbb{R}^3}</script> the intersection set would be a line). Hence, applying an elementary row operation modifies the hyperplane geometry but does not change the solution space (i.e. the set of intersections between all hyperplanes). Therefore, if we apply an elementary row operation that “adds a multiple of one row to another”, and this operation produces a row of zeros, then for a solution to exist, it must be that these two rows represent the same hyperplane, and therefore, the distance to the origin must be the same or else there is no intersections, and thereby, no solutions. Hence, when we subtract the distances to the origin for both rows (i.e. the corresponding values of the <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{b}</script> vector), the value must be 0. We can view each invertible matrix product as an operation that transforms the hyperplanes in our equation while preserving the set of intersection points between the hyperplanes that are combined.</li>
</ol>

<p><a name="how-can-we-solve-$$\boldsymbol{ax}-=-\boldsymbol{b}$$?"></a></p>

<p><br /></p>

<hr />
<h4 id="how-can-we-solve-boldsymbolax--boldsymbolb">How can we solve <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>?</h4>
<hr />

<p>Suppose we have an <script type="math/tex">m</script> by <script type="math/tex">n</script> matrix <script type="math/tex">\boldsymbol{A}</script> where <script type="math/tex">% <![CDATA[
m < n %]]></script>. How do we quickly show that for any solvable equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>, we must have an infinite number of solutions? One way is to recognize this result is to use the structure of the reduced row echelon form. If we put the matrix <script type="math/tex">\boldsymbol{A}</script> in reduced row echelon form with the pivot columns on the left side, then it follows that the number of pivots must be less than or equal to the smaller dimension <script type="math/tex">m</script>. To understand why, remember that to compute the reduced row echelon form, we compute the following algorithm</p>

<ol>
  <li>Start with the element in the first row and first column. If this element is zero, then try to swap rows to make it nonzero. If the entire first row is zeros, then swap columns to make it nonzero.</li>
  <li>Scale the first row to make the element in the first row, first column equal to 1</li>
  <li>Add a multiple of the first row to all other rows to make the element in the first column equal 0. At this point, the only nonzero element should be the element in the first row, first column</li>
  <li>Now proceed to complete steps 1-3 for the second row, second column, etc. Continue until there are no more nonzero entries in the remaining rows</li>
  <li>The total computation of the reduced row echelon form for <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> can be written as <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{A}\left( {{\boldsymbol{Q}_1} \cdots {\boldsymbol{Q}_N}} \right)\left( {\boldsymbol{Q}_N^{ - 1} \cdots \boldsymbol{Q}_1^{ - 1}} \right)\boldsymbol{x} = {\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{b}</script> where <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}</script> are the elementary row operations (i.e. scaling rows, swapping rows, adding a multiple of one row to another), and <script type="math/tex">{\boldsymbol{Q}_1} \cdots {\boldsymbol{Q}_N}</script> represent column permutation matrices used to swap the order of the columns. Recall that the inverse of a permutation matrix is the matrix itself. To understand this, consider that the inverse of swapping two columns is to simply apply the same operation again to swap the columns back. Hence by left multiplying <script type="math/tex">\boldsymbol{x}</script> with <script type="math/tex">\boldsymbol{Q}_N^{ - 1} \cdots \boldsymbol{Q}_1^{ - 1}</script>, we are effectively performing row permutations on <script type="math/tex">\boldsymbol{x}</script> so that whatever columns we swapped in <script type="math/tex">\boldsymbol{A}</script>, we swap the same rows in <script type="math/tex">\boldsymbol{x}</script> so that the result of the matrix product is unaffected as can be seen since <script type="math/tex">\boldsymbol{A}\left( {{\boldsymbol{Q}_1} \cdots {\boldsymbol{Q}_N}} \right)\left( {\boldsymbol{Q}_N^{ - 1} \cdots \boldsymbol{Q}_1^{ - 1}} \right)\boldsymbol{x} = \boldsymbol{AIx = Ax}</script>. Looking back at steps 1-3 described above, we see that during the computation of the reduced row echelon form for <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>, we will apply a sequence of row and column operations. As a hypothetical example, we may swap two rows (represented by <script type="math/tex">{\boldsymbol{E}_1}</script>), then add a multiple of one row to another (represented by <script type="math/tex">{\boldsymbol{E}_2}</script>), then swap two columns (represented by <script type="math/tex">{\boldsymbol{Q}_1}</script>), then add a multiple of one row to another (represented by <script type="math/tex">{\boldsymbol{E}_3}</script>), etc. The key point is that it is intuitive to think of the row operations and column swaps as being intermingled (i.e. the order from the described example was <script type="math/tex">{\boldsymbol{E}_1},{\boldsymbol{E}_2},{\boldsymbol{Q}_1},{\boldsymbol{E}_3}</script>). Due to the matrix commutativity property, we can achieve this interpretation because we are “free” to compute the multiplication in the order that makes intuitive sense. To make this concrete when computing <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{A}{\boldsymbol{Q}_1} \cdots {\boldsymbol{Q}_N}</script>, the commutative properties tell us that we are free to put parentheses wherever we want. Hence, to represent the sequence described in the example above we have <script type="math/tex">{\boldsymbol{E}_M} \cdots \left( {{\boldsymbol{E}_3}\left( {\left( {{\boldsymbol{E}_2}\left( {{\boldsymbol{E}_1}\boldsymbol{A}} \right)} \right){\boldsymbol{Q}_1}} \right)} \right) \cdots {\boldsymbol{Q}_N}</script>. In this case we see that the order of multiplications is <script type="math/tex">{\boldsymbol{E}_1},{\boldsymbol{E}_2},{\boldsymbol{Q}_1},{\boldsymbol{E}_3}</script> which matches the example.</li>
  <li>The final form of the reduced row echelon matrix produced by the above steps is a matrix with the general form <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}&{{\boldsymbol{F}_{r,n - r}}}\\{{\boldsymbol{0}_{m - r,r}}}&{{\boldsymbol{0}_{m - r,n - r}}}\end{array}} \right] %]]></script>, where <script type="math/tex">r</script> is the rank and is defined as the number of pivots. The subscripts of the block matrices in <script type="math/tex">\boldsymbol{R}</script> denote the row by column size of each of the blocks. It is important to note that depending on the shape and rank of the matrix, some of these blocks may be empty. For example, if <script type="math/tex">% <![CDATA[
r = n < m %]]></script> then the matrix reduces down to <script type="math/tex">\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{I}_{r,r}}}\\{{\boldsymbol{0}_{m - r,r}}}\end{array}} \right]</script>. The block denoted by <script type="math/tex">{\boldsymbol{F}_{r,n - r}}</script> refers to the coefficients for the non-pivot variables, called the “free variables”. Note that the partitioning of the variables into “free” vs. “pivot” is somewhat arbitrary because we could simply apply a column swap to turn a “free” variable into a “pivot” variable. Therefore, what is most important is the final block partitioned form of the matrix and the fact that we can write any matrix in the reduced row echelon form using the steps described above.</li>
</ol>

<p>Returning to the central question: how can we solve <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>? One method is as follows:</p>
<ol>
  <li>Convert the system into reduced row echelon form</li>
  <li>We can find a particular solution by setting the free variables to 0 and solve for the pivot variables</li>
  <li>Next, we solve for the nullspace basis vectors by setting the free variables to the standard basis and solving for the pivot variables</li>
  <li>Combine the particular solution and the linear combinations of the nullspace basis vectors to form the complete solution</li>
</ol>

<p>Let us discuss the intuition for the method described above. We can visualize the solution space to <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> as being a linear manifold where the particular solution is a point on the linear manifold, and the nullspace basis describes a set of direction vectors that are parallel to the linear manifold. Hence, the particular solution plus a linear combination of the nullspace basis vectors defines all vectors on the linear manifold and ensures that <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>.</p>

<p>To show that any solution to <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> can be written in terms of a particular solution <script type="math/tex">{\boldsymbol{x}^*}</script> and vectors in the nullspace, let us postulate the potential of another solution <script type="math/tex">{\boldsymbol{y}^*}</script> that cannot be written in terms of <script type="math/tex">{\boldsymbol{x}^*}</script> and the vectors in the nullspace. If we take the difference between <script type="math/tex">{\boldsymbol{y}^*}</script> and <script type="math/tex">{\boldsymbol{x}^*}</script> we can write <script type="math/tex">\boldsymbol{A}\left( {{\boldsymbol{y}^*} - {\boldsymbol{x}^*}} \right) = \boldsymbol{A}{\boldsymbol{y}^*} - \boldsymbol{A}{\boldsymbol{x}^*} = \boldsymbol{b} - \boldsymbol{b} = \boldsymbol{0}</script>. Therefore, we see that <script type="math/tex">{\boldsymbol{y}^*} - {\boldsymbol{x}^*}</script> is in the nullspace of <script type="math/tex">\boldsymbol{A}</script>, and we can write the solution <script type="math/tex">{\boldsymbol{y}^*}</script> in terms of <script type="math/tex">{\boldsymbol{x}^*}</script> and the vectors in the nullspace as <script type="math/tex">{\boldsymbol{y}^*} = \left( {{\boldsymbol{y}^*} - {\boldsymbol{x}^*}} \right) + {\boldsymbol{x}^*}</script>. Hence any solution of <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> can be written in terms of a particular solution <script type="math/tex">{\boldsymbol{x}^*}</script> and vectors in the nullspace.</p>

<p><a name="least-squares"></a></p>

<p><br /></p>

<hr />
<h4 id="least-squares">Least Squares</h4>
<hr />

<p>In many practical situations we have noisy measurements with many measurements and few parameters. In this case, the chance of having the solution in the column space is very unlikely. Hence the equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script> will not have a solution. If we cannot find a solution to the equation <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>,  then an alternative approach is to find the solution that minimizes the error <script type="math/tex">{\left\| {\boldsymbol{b} - \boldsymbol{Ax}} \right\|^2} = {\left\| \boldsymbol{e} \right\|^2}</script>. This can be interpreted as finding the point in the column space of <script type="math/tex">\boldsymbol{A}</script> that is closest to <script type="math/tex">\boldsymbol{b}</script>. Recall that we can visualize the column space as a linear manifold. The displacement from a linear manifold to a point in space can be written as a combination of a component parallel to the manifold, and a component perpendicular to the manifold. Hence, the distance from a linear manifold to a point in space can be minimized by finding the point on the linear manifold such that the displacement only consists of the perpendicular component. For linear manifolds, this will be a unique point in space. We can find this point by solving for the <script type="math/tex">\boldsymbol{x}</script> which produces a displacement vector <script type="math/tex">\boldsymbol{b} - \boldsymbol{Ax}</script> that is orthogonal to the column space. For a vector to be orthogonal to the column space, it must be orthogonal to all columns in <script type="math/tex">\boldsymbol{A}</script>. Therefore, it follows that <script type="math/tex">{\boldsymbol{A}^T}\left( {\boldsymbol{b} - \boldsymbol{Ax}} \right) = \boldsymbol{0}</script>. Using the QR factorization of <script type="math/tex">\boldsymbol{A}</script> we can write the equation as</p>

<script type="math/tex; mode=display">\begin{array}{l}{\boldsymbol{A}^T}\left( {\boldsymbol{b} - \boldsymbol{Ax}} \right) = \boldsymbol{0}\\ \Rightarrow {\left( {\boldsymbol{QR}} \right)^T}\left( {\boldsymbol{b} - \boldsymbol{QRx}} \right) = \boldsymbol{0}\\ \Rightarrow {\boldsymbol{R}^T}\boldsymbol{Rx} = {\boldsymbol{R}^T}{\boldsymbol{Q}^T}\boldsymbol{b}\end{array}</script>

<p>To simplify this further, we would like to multiply both sides by <script type="math/tex">{\left( {{\boldsymbol{R}^T}} \right)^{ - 1}}</script>. However, we must first show that this inverse exists. The nullspace is defined by solutions to <script type="math/tex">\boldsymbol{Ax = 0}</script>. Substituting in the QR factorization we get <script type="math/tex">\boldsymbol{Ax = 0} \Rightarrow \boldsymbol{QRx = 0}</script>. Since the columns of <script type="math/tex">\boldsymbol{Q}</script> are orthonormal, it follows that the nullspace of <script type="math/tex">\boldsymbol{Q}</script> is only the zero vector. Hence, the nullspace of <script type="math/tex">\boldsymbol{QR}</script> must be equal to the nullspace of <script type="math/tex">\boldsymbol{R}</script> which in turn is equal to the nullspace of <script type="math/tex">\boldsymbol{A}</script>. If <script type="math/tex">\boldsymbol{A}</script> has full column rank (i.e. all its columns are independent) which is typically the case for tall skinny data matrices, then the nullspace of <script type="math/tex">\boldsymbol{A}</script> will only be the zero vector, which implies that the nullspace of <script type="math/tex">\boldsymbol{R}</script> will only be the zero vector. Hence, when <script type="math/tex">\boldsymbol{A}</script> has full column rank then <script type="math/tex">{\left( {{\boldsymbol{R}^T}} \right)^{ - 1}}</script> exists and we can write the equation from above as <script type="math/tex">\boldsymbol{Rx} = {\boldsymbol{Q}^T}\boldsymbol{b}</script>. Solving this equation will give us the least squares solution to the equation <script type="math/tex">\boldsymbol{Ax = b}</script>.</p>

<p><a name="determinants"></a></p>

<p><br /></p>

<hr />
<h2 id="determinants">Determinants</h2>
<hr />

<p>Every square matrix has a specific number called the determinant which encodes information about the matrix. One useful interpretation of the determinant is as the volume of the parallelepiped described by the vectors of the matrix (either the rows or columns). The determinant can be used to find a closed form expression for the inverse of a matrix. Cramer’s Rule uses this expression for the inverse to produce a closed form expression for a solution to a system of linear equations.</p>

<p>Instead of simply writing the formula for the determinant, it is useful to describe the determinant in terms of its properties, and then use these properties to define the formula. Four axiomatic properties which form the basis for the determinant are the following:</p>

<ol>
  <li>The determinant of the identity matrix is <script type="math/tex">\det \boldsymbol{I = }1</script></li>
  <li>If you exchange two rows of a matrix, you reverse the sign of its determinant (i.e. positive to negative or vice versa)</li>
  <li>The determinant behaves like a linear function on the rows of the matrix <script type="math/tex">% <![CDATA[
\left\lvert  {\begin{array}{*{20}{c}}{a + a'}&{b + b'}\\c&d\end{array}} \right\rvert  = \left\lvert  {\begin{array}{*{20}{c}}a&b\\c&d\end{array}} \right\rvert  + \left\lvert  {\begin{array}{*{20}{c}}{a'}&{b'}\\c&d\end{array}} \right\rvert %]]></script></li>
  <li>If we multiply one row of a matrix by <script type="math/tex">t</script>, then the determinant is multiplied by <script type="math/tex">t</script> such that: <script type="math/tex">% <![CDATA[
\left\lvert  {\begin{array}{*{20}{c}}{ta}&{tb}\\c&d\end{array}} \right\rvert  = t\left\lvert  {\begin{array}{*{20}{c}}a&b\\c&d\end{array}} \right\rvert %]]></script></li>
</ol>

<p>From these four properties we can deduce all other properties of the determinant. Some of these properties include the following</p>

<ul>
  <li><strong>Two Rows Equal:</strong> If two rows of a matrix are equal then the determinant is 0. To see this, suppose that we swap the two equal rows, then we can apply property 2 to see that the determinant must change sign. However, the elements of the swapped matrix have not changed since the two swapped rows are equal. Therefore, the determinant must be 0.</li>
  <li><strong>Adding a Multiple of a Row:</strong> If we add a multiple of one row to another row, the determinant does not change. To see this, suppose we add a multiple of one row to another row. We can use property 3 to separate out the components of the first row, and then apply property 4 and the <strong>Two Rows Equal:</strong> properties to show the following<script type="math/tex">% <![CDATA[
\left\lvert  {\begin{array}{*{20}{c}}{a + tc}&{b + td}\\c&d\end{array}} \right\rvert  = \left\lvert  {\begin{array}{*{20}{c}}a&b\\c&d\end{array}} \right\rvert  + \left\lvert  {\begin{array}{*{20}{c}}{tc}&{td}\\c&d\end{array}} \right\rvert  = \left\lvert  {\begin{array}{*{20}{c}}a&b\\c&d\end{array}} \right\rvert  + t\left\lvert  {\begin{array}{*{20}{c}}c&d\\c&d\end{array}} \right\rvert  = \left\lvert  {\begin{array}{*{20}{c}}a&b\\c&d\end{array}} \right\rvert %]]></script></li>
  <li><strong>Zero Row:</strong> If one of the rows is zero, then the determinant of the matrix is zero. To see this, apply property 4 where <script type="math/tex">t = 0</script>.</li>
  <li><strong>Triangular Matrix:</strong> The determinant of a triangular matrix is the product of its diagonal entries <script type="math/tex">{d_1}{d_2} \cdots {d_n}</script>. To see this, we can use the <strong>Adding a Multiple of a Row</strong> property to convert the triangular matrix into a diagonal matrix without changing the determinant. Then property 4 tells us that the determinant of the diagonal matrix is equal to the product of the diagonal entries <script type="math/tex">{d_1}{d_2} \cdots {d_n}</script> times the determinant of the identity matrix. From property 1 we know that the determinant of the identity matrix is 1. Note that if one of the <script type="math/tex">{d_k}</script> is zero, then we can apply the <strong>Adding a Multiple of a Row:</strong> property to produce a row of zeros, and finally the <strong>Zero Row</strong> property to show that the determinant for this matrix is 0. Hence the determinant for any triangular matrix is <script type="math/tex">{d_1}{d_2} \cdots {d_n}</script>.</li>
  <li><strong>Singular Matrix:</strong> If a matrix is singular, then its determinant will be zero. To see why, note that we can perform row operations on a singular matrix to produce a row of zeros. Therefore, from the <strong>Zero Row:</strong> property, the determinant will be 0.</li>
  <li><strong>Product Rule:</strong> For any square matrices we have <script type="math/tex">\left( {\det \boldsymbol{A}} \right)\left( {\det \boldsymbol{B}} \right) = \det \boldsymbol{AB}</script>. This is harder to prove. See <a href="https://proofwiki.org/wiki/Determinant_of_Matrix_Product">here</a>. A result from this property is that <script type="math/tex">1 = \det \boldsymbol{I = }\det \boldsymbol{A}{\boldsymbol{A}^{ - 1}} = \left( {\det \boldsymbol{A}} \right)\left( {\det {\boldsymbol{A}^{ - 1}}} \right)</script>, therefore <script type="math/tex">\det \boldsymbol{A} = \frac{1}{{\det {\boldsymbol{A}^{ - 1}}}}</script>.</li>
  <li><strong>Transpose Rule:</strong> For any square matrix, <script type="math/tex">\det {\boldsymbol{A}^T} = \det \boldsymbol{A}</script>. To see why, use elimination to produce the LU factorization such that <script type="math/tex">\boldsymbol{A} = \boldsymbol{LU}</script>. Now taking the determinant of both sides gives <script type="math/tex">\det \boldsymbol{A} = \det \boldsymbol{LU} = \left( {\det \boldsymbol{L}} \right)\left( {\det \boldsymbol{U}} \right)</script>. Because <script type="math/tex">\boldsymbol{L}</script> has 1’s on the diagonal, then <script type="math/tex">\det \boldsymbol{L} = \det {\boldsymbol{L}^T} = 1</script>. Likewise, since <script type="math/tex">\boldsymbol{U}</script> is an upper triangular matrix, then the determinant is the product of the diagonals and since the diagonals stay the same for the transpose, we have <script type="math/tex">\det \boldsymbol{U} = \det {\boldsymbol{U}^T}</script>. Hence, we see that <script type="math/tex">\det \boldsymbol{A} = \left( {\det \boldsymbol{L}} \right)\left( {\det \boldsymbol{U}} \right) = \left( {\det {\boldsymbol{U}^T}} \right)\left( {\det {\boldsymbol{L}^T}} \right) = \det {\boldsymbol{A}^T}</script></li>
</ul>

<p>To understand how we can compute the determinant of a matrix, let us take the example of a 2 by 2 matrix</p>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{A} = \left[ {\begin{array}{*{20}{c}}a&b\\c&d\end{array}} \right] %]]></script>

<p>By applying property 3 we can write</p>

<script type="math/tex; mode=display">% <![CDATA[
\left\lvert  {\begin{array}{*{20}{c}}a&b\\c&d\end{array}} \right\rvert  = \left\lvert  {\begin{array}{*{20}{c}}0&b\\c&d\end{array}} \right\rvert  + \left\lvert  {\begin{array}{*{20}{c}}a&0\\c&d\end{array}} \right\rvert  = \left\lvert  {\begin{array}{*{20}{c}}a&0\\0&d\end{array}} \right\rvert  + \left\lvert  {\begin{array}{*{20}{c}}a&0\\c&0\end{array}} \right\rvert  + \left\lvert  {\begin{array}{*{20}{c}}0&b\\0&d\end{array}} \right\rvert  + \left\lvert  {\begin{array}{*{20}{c}}0&b\\c&0\end{array}} \right\rvert %]]></script>

<p>Notice that for the determinants with a column filled with zeros, we can add a multiple of one row to another which would create a row of zeros. From the <strong>Zero Row</strong> property, we know that the value of these determinants is 0. Hence, we can write</p>

<p><script type="math/tex">% <![CDATA[
\left\lvert  {\begin{array}{*{20}{c}}a&b\\c&d\end{array}} \right\rvert  = \left\lvert  {\begin{array}{*{20}{c}}a&0\\0&d\end{array}} \right\rvert  + \left\lvert  {\begin{array}{*{20}{c}}0&b\\c&0\end{array}} \right\rvert %]]></script>
From property 4 and property 1 we see that <script type="math/tex">% <![CDATA[
\left\lvert  {\begin{array}{*{20}{c}}a&0\\0&d\end{array}} \right\rvert  = ad\left\lvert  {\begin{array}{*{20}{c}}1&0\\0&1\end{array}} \right\rvert  = ad %]]></script> and from properties 1, 2, and 4 we see that <script type="math/tex">% <![CDATA[
\left\lvert  {\begin{array}{*{20}{c}}0&b\\c&0\end{array}} \right\rvert  =  - \left\lvert  {\begin{array}{*{20}{c}}c&0\\0&b\end{array}} \right\rvert  =  - cb\left\lvert  {\begin{array}{*{20}{c}}1&0\\0&1\end{array}} \right\rvert  =  - cb %]]></script>. Finally, we can combine these together to show that <script type="math/tex">% <![CDATA[
\left\lvert  {\begin{array}{*{20}{c}}a&b\\c&d\end{array}} \right\rvert  = ad - bc %]]></script>.</p>

<p>To calculate the determinants of higher dimensional matrices, we can extend the principal used in the 2 by 2 case, where we used property 3 to expand each row so that there is only one nonzero entry in each row. For the 2 by 2 case shown above, this was written as</p>

<script type="math/tex; mode=display">% <![CDATA[
\left\lvert  {\begin{array}{*{20}{c}}a&b\\c&d\end{array}} \right\rvert  = \left\lvert  {\begin{array}{*{20}{c}}0&b\\c&d\end{array}} \right\rvert  + \left\lvert  {\begin{array}{*{20}{c}}a&0\\c&d\end{array}} \right\rvert  = \left\lvert  {\begin{array}{*{20}{c}}a&0\\0&d\end{array}} \right\rvert  + \left\lvert  {\begin{array}{*{20}{c}}a&0\\c&0\end{array}} \right\rvert  + \left\lvert  {\begin{array}{*{20}{c}}0&b\\0&d\end{array}} \right\rvert  + \left\lvert  {\begin{array}{*{20}{c}}0&b\\c&0\end{array}} \right\rvert %]]></script>

<p>Notice that if we expand the determinant of an <script type="math/tex">n</script> by <script type="math/tex">n</script> matrix in this way, then the number of “sub” determinants will be <script type="math/tex">{n^n}</script>. Thankfully, we can simplify this calculation since any determinant where two rows have the same column index that is nonzero will be zero (to see why, if two such rows exist, then we can add a multiple of one row to the other to create a row of zeros which means the determinant is zero). In this case the number of “sub” determinants reduces from <script type="math/tex">{n^n}</script> to <script type="math/tex">n!</script>. To see why it is <script type="math/tex">n!</script> we can ask how each “sub” determinant is constructed. We can construct a “sub” determinant by choosing which elements will be nonzero. We can think of this process as picking any one of the <script type="math/tex">n</script> elements in the first row, and then choosing one of the <script type="math/tex">n - 1</script> remaining columns in the second row, etc. Therefore, we have <script type="math/tex">n!</script> possible “sub” determinants of this form.</p>

<p>From this factorization, we see that if an element in row <script type="math/tex">i</script> and column <script type="math/tex">j</script> is nonzero, then every other element in row <script type="math/tex">i</script> and column <script type="math/tex">j</script> must be zero for the determinant to be nonzero. Hence, we can pose the determinant calculation as a recursive algorithm where we compute the determinant using the summation of cofactors which are the determinants of submatrices. To see more detail on the explicit calculation of the determinant, see <a href="https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/determinant-formulas-and-cofactors/">here</a>.</p>

<p><a name="eigenvalues-and-eigenvectors"></a></p>

<p><br /></p>

<hr />
<h2 id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</h2>
<hr />

<p>We can think about a square matrix <script type="math/tex">\boldsymbol{A}</script> as a function that transforms a vector <script type="math/tex">\boldsymbol{x}</script> into a vector <script type="math/tex">\boldsymbol{Ax}</script>. The vectors which preserve their direction after the transformation (i.e. <script type="math/tex">\boldsymbol{x}</script> is parallel to <script type="math/tex">\boldsymbol{Ax}</script>) are called the eigenvectors of <script type="math/tex">\boldsymbol{A}</script>. Therefore, the eigenvectors satisfy <script type="math/tex">\boldsymbol{Ax = }\lambda \boldsymbol{x}</script>. If <script type="math/tex">\boldsymbol{A}</script> is singular, then <script type="math/tex">\lambda  = 0</script> is an eigenvalue.</p>

<p>What is the importance of eigenvectors/eigenvalues? Here are some key insights:</p>
<ul>
  <li><strong>Linear Transformations:</strong> Recall that we can view a matrix as representing a linear transformation <script type="math/tex">f:V \to W</script>from a vector space <script type="math/tex">V</script> to a vector space <script type="math/tex">W</script> where we implicitly have a basis <script type="math/tex">\left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script> for  <script type="math/tex">V</script> and a basis <script type="math/tex">\left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_m}} \right\}</script> for <script type="math/tex">W</script>. Notice that when the transformation is noninvertible, then <script type="math/tex">m</script> will not equal <script type="math/tex">n</script>. We can interpret each column of a general matrix <script type="math/tex">\boldsymbol{A}</script> as expressing the coordinates for a transformed input basis vector in terms of the output basis vectors. Therefore, for the transformation <script type="math/tex">\boldsymbol{w = Av}</script>, we can interpret <script type="math/tex">\boldsymbol{v}</script> as representing an input coordinate vector relative to the basis <script type="math/tex">\left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script>, and <script type="math/tex">\boldsymbol{w}</script> as representing an output transformed coordinate vector relative to the basis <script type="math/tex">\left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_m}} \right\}</script>. The choice of basis for <script type="math/tex">V</script> and <script type="math/tex">W</script> is arbitrary but will change the matrix used to represent the linear transformation. As such, notice that for eigenvectors written as <script type="math/tex">\boldsymbol{Ax = }\lambda \boldsymbol{x}</script>, the input coordinates <script type="math/tex">\boldsymbol{x}</script> are relative to basis <script type="math/tex">\left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script> and are equal to the scaled output coordinates <script type="math/tex">\lambda \boldsymbol{x}</script> which is relative to basis<script type="math/tex">\left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_m}} \right\}</script>. If <script type="math/tex">\boldsymbol{A}</script> is diagonalizable, then we have eigenvalues <script type="math/tex">{\lambda _1}, \ldots ,{\lambda _n}</script>and eigenvectors <script type="math/tex">{\boldsymbol{e}_1}, \ldots ,{\boldsymbol{e}_n}</script>, which are a basis for the coordinate space <script type="math/tex">{\mathbb{R}^n}</script>. We can write any input coordinate vector in terms of these eigenvectors as: <script type="math/tex">\boldsymbol{x} = {c_1}{\boldsymbol{e}_1} +  \cdots  + {c_n}{\boldsymbol{e}_n}</script>. Computing the action of the transformation we see that <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{A}\left( {{c_1}{\boldsymbol{e}_1} +  \cdots  + {c_n}{\boldsymbol{e}_n}} \right) = {c_1}{\lambda _1}{\boldsymbol{e}_1} +  \cdots  + {c_n}{\lambda _n}{\boldsymbol{e}_n}</script>. We can write this expression as <script type="math/tex">\boldsymbol{APx} = \boldsymbol{PDx}</script> where <script type="math/tex">% <![CDATA[
\boldsymbol{P} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{e}_1}}& \cdots &{{\boldsymbol{e}_n}}\end{array}} \right] %]]></script> and <script type="math/tex">% <![CDATA[
\boldsymbol{D} = \left[ {\begin{array}{*{20}{c}}{{\lambda _1}}& \cdots &0\\ \vdots & \ddots & \vdots \\0& \cdots &{{\lambda _n}}\end{array}} \right] %]]></script>. Therefore, we see that <script type="math/tex">\boldsymbol{A = PD}{\boldsymbol{P}^{ - 1}}</script>. We can interpret this in the following way: the matrix <script type="math/tex">{\boldsymbol{P}^{ - 1}}</script> performs a change of basis on the input coordinate vector from <script type="math/tex">\left\{ {{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_n}} \right\}</script> to <script type="math/tex">\left\{ {{\boldsymbol{e}_1}, \ldots ,{\boldsymbol{e}_n}} \right\}</script>, then using <script type="math/tex">\boldsymbol{PD}</script> to perform a scaling transformation that transforms coordinates from <script type="math/tex">\left\{ {{\boldsymbol{e}_1}, \ldots ,{\boldsymbol{e}_n}} \right\}</script> to <script type="math/tex">\left\{ {{\boldsymbol{w}_1}, \ldots ,{\boldsymbol{w}_m}} \right\}</script>. The key importance is the following: given a linear transformation represented by a square matrix <script type="math/tex">\boldsymbol{A}</script>, we can find a change of basis that enables us to view the linear transformation as a scaling operation. The specific change of basis that enables this are the eigenvectors of <script type="math/tex">\boldsymbol{A}</script>.</li>
  <li><strong>Dynamical System:</strong> Many dynamical systems involve rules that govern the evolution of the current state into the next state. When the state update equations are linear, we can describe the state transition using a linear transformation represented as a matrix <script type="math/tex">\boldsymbol{S}</script>. Suppose we have some initial conditions for the system at <script type="math/tex">t = 0</script>, given as <script type="math/tex">{\boldsymbol{u}_0}</script>. To calculate the state of the system at <script type="math/tex">t = n</script>, we must recursively apply the state transition transformation <script type="math/tex">n</script> times. Hence, the state of the system at <script type="math/tex">t = n</script> is <script type="math/tex">{\boldsymbol{u}_n} = {\boldsymbol{S}^n}{\boldsymbol{u}_0}</script>. Notice that this computation involves taking powers of the matrix <script type="math/tex">\boldsymbol{S}</script>. Taking large powers of a general matrix is computationally difficult but taking powers of a diagonal matrix is easy. Therefore, from the eigendecomposition of <script type="math/tex">\boldsymbol{S}</script> we can write <script type="math/tex">{\boldsymbol{S}^n} = {\left( {\boldsymbol{PD}{\boldsymbol{P}^{ - 1}}} \right)^n} = \boldsymbol{P}{\boldsymbol{D}^n}{\boldsymbol{P}^{ - 1}}</script>. Hence, the eigenvector basis is ideal when taking powers of a matrix.</li>
  <li><strong>Differential Equations:</strong> Consider a system of linear differential equations. We can describe the system of equations using a linear transformation represented by matrix <script type="math/tex">\boldsymbol{A}</script>. It often becomes easy to solve the system if we can propose a change of variables that decouples the variables in the equations. Using the eigenvectors of <script type="math/tex">\boldsymbol{A}</script> as a new basis, we can decouple the variables in the system. See <a href="https://math.stackexchange.com/questions/23312/what-is-the-importance-of-eigenvalues-eigenvectors">here</a> for a great example.</li>
  <li><strong>Rank-1 Decomposition:</strong> For symmetric matrices such as covariance matrices, we can write the eigen-decomposition as <script type="math/tex">\boldsymbol{A} = \boldsymbol{Q}\Lambda {\boldsymbol{Q}^T}</script>. We can expand this factorization as a sum of rank-1 matrices such that <script type="math/tex">\boldsymbol{A} = {\lambda _1}{\boldsymbol{q}_1}\boldsymbol{q}_1^T +  \cdots  + {\lambda _n}{\boldsymbol{q}_n}\boldsymbol{q}_n^T</script>. This enables applications such as PCA.</li>
</ul>

<p>We can find the eigenvalues and eigenvectors for a matrix <script type="math/tex">\boldsymbol{A}</script> by solving the equation <script type="math/tex">\boldsymbol{Ax = }\lambda \boldsymbol{x}</script>. We can rewrite this equation as <script type="math/tex">\left( {\boldsymbol{A} - \lambda \boldsymbol{I}} \right)\boldsymbol{x} = \boldsymbol{0}</script>. This implies that the matrix <script type="math/tex">\boldsymbol{A} - \lambda \boldsymbol{I}</script> must be singular, otherwise the only solutions are the trivial solutions. Therefore, since <script type="math/tex">\boldsymbol{A} - \lambda \boldsymbol{I}</script> is singular we know that <script type="math/tex">\det \left( {\boldsymbol{A} - \lambda \boldsymbol{I}} \right) = 0</script>. Once we have found the eigenvalues, then we can use elimination to find the nullspace of <script type="math/tex">\boldsymbol{A} - \lambda \boldsymbol{I}</script>. The vectors in the nullspace are eigenvectors of <script type="math/tex">\boldsymbol{A}</script> with eigenvalue <script type="math/tex">\lambda</script>.</p>

<p>As an example, consider the projection matrix defined as <script type="math/tex">\boldsymbol{P} = \boldsymbol{A}{\left( {{\boldsymbol{A}^T}\boldsymbol{A}} \right)^{ - 1}}{\boldsymbol{A}^T}</script>. The projection matrix will project any vector onto the column space of <script type="math/tex">\boldsymbol{A}</script>. Therefore, any vector in the column space of <script type="math/tex">\boldsymbol{A}</script> will be an eigenvector of <script type="math/tex">\boldsymbol{P}</script>. We can see this since the column space of <script type="math/tex">\boldsymbol{A}</script> is defined by the columns of <script type="math/tex">\boldsymbol{A}</script>, so <script type="math/tex">\boldsymbol{PA} = \boldsymbol{A}{\left( {{\boldsymbol{A}^T}\boldsymbol{A}} \right)^{ - 1}}{\boldsymbol{A}^T}\boldsymbol{A} = \boldsymbol{A}</script>. Therefore, we see that the columns of <script type="math/tex">\boldsymbol{A}</script> are the eigenvectors of <script type="math/tex">\boldsymbol{P}</script> with eigenvalues <script type="math/tex">\lambda  = 1</script>. Also, any vector orthogonal to the column space will be an eigenvector of <script type="math/tex">\boldsymbol{P}</script> with eigenvalue <script type="math/tex">\lambda  = 0</script>. This can be seen since if <script type="math/tex">\boldsymbol{x}</script> is orthogonal to the columns of <script type="math/tex">\boldsymbol{A}</script>, then <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{x} = \boldsymbol{0}</script> and <script type="math/tex">\boldsymbol{Px} = \boldsymbol{0}</script>.</p>

<p>Suppose that we have <script type="math/tex">n</script> linearly independent eigenvectors of <script type="math/tex">\boldsymbol{A}</script> and we put these vectors as the columns of <script type="math/tex">\boldsymbol{S}</script>. Then we can write<script type="math/tex">% <![CDATA[
\boldsymbol{S = }\left[ {\begin{array}{*{20}{c}}{{\boldsymbol{x}_1}}& \cdots &{{\boldsymbol{x}_n}}\end{array}} \right] %]]></script>. Multiplying we can get  <script type="math/tex">% <![CDATA[
\boldsymbol{AS = }\left[ {\begin{array}{*{20}{c}}{\boldsymbol{A}{\boldsymbol{x}_1}}& \cdots &{\boldsymbol{A}{\boldsymbol{x}_n}}\end{array}} \right]\boldsymbol{ = }\left[ {\begin{array}{*{20}{c}}{{\lambda _1}{\boldsymbol{x}_1}}& \cdots &{{\lambda _n}{\boldsymbol{x}_n}}\end{array}} \right] = \boldsymbol{S}\Lambda %]]></script>, where <script type="math/tex">\Lambda</script> is a diagonal matrix with the eigenvalues on the diagonal. Then we can multiply by the inverse to get <script type="math/tex">\boldsymbol{A} = \boldsymbol{S}\Lambda {\boldsymbol{S}^{ - 1}}</script>.</p>

<p>What can we say about the eigenvalues and eigenvectors of <script type="math/tex">{\boldsymbol{A}^2}</script>? If <script type="math/tex">\boldsymbol{Ax = }\lambda \boldsymbol{x}</script> then we can multiply both sides by <script type="math/tex">\boldsymbol{A}</script> to get <script type="math/tex">{\boldsymbol{A}^2}\boldsymbol{x = }\lambda \boldsymbol{Ax = }{\lambda ^2}\boldsymbol{x}</script>. Therefore, we see that <script type="math/tex">{\boldsymbol{A}^2}</script> has the same eigenvectors as <script type="math/tex">\boldsymbol{A}</script> and its eigenvalues are squared. We can also see this from <script type="math/tex">{\boldsymbol{A}^2} = \boldsymbol{S}\Lambda {\boldsymbol{S}^{ - 1}}\boldsymbol{S}\Lambda {\boldsymbol{S}^{ - 1}} = \boldsymbol{S}{\Lambda ^2}{\boldsymbol{S}^{ - 1}}</script>. This is an incredibly powerful property of the eigenvalue diagonalization factorization.</p>

<p>In order to diagonalize a matrix, we need to have <script type="math/tex">n</script> independent eigenvectors. When can we be certain that a matrix is diagonalizable? Two conditions that will guarantee that the matrix is diagonalizable are: 1) the matrix is symmetric (this comes from the spectral theorem), and 2) all eigenvalues of the matrix are different.</p>

<p>To see why <script type="math/tex">n</script> independent eigenvectors exist if all the eigenvalues are different, consider the following proof. Suppose that we have <script type="math/tex">n</script> different eigenvalues <script type="math/tex">{\lambda _1} \ldots ,{\lambda _n}</script>, but a dependent eigenvector. Then there exists a linear combination of <script type="math/tex">{\boldsymbol{x}_k}</script> in terms of the other independent eigenvectors such that <script type="math/tex">{\boldsymbol{x}_k} = {c_1}{\boldsymbol{x}_1} +  \cdots  + {c_n}{\boldsymbol{x}_n}</script> . From the definition of an eigenvector, we know that <script type="math/tex">\boldsymbol{A}{\boldsymbol{x}_k} = {\lambda _k}{\boldsymbol{x}_k}</script>. Substituting the linear combination on both sides gives:</p>

<script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{A}{\boldsymbol{x}_k} = {\lambda _k}{\boldsymbol{x}_k}\\ \Rightarrow \boldsymbol{A}\left( {{c_1}{\boldsymbol{x}_1} +  \cdots  + {c_n}{\boldsymbol{x}_n}} \right) = {\lambda _k}\left( {{c_1}{\boldsymbol{x}_1} +  \cdots  + {c_n}{\boldsymbol{x}_n}} \right)\\ \Rightarrow {c_1}{\lambda _1}{\boldsymbol{x}_1} +  \cdots  + {c_n}{\lambda _n}{\boldsymbol{x}_n} = {c_1}{\lambda _k}{\boldsymbol{x}_1} +  \cdots  + {c_n}{\lambda _k}{\boldsymbol{x}_n}\\ \Rightarrow {c_1}\left( {{\lambda _1} - {\lambda _k}} \right){\boldsymbol{x}_1} +  \cdots  + {c_n}\left( {{\lambda _n} - {\lambda _k}} \right){\boldsymbol{x}_n} = 0\end{array}</script>

<p>Since all the eigenvectors are different, we see that the <script type="math/tex">{\lambda _j} - {\lambda _k} \ne 0</script>, hence the collection of vectors must be dependent which is a contradiction. Therefore, all the eigenvectors must be independent. If we have repeated eigenvalues, we may or may not have <script type="math/tex">n</script> independent eigenvectors. For example, the identity matrix has eigenvalues all equal to 1, but has <script type="math/tex">n</script> independent eigenvectors.</p>

<p>Some useful properties of eigenvalues are the following:</p>
<ul>
  <li>The eigenvalues of <script type="math/tex">\boldsymbol{AB}</script> equals the eigenvalues of <script type="math/tex">\boldsymbol{BA}</script>.</li>
  <li>The sum of the eigenvalues of <script type="math/tex">\boldsymbol{A}</script> is equal to the sum of the diagonal entries of<script type="math/tex">\boldsymbol{A}</script> (i.e. the trace).</li>
  <li>The product of the eigenvalues of <script type="math/tex">\boldsymbol{A}</script> is equal to the determinant of <script type="math/tex">\boldsymbol{A}</script>.</li>
  <li>If we add a multiple of the identity matrix to <script type="math/tex">\boldsymbol{A}</script>, then the eigenvectors will stay the same for all multiples, but the eigenvalues will increase by the scaling factor. To see this, suppose that <script type="math/tex">\boldsymbol{x}</script> is an eigenvector associated with an eigenvalue <script type="math/tex">\lambda</script>, then <script type="math/tex">\left( {\boldsymbol{A} + c\boldsymbol{I}} \right)\boldsymbol{x} = \boldsymbol{Ax + }c\boldsymbol{Ix = }\lambda \boldsymbol{x + }c\boldsymbol{x} = \left( {\lambda  + c} \right)\boldsymbol{x}</script>.</li>
  <li>A matrix with real valued entries can have complex eigenvalues and eigenvectors. However, a symmetric matrix will always have real eigenvalues. On the other hand, antisymmetric matrices where <script type="math/tex">{\boldsymbol{A}^T} =  - \boldsymbol{A}</script> have all imaginary eigenvalues.</li>
  <li>The eigenvalues of a triangular matrix are equal to the diagonal entries of the matrix.</li>
  <li>For a symmetric matrix, the signs of the pivots of the symmetric matrix in reduced row echelon form tell us the signs of the eigenvalues.</li>
</ul>

<p><a name="solving-difference-equations"></a></p>

<p><br /></p>

<hr />
<h4 id="solving-difference-equations">Solving Difference Equations</h4>
<hr />

<p>Suppose that we have a difference equation such that <script type="math/tex">{\boldsymbol{u}_{k + 1}} = \boldsymbol{A}{\boldsymbol{u}_k}</script>, then for some initial starting condition <script type="math/tex">{\boldsymbol{u}_0}</script> we can write the <script type="math/tex">{k^{th}}</script> state as <script type="math/tex">{\boldsymbol{u}_k} = {\boldsymbol{A}^k}{\boldsymbol{u}_0}</script>. We can write the vector as a linear combination of the eigenvectors of <script type="math/tex">\boldsymbol{A}</script>, such that <script type="math/tex">{\boldsymbol{u}_0} = {c_1}{\boldsymbol{x}_1} +  \cdots  + {c_n}{\boldsymbol{x}_n}</script>. Then when we multiply by <script type="math/tex">\boldsymbol{A}</script> we get: <script type="math/tex">\boldsymbol{A}{\boldsymbol{u}_0} = {c_1}{\lambda _1}{\boldsymbol{x}_1} +  \cdots  + {c_n}{\lambda _n}{\boldsymbol{x}_n}</script>. Therefore, the <script type="math/tex">{k^{th}}</script> state is <script type="math/tex">{\boldsymbol{A}^k}{\boldsymbol{u}_0} = {c_1}\lambda _1^k{\boldsymbol{x}_1} +  \cdots  + {c_n}\lambda _n^k{\boldsymbol{x}_n} = {\Lambda ^k}\boldsymbol{Sc}</script>.</p>

<p>As an application of the difference equations, we can examine the Fibonacci numbers. We can write the Fibonacci equation as <script type="math/tex">{\boldsymbol{u}_k} = \left[ {\begin{array}{*{20}{c}}{{F_{k + 1}}}\\{{F_k}}\end{array}} \right]</script> and <script type="math/tex">% <![CDATA[
{\boldsymbol{u}_{k + 1}} = \left[ {\begin{array}{*{20}{c}}1&1\\1&0\end{array}} \right]{\boldsymbol{u}_k} %]]></script>. Finding the eigenvalues and eigenvectors of this matrix can allow us to calculate the Fibonacci numbers.</p>

<p><a name="solving-differential-equations"></a></p>

<p><br /></p>

<hr />
<h4 id="solving-differential-equations">Solving Differential Equations</h4>
<hr />

<p>We can visualize a matrix as a linear operator that acts on vectors, and we can visualize the derivative as a linear operator that acts on functions. An interesting connection between the two is the connection between the eigenvalues of a matrix and the solutions to a linear differential equation to a linear differential equation. For example, note that the solutions to the equation <script type="math/tex">\boldsymbol{Ax = }\lambda \boldsymbol{x}</script> are the eigenvectors (assuming that <script type="math/tex">\boldsymbol{A}</script> is a square matrix with independent eigenvectors). Likewise, for the differential equations, the solution to the equation <script type="math/tex">{D_\boldsymbol{x}}\,f\left( \boldsymbol{x} \right) = \lambda f\left( \boldsymbol{x} \right)</script> are functions called eigenfunctions. For first order linear differential equations, the solutions are <script type="math/tex">f\left( \boldsymbol{x} \right) = \left[ {\begin{array}{*{20}{c}}{{c_1}{e^{\lambda {x_1}}}}\\ \vdots \\{{c_n}{e^{\lambda {x_n}}}}\end{array}} \right]</script>. Now suppose that we have a system of differential equations such that <script type="math/tex">{D_t}\,\left[ {\begin{array}{*{20}{c}}{{u_1}\left( t \right)}\\{{u_2}\left( t \right)}\end{array}} \right] = \boldsymbol{A}\left[ {\begin{array}{*{20}{c}}{{u_1}\left( t \right)}\\{{u_2}\left( t \right)}\end{array}} \right]</script>. In this case, we are now combining the differentiation operator with a vector of functions that is transformed by a matrix. We could write <script type="math/tex">\left[ {\begin{array}{*{20}{c}}{{u_1}\left( t \right)}\\{{u_2}\left( t \right)}\end{array}} \right]</script> as a linear combination of the eigenvectors such that  <script type="math/tex">\left[ {\begin{array}{*{20}{c}}{{u_1}\left( t \right)}\\{{u_2}\left( t \right)}\end{array}} \right] = {f_1}\left( t \right){\boldsymbol{x}_1} + {f_2}\left( t \right){\boldsymbol{x}_2}</script> for some functions <script type="math/tex">{f_1}\left( t \right)</script> and <script type="math/tex">{f_2}\left( t \right)</script>. Substituting this into the equation above we get</p>

<script type="math/tex; mode=display">\begin{array}{l}{D_t}\,\left( {{f_1}\left( t \right){\boldsymbol{x}_1} + {f_2}\left( t \right){\boldsymbol{x}_2}} \right) = \boldsymbol{A}\left( {{f_1}\left( t \right){\boldsymbol{x}_1} + {f_2}\left( t \right){\boldsymbol{x}_2}} \right)\\ \Rightarrow {D_t}\,{f_1}\left( t \right){\boldsymbol{x}_1} + {D_t}\,{f_2}\left( t \right){\boldsymbol{x}_2} = {f_1}\left( t \right){\lambda _1}{\boldsymbol{x}_1} + {f_2}\left( t \right){\lambda _2}{\boldsymbol{x}_2}\end{array}</script>

<p>This implies that <script type="math/tex">{D_t}\,{f_1}\left( t \right) = {f_1}\left( t \right){\lambda _1}</script> and <script type="math/tex">{D_t}\,{f_2}\left( t \right) = {f_2}\left( t \right){\lambda _2}</script>. The general solutions to these equations are <script type="math/tex">\left[ {\begin{array}{*{20}{c}}{{f_1}\left( t \right)}\\{{f_2}\left( t \right)}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}{{c_1}{e^{{\lambda _1}t}}}\\{{c_2}{e^{{\lambda _2}t}}}\end{array}} \right]</script>. Therefore, we see that the general solution is <script type="math/tex">\left[ {\begin{array}{*{20}{c}}{{u_1}\left( t \right)}\\{{u_2}\left( t \right)}\end{array}} \right] = {f_1}\left( t \right){\boldsymbol{x}_1} + {f_2}\left( t \right){\boldsymbol{x}_2} = {c_1}{e^{{\lambda _1}t}}{\boldsymbol{x}_1} + {c_2}{e^{{\lambda _2}t}}{\boldsymbol{x}_2}</script></p>

<p>We can solve for the constants in this solution by using the initial conditions for the system. To generalize this idea for any system, suppose that we have the equation<script type="math/tex">{D_t}\,\boldsymbol{u}\left( t \right) = \boldsymbol{Au}\left( t \right)</script>. We can write the values of <script type="math/tex">\boldsymbol{u}\left( t \right)</script> at any given time point as a linear combination of the eigenvectors of <script type="math/tex">\boldsymbol{A}</script> (assuming the eigenvectors are independent and span the complete space) such that <script type="math/tex">\boldsymbol{u}\left( t \right) = \boldsymbol{Sv}\left( t \right)</script>. We can write the equation as follows:</p>

<script type="math/tex; mode=display">\begin{array}{l}{D_t}\,\boldsymbol{Sv}\left( t \right) = \boldsymbol{ASv}\left( t \right)\\ \Rightarrow \boldsymbol{S}\left( {{D_t}\,\boldsymbol{v}\left( t \right)} \right) = \boldsymbol{ASv}\left( t \right)\\ \Rightarrow {D_t}\,\boldsymbol{v}\left( t \right) = {\boldsymbol{S}^{ - 1}}\boldsymbol{ASv}\left( t \right) = \Lambda \boldsymbol{v}\left( t \right)\end{array}</script>

<p>The general solution to this system is given as</p>

<script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{v}\left( t \right) = {e^{\Lambda t}}\boldsymbol{v}\left( 0 \right)\\ \Rightarrow {\boldsymbol{S}^{ - 1}}\boldsymbol{u}\left( t \right) = {e^{\Lambda t}}{\boldsymbol{S}^{ - 1}}\boldsymbol{u}\left( 0 \right)\\ \Rightarrow \boldsymbol{u}\left( t \right) = \boldsymbol{S}{e^{\Lambda t}}{\boldsymbol{S}^{ - 1}}\boldsymbol{u}\left( 0 \right) = {e^{\boldsymbol{A}t}}\boldsymbol{u}\left( 0 \right)\end{array}</script>

<p>What is a matrix exponential? We can write out the Taylor series for the exponential as
<script type="math/tex">{e^x} = 1 + x + \frac{{{x^2}}}{2} + \frac{{{x^3}}}{6} +  \cdots</script></p>

<p>We can define the matrix exponential by extending this formula such that</p>

<script type="math/tex; mode=display">{e^{At}} = \boldsymbol{I} + \boldsymbol{A}t + \frac{{{{\left( {\boldsymbol{A}t} \right)}^2}}}{2} + \frac{{{{\left( {\boldsymbol{A}t} \right)}^3}}}{6} +  \cdots</script>

<p>Using the eigenvalue decomposition for <script type="math/tex">\boldsymbol{A}</script>, we can write</p>

<script type="math/tex; mode=display">\begin{array}{l}{e^{At}} = \boldsymbol{S}{\boldsymbol{S}^{ - 1}} + \left( {\boldsymbol{S}\Lambda {\boldsymbol{S}^{ - 1}}} \right)t + \frac{{\boldsymbol{S}{\Lambda ^2}{\boldsymbol{S}^{ - 1}}}}{2}{t^2} + \frac{{\boldsymbol{S}{\Lambda ^3}{\boldsymbol{S}^{ - 1}}}}{6}{t^3} +  \cdots \\ = \boldsymbol{S}{e^{\Lambda t}}{\boldsymbol{S}^{ - 1}}\end{array}</script>

<p>To calculate <script type="math/tex">{e^{\Lambda t}}</script> we recall that <script type="math/tex">\Lambda</script> is the diagonal matrix with the eigenvalues on the diagonal. When we substitute this into the Taylor series expansion, we see that the result of the sum is a diagonal matrix where the <script type="math/tex">{k^{th}}</script> diagonal entry is itself a Taylor series expansion for <script type="math/tex">{e^{{\lambda _k}t}}</script>. Therefore, we see that</p>

<script type="math/tex; mode=display">% <![CDATA[
{e^{\Lambda t}} = \left[ {\begin{array}{*{20}{c}}{{e^{{\lambda _1}t}}}&0& \cdots &0\\0&{{e^{{\lambda _2}t}}}&{}&0\\ \vdots &{}& \ddots & \vdots \\0& \cdots &0&{{e^{{\lambda _n}t}}}\end{array}} \right] %]]></script>

<p>We can understand the stability of a system by examining the eigenvalues of the system. We have three possibilities:</p>

<ol>
  <li><strong>Transient:</strong> We see that the solution will go to zero if for all eigenvalues <script type="math/tex">% <![CDATA[
{\mathop{\rm Re}\nolimits} \left( \lambda  \right) < 0 %]]></script>. To see why, remember that if <script type="math/tex">\lambda  = a + bi</script> we can write <script type="math/tex">{e^{\lambda t}} = {e^{\left( {a + bi} \right)t}} = {e^{at}}{e^{bit}} = {e^{at}}\left( {\cos \,\,bt + i\sin \,bt} \right)</script>. Hence, the imaginary part causes a rotational component with unit length. What controls the scale is the real part.</li>
  <li><strong>Steady State:</strong> If at least one eigenvalue is 0 and all other eigenvalues have negative real part then the system will converge to a steady state response</li>
  <li><strong>Explode:</strong> If <script type="math/tex">{\mathop{\rm Re}\nolimits} \left( \lambda  \right) > 0</script> for any eigenvalue, then the system will explode</li>
</ol>

<p><a name="markov-chains"></a></p>

<p><br /></p>

<hr />
<h4 id="markov-chains">Markov Chains</h4>
<hr />

<p>A Markov matrix is defined such that all entries are nonnegative and the sum of the entries in each column is 1. A Markov matrix <script type="math/tex">\boldsymbol{A}</script> will have one eigenvalue which equals 1, and the magnitude of all other eigenvalues will be less than 1. The way we can see that 1 is an eigenvalue of a Markov matrix is to recognize that since all the columns add to 1, then subtracting the identity matrix from the Markov matrix will make it such that the columns of <script type="math/tex">\boldsymbol{A} - \boldsymbol{I}</script> sum to 0. Therefore, the sum of the row vectors must be the zero vector which means that the rows of <script type="math/tex">\boldsymbol{A} - \boldsymbol{I}</script> are dependent. Hence, <script type="math/tex">\boldsymbol{A} - \boldsymbol{I}</script> is singular and has eigenvalue 0, and the matrix <script type="math/tex">\boldsymbol{A}</script> must have eigenvalue 1. If we have the difference equation <script type="math/tex">{\boldsymbol{u}_{k + 1}} = \boldsymbol{A}{\boldsymbol{u}_k}</script> and the matrix <script type="math/tex">\boldsymbol{A}</script> is Markov, then we can solve for the long-term state probabilities by using the eigenvalues and eigenvectors of <script type="math/tex">\boldsymbol{A}</script>.</p>

<p><a name="symmetric-matrices"></a></p>

<p><br /></p>

<hr />
<h4 id="symmetric-matrices">Symmetric Matrices</h4>
<hr />

<p>Symmetric matrices are the most important class of matrices. Symmetric matrices have the property <script type="math/tex">{\boldsymbol{A}^T} = \boldsymbol{A}</script>. Symmetric matrices have the property that the eigenvalues are real, and the eigenvectors are orthogonal (given from the spectral theorem).</p>

<p>Usually, we can write the eigendecomposition of a matrix as <script type="math/tex">\boldsymbol{A} = \boldsymbol{S}\Lambda {\boldsymbol{S}^{ - 1}}</script>. However, when the matrix is symmetric, we can write <script type="math/tex">\boldsymbol{A} = \boldsymbol{Q}\Lambda {\boldsymbol{Q}^T}</script>. This is called the spectral theorem or principal axis theorem.</p>

<p>To show that the eigenvalues must be real for a symmetric matrix we can write the expression <script type="math/tex">\boldsymbol{Ax = }\lambda \boldsymbol{x}</script>. Regardless of the eigenvalues we can write the complex conjugate as</p>

<script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{\bar A\bar x = }\bar \lambda \boldsymbol{\bar x}\\ \Rightarrow \boldsymbol{A\bar x = }\bar \lambda \boldsymbol{\bar x}\end{array}</script>

<p>Which follows if the matrix has real components. If we transpose this equation, we get</p>

<script type="math/tex; mode=display">\begin{array}{l}{{\boldsymbol{\bar x}}^T}{\boldsymbol{A}^T}\boldsymbol{ = }\bar \lambda {{\boldsymbol{\bar x}}^T}\\ \Rightarrow {{\boldsymbol{\bar x}}^T}\boldsymbol{A = }\bar \lambda {{\boldsymbol{\bar x}}^T}\\ \Rightarrow {{\boldsymbol{\bar x}}^T}\boldsymbol{Ax = }\bar \lambda {{\boldsymbol{\bar x}}^T}\boldsymbol{x}\\ \Rightarrow {{\boldsymbol{\bar x}}^T}\lambda \boldsymbol{x = }\bar \lambda {{\boldsymbol{\bar x}}^T}\boldsymbol{x}\end{array}</script>

<p>This implies that <script type="math/tex">\lambda  = \bar \lambda</script> since the dot product <script type="math/tex">{\boldsymbol{\bar x}^T}\boldsymbol{x}</script> represents the length of the vector and will be nonzero since the eigenvectors are nonzero. Therefore, we see that the eigenvalues must be real. The equivalent to symmetric matrices for complex entries are called Hermitian matrices and <script type="math/tex">{\boldsymbol{\bar A}^T} = \boldsymbol{A}</script>.</p>

<p>We can expand the eigenvalue factorization into rank 1 matrices such that</p>

<script type="math/tex; mode=display">\boldsymbol{A} = \boldsymbol{Q}\Lambda {\boldsymbol{Q}^T} = {\lambda _1}{\boldsymbol{q}_1}{\boldsymbol{q}_1}^T +  \cdots  + {\lambda _n}{\boldsymbol{q}_n}{\boldsymbol{q}_n}^T</script>

<p>Recall that if we want to project a vector <script type="math/tex">\boldsymbol{b}</script> onto the vector <script type="math/tex">\boldsymbol{q}</script>, then we have the normal equation for the error such that <script type="math/tex">{\boldsymbol{q}^T}\left( {\boldsymbol{b} - x\boldsymbol{q}} \right) = 0</script>. Solving for the coefficient we get <script type="math/tex">x = \frac{{{\boldsymbol{q}^T}\boldsymbol{b}}}{{{\boldsymbol{q}^T}\boldsymbol{q}}}</script>. Now computing the projection, we get <script type="math/tex">x\boldsymbol{q} = \boldsymbol{q}\frac{{{\boldsymbol{q}^T}\boldsymbol{b}}}{{{\boldsymbol{q}^T}\boldsymbol{q}}} = \left( {\frac{{\boldsymbol{q}{\boldsymbol{q}^T}}}{{{\boldsymbol{q}^T}\boldsymbol{q}}}} \right)\boldsymbol{b}</script>. Therefore, we see that the matrix <script type="math/tex">\frac{{\boldsymbol{q}{\boldsymbol{q}^T}}}{{{\boldsymbol{q}^T}\boldsymbol{q}}}</script> is the projection matrix to project onto the vector <script type="math/tex">\boldsymbol{q}</script>. Furthermore, if <script type="math/tex">\boldsymbol{q}</script> is unit length, then the projection matrix simply becomes <script type="math/tex">\boldsymbol{q}{\boldsymbol{q}^T}</script>.</p>

<p>Another way to interpret this projection matrix is to consider what the expression <script type="math/tex">\frac{{\boldsymbol{q}{\boldsymbol{q}^T}}}{{{\boldsymbol{q}^T}\boldsymbol{q}}}\boldsymbol{b}</script> means. Rearranging and expanding using the definition of the dot product, we see:</p>

<script type="math/tex; mode=display">\frac{{\boldsymbol{q}{\boldsymbol{q}^T}}}{{{\boldsymbol{q}^T}\boldsymbol{q}}}\boldsymbol{b = q}\frac{{{\boldsymbol{q}^T}\boldsymbol{b}}}{{{\boldsymbol{q}^T}\boldsymbol{q}}} = \boldsymbol{q}\frac{{\left\| \boldsymbol{q} \right\|\left\| \boldsymbol{b} \right\|\cos \theta }}{{{\boldsymbol{q}^T}\boldsymbol{q}}} = \frac{\boldsymbol{q}}{{\left\| \boldsymbol{q} \right\|}}\left\| \boldsymbol{b} \right\|\cos \theta</script>

<p>Therefore, the projection computes a scaling factor <script type="math/tex">\left\| \boldsymbol{b} \right\|\cos \theta</script> which we can see after drawing a triangle with <script type="math/tex">\boldsymbol{b}</script> as the hypotenuse and a ray along the vector <script type="math/tex">\boldsymbol{q}</script>, represents the distance along <script type="math/tex">\boldsymbol{q}</script> where the perpendicular projection of <script type="math/tex">\boldsymbol{b}</script> occurs. Hence multiplying this scaling factor by the unit vector <script type="math/tex">\frac{\boldsymbol{q}}{{\left\| \boldsymbol{q} \right\|}}</script> gives the vector representing the projection of <script type="math/tex">\boldsymbol{b}</script> onto <script type="math/tex">\boldsymbol{q}</script>.</p>

<p>Returning to the eigenvector decomposition written as <script type="math/tex">\boldsymbol{A} = \boldsymbol{Q}\Lambda {\boldsymbol{Q}^T} = {\lambda _1}{\boldsymbol{q}_1}{\boldsymbol{q}_1}^T +  \cdots  + {\lambda _n}{\boldsymbol{q}_n}{\boldsymbol{q}_n}^T</script>, we see that each component <script type="math/tex">{\lambda _k}{\boldsymbol{q}_k}{\boldsymbol{q}_k}^T</script> represents a perpendicular projection onto the unit eigenvector <script type="math/tex">{\boldsymbol{q}_k}</script> where the eigenvalue <script type="math/tex">{\lambda _k}</script> describes how much this projection contributes to the action of the matrix. Hence, every symmetric matrix can be interpreted as a combination of perpendicular projection matrices.</p>

<p><a name="special-matrices"></a></p>

<p><br /></p>

<hr />
<h2 id="special-matrices">Special Matrices</h2>
<hr />

<p>In this section we will discuss some classes of matrices which have special properties.</p>

<p><a name="positive-definite-matrices"></a></p>

<p><br /></p>

<hr />
<h4 id="positive-definite-matrices">Positive Definite Matrices</h4>
<hr />

<p>How do we know if a symmetric matrix is positive definite? There are a few different kinds of tests for positive definiteness: 1) are the eigenvalues all positive, 2) are the sub-determinants all positive, 3) are the pivots all positive, 4) <script type="math/tex">{\boldsymbol{x}^T}\boldsymbol{Ax > 0}</script> at all nonzero points.</p>

<p>Given that the eigenvalues of a positive definite matrix are positive, then we know that the inverse of the matrix must also be positive definite since the eigenvalues are reciprocals which preserves the sign. If <script type="math/tex">\boldsymbol{A}</script> and <script type="math/tex">\boldsymbol{B}</script> are positive definite, then we can easy show that <script type="math/tex">{\boldsymbol{x}^T}\left( {\boldsymbol{A} + \boldsymbol{B}} \right)\boldsymbol{x} = {\boldsymbol{x}^T}\boldsymbol{Ax} + {\boldsymbol{x}^T}\boldsymbol{Bx} > 0</script>, so <script type="math/tex">\boldsymbol{A} + \boldsymbol{B}</script> is positive definite.</p>

<p><a name="gram-matrices"></a></p>

<p><br /></p>

<hr />
<h4 id="gram-matrices">Gram Matrices</h4>
<hr />

<p>Suppose we have an <script type="math/tex">m</script> by <script type="math/tex">n</script> matrix <script type="math/tex">\boldsymbol{A}</script>. We know that <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> is square and symmetric, is it also positive definite? Looking at the expression <script type="math/tex">{\boldsymbol{x}^T}{\boldsymbol{A}^T}\boldsymbol{Ax} = {\left\| {\boldsymbol{Ax}} \right\|^2} \ge 0</script>, therefore <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> is positive semidefinite. For  <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> to be positive definite, we must ensure that <script type="math/tex">\boldsymbol{Ax} \ne 0</script> except for the zero vector. This means that the nullspace is empty and the column space must be full rank. Therefore, we can ensure that <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> is positive definite if the rank of <script type="math/tex">\boldsymbol{A}</script> is <script type="math/tex">n</script>. The matrix <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> is called the Gram matrix and is always semi-positive definite. If <script type="math/tex">\boldsymbol{A}</script> is invertible, then <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> will be positive definite.</p>

<p><a name="similar-matrices"></a></p>

<p><br /></p>

<hr />
<h4 id="similar-matrices">Similar Matrices</h4>
<hr />

<p>Suppose that we have two square matrices <script type="math/tex">\boldsymbol{A}</script> and <script type="math/tex">\boldsymbol{B}</script>. These matrices are called similar if there exists a matrix <script type="math/tex">\boldsymbol{M}</script> such that <script type="math/tex">\boldsymbol{B} = {\boldsymbol{M}^{ - 1}}\boldsymbol{AM}</script>. From the eigenvalue decomposition, we can show that  <script type="math/tex">{\boldsymbol{S}^{ - 1}}\boldsymbol{AS = }\Lambda</script> which implies that <script type="math/tex">\boldsymbol{A}</script> is similar to the eigenvalue matrix <script type="math/tex">\Lambda</script>. From here we can now define a family of similar matrices where we can transform the matrix such that for any invertible matrix <script type="math/tex">\boldsymbol{D}</script> we can write <script type="math/tex">{\boldsymbol{D}^{ - 1}}{\boldsymbol{S}^{ - 1}}\boldsymbol{ASD = }{\boldsymbol{D}^{ - 1}}\Lambda \boldsymbol{D}</script>. Notice that <script type="math/tex">{\boldsymbol{D}^{ - 1}}\Lambda \boldsymbol{D}</script> is another eigenvalue decomposition with the same eigenvalues but different eigenvectors. Hence <script type="math/tex">\boldsymbol{A}</script> is similar to all other matrices with the same eigenvalues.</p>

<p>Therefore, similar matrices must have the same eigenvalues. To show this, we can write the eigenvalue decomposition as <script type="math/tex">\boldsymbol{Ax} = \lambda \boldsymbol{x}</script>. Now suppose that we have a similar matrix <script type="math/tex">\boldsymbol{B} = {\boldsymbol{M}^{ - 1}}\boldsymbol{AM}</script>. We can write out the eigenvalue decomposition as</p>

<script type="math/tex; mode=display">\begin{array}{l}\boldsymbol{Ax} = \lambda \boldsymbol{x}\\ \Rightarrow \boldsymbol{MB}{\boldsymbol{M}^{ - 1}}\boldsymbol{x = }\lambda \boldsymbol{x}\\ \Rightarrow \boldsymbol{B}{\boldsymbol{M}^{ - 1}}\boldsymbol{x = }\lambda {\boldsymbol{M}^{ - 1}}\boldsymbol{x}\\ \Rightarrow \boldsymbol{By = }\lambda \boldsymbol{y}\end{array}</script>

<p>Therefore, we see that the eigenvalues are preserved, but the eigenvectors have been transformed <script type="math/tex">\boldsymbol{y} = {\boldsymbol{M}^{ - 1}}\boldsymbol{x}</script>.</p>

<p>Suppose that we now consider matrices with the same eigenvalues which may not be diagonalizable. Consider a matrix which is a multiple of the identity <script type="math/tex">\boldsymbol{A} = c\boldsymbol{I}</script> where all the eigenvalues equal <script type="math/tex">c</script>. We see that multiplying by any invertible matrix does not change the matrix since <script type="math/tex">{\boldsymbol{M}^{ - 1}}\boldsymbol{AM} = c{\boldsymbol{M}^{ - 1}}\boldsymbol{IM} = c\boldsymbol{I}</script>. Therefore, the matrix <script type="math/tex">\boldsymbol{A} = c\boldsymbol{I}</script> only has one matrix in its family.</p>

<p>If we now look at the matrix <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}4&1\\0&4\end{array}} \right] %]]></script>, clearly the eigenvalues are both 4. However, this matrix has a larger number of similar matrices than <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}4&0\\0&4\end{array}} \right] %]]></script> because any matrix <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}4&c\\0&4\end{array}} \right] %]]></script> will be similar. The most diagonal representative from each family of similar matrices is called the Jordan normal form. This enables us to effectively “complete” the diagonalization process for matrices which cannot be diagonalized.</p>

<p>A Jordan block is a partitioning of the matrix where each block has one eigenvector. Every square matrix <script type="math/tex">\boldsymbol{A}</script> is similar to a Jordan matrix <script type="math/tex">\boldsymbol{J}</script>, where <script type="math/tex">\boldsymbol{J}</script> is a partitioned matrix where each block has one associated eigenvector. To understand this, start with any <script type="math/tex">\boldsymbol{A}</script>. If the eigenvalues are distinct, then the matrix is diagonalizable and this matrix is similar to its diagonal eigenvector matrix. In this case, the Jordan form is <script type="math/tex">\boldsymbol{J} = \Lambda</script>. Now, if the eigenvalues are repeated and “missing” eigenvectors, then its Jordan matrix will have partitioned blocks that each have one associated eigenvector.</p>

<p><a name="orthogonal-matrices"></a></p>

<p><br /></p>

<hr />
<h4 id="orthogonal-matrices">Orthogonal Matrices</h4>
<hr />

<p>Consider that we have a projection (or expansion) onto an orthonormal basis defined by <script type="math/tex">{\boldsymbol{q}_1}, \ldots ,{\boldsymbol{q}_n}</script>. We can write this projection as <script type="math/tex">\boldsymbol{v} = {x_1}{\boldsymbol{q}_1} +  \cdots  + {x_n}{\boldsymbol{q}_n}</script>. An important question is how we find the corresponding coefficients. For an orthonormal basis, this process is very easy, we can just take the dot product of both sides of this equation by the corresponding basis. For example, for the <script type="math/tex">{k^{th}}</script> coefficient, the coefficient is defined as <script type="math/tex">\boldsymbol{q}_k^T\boldsymbol{v = }{x_k}</script>. We can write this in matrix notation as <script type="math/tex">\boldsymbol{Qx = v}</script>. Given that <script type="math/tex">{\boldsymbol{Q}^{ - 1}} = {\boldsymbol{Q}^T}</script>, we can write <script type="math/tex">\boldsymbol{x = }{\boldsymbol{Q}^T}\boldsymbol{v}</script>.</p>

<p>An orthogonal matrix is defined as a matrix where each column and row is perpendicular to every other column and row respectively. Specifically, a matrix <script type="math/tex">\boldsymbol{Q}</script>is called orthonormal if <script type="math/tex">{\boldsymbol{Q}^T}\boldsymbol{Q} = \boldsymbol{I}</script> where <script type="math/tex">\boldsymbol{I}</script> is the identity matrix. We can write this matrix in a rank-1 decomposition as <script type="math/tex">{\boldsymbol{Q}^T}\boldsymbol{Q} = {\boldsymbol{q}_1}\boldsymbol{q}_1^T +  \cdots  + {\boldsymbol{q}_n}\boldsymbol{q}_n^T = \boldsymbol{I}</script>.</p>

<p>We can show that if <script type="math/tex">\boldsymbol{Q}</script> has orthonormal columns then it must have orthonormal rows as well. If <script type="math/tex">\boldsymbol{Q}</script> has orthonormal columns, then it follows that <script type="math/tex">{\boldsymbol{Q}^T}\boldsymbol{Q} = \boldsymbol{I}</script>. By left multiplying this expression by <script type="math/tex">\boldsymbol{Q}</script>, then we can rearrange the expression as follows</p>

<script type="math/tex; mode=display">\begin{array}{l}{\boldsymbol{Q}^T}\boldsymbol{Q} = \boldsymbol{I}\\\boldsymbol{Q}{\boldsymbol{Q}^T}\boldsymbol{Q} = \boldsymbol{Q}\\\left( {\boldsymbol{Q}{\boldsymbol{Q}^T} - \boldsymbol{I}} \right)\boldsymbol{Q} = \boldsymbol{0}\end{array}</script>

<p>Given that <script type="math/tex">\boldsymbol{Q}</script> has full rank (since all columns are mutually orthogonal and nonzero), it follows that any nonzero row in <script type="math/tex">\boldsymbol{Q}{\boldsymbol{Q}^T} - \boldsymbol{I}</script> can be written as a linear combination of the columns of <script type="math/tex">\boldsymbol{Q}</script>. As such, the product of a nonzero row of <script type="math/tex">\boldsymbol{Q}{\boldsymbol{Q}^T} - \boldsymbol{I}</script> with <script type="math/tex">\boldsymbol{Q}</script> must result in a nonzero vector. Therefore, since the product of all rows of <script type="math/tex">\boldsymbol{Q}{\boldsymbol{Q}^T} - \boldsymbol{I}</script> with <script type="math/tex">\boldsymbol{Q}</script> result in zero vectors (as seen from the equation <script type="math/tex">\left( {\boldsymbol{Q}{\boldsymbol{Q}^T} - \boldsymbol{I}} \right)\boldsymbol{Q} = \boldsymbol{0}</script>), it follows that <script type="math/tex">\boldsymbol{Q}{\boldsymbol{Q}^T} - \boldsymbol{I} = 0</script> which implies that <script type="math/tex">\boldsymbol{Q}{\boldsymbol{Q}^T} = \boldsymbol{I}</script>. From this result we see that <script type="math/tex">\boldsymbol{Q}{\boldsymbol{Q}^T} = {\boldsymbol{Q}^T}\boldsymbol{Q} = \boldsymbol{I}</script>. Therefore, <script type="math/tex">{\boldsymbol{Q}^{ - 1}}\boldsymbol{ = }{\boldsymbol{Q}^T}</script> and the rows of <script type="math/tex">\boldsymbol{Q}</script> must also be orthonormal.</p>

<p><a name="graph-matrices"></a></p>

<p><br /></p>

<hr />
<h4 id="graph-matrices">Graph Matrices</h4>
<hr />

<p>A graph is a set of nodes and edges connecting the nodes. In the incidence matrix, every row corresponds to an edge. Loops in the graph correspond to linearly dependent rows. If we have an incidence matrix <script type="math/tex">\boldsymbol{A}</script> , how can we interpret the <script type="math/tex">\boldsymbol{x}</script> in the product <script type="math/tex">\boldsymbol{Ax}</script>? We can think about the element <script type="math/tex">{\boldsymbol{x}_k}</script> as representing the potential of the <script type="math/tex">{k^{th}}</script> node. Therefore, the product <script type="math/tex">\boldsymbol{Ax}</script> tells us the potential differences between the nodes. In this way, the nullspace of <script type="math/tex">\boldsymbol{Ax}</script> can be visualized as the settings of the potentials <script type="math/tex">\boldsymbol{x}</script> such that the potential differences are 0. Therefore, we can write the vectors in the nullspace as <script type="math/tex">\boldsymbol{x} = c\left[ {\begin{array}{*{20}{c}}1\\ \vdots \\1\end{array}} \right]</script> where <script type="math/tex">c</script> is a constant. In terms of an electrical network, the potential difference is zero on each edge if each node has the same potential. We can’t tell what that potential is by observing the flow of electricity through the network, since only the potential differences correspond to flow. Therefore, only if one of the nodes is grounded to potential 0 can we determine the absolute potential of all other nodes in the graph. If we ground a node in the network, we are setting one of the nodes in <script type="math/tex">\boldsymbol{x}</script> to 0, which effectively removes one of the columns from <script type="math/tex">\boldsymbol{A}</script>.</p>

<p>For an incidence matrix <script type="math/tex">\boldsymbol{A}</script> what does the nullspace of  <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{y = 0}</script> correspond to? We can think about the element <script type="math/tex">{\boldsymbol{y}_l}</script> as representing the current flowing along the <script type="math/tex">{l^{th}}</script> edge. Therefore, the sum of the currents into and out of every node must be 0. The matrix product <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{y}</script> tells us the net current at each node. Hence the solutions to <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{y = 0}</script> are the currents that follow Kirchhoff’s current law. The basis vectors for the nullspace <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{y = 0}</script> are the currents required for each of the loops to satisfy Kirchhoff’s current law. We can add current sources to <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{y = f}</script> where <script type="math/tex">\boldsymbol{f}</script> are the current sources.</p>

<p><a name="change-of-basis"></a></p>

<p><br /></p>

<hr />
<h2 id="change-of-basis">Change of Basis</h2>
<hr />

<p>A central question in linear algebra is the following: given a vector space <script type="math/tex">V</script>, what is the best basis to choose to represent the vectors? For example, the vector space <script type="math/tex">V</script> could the space of all grayscale intensity images, where each image is a vector in this space. A grayscale intensity image by definition is a <script type="math/tex">m\,\, \times \,\,n</script> matrix in <script type="math/tex">{\mathbb{R}^{m\,\, \times \,\,n\,\,}}</script>. The most straightforward basis to choose is simply the standard basis where the <script type="math/tex">\left( {i,j} \right)</script> element of the coordinate vector describes the pixel intensity of the <script type="math/tex">\left( {i,j} \right)</script> element of the image. However, for specific applications such as image compression, this is not a very helpful basis, as we would prefer a basis where each basis vector describes global characteristics of the image. Several different basis representations have been researched include the following</p>

<ul>
  <li><strong>Fourier Basis:</strong> The Fourier basis is composed of complex exponentials and can be interpreted as representing a vector in “frequency” domain. This can be useful for a multitude of applications including filtering and compression</li>
  <li><strong>Wavelet Basis:</strong> Wavelets are another orthogonal basis that like the Fourier basis, attempts to represent a vector in a “frequency” domain. Wavelets have several properties which are different than the Fourier basis.</li>
  <li><strong>Eigenvalue Basis:</strong> If we represent a vector using the eigenvalue basis determined from a matrix, then the linear transformation is simply a scaling. This has great advantage when recurrently applying a transformation as in the application of difference equations.</li>
</ul>

<p><a name="fourier-basis"></a></p>

<p><br /></p>

<hr />
<h4 id="fourier-basis">Fourier Basis</h4>
<hr />

<p>The goal of Fourier analysis is to represent a signal using a set of complex exponential basis functions. The primary reasons why complex exponentials are used as basis functions is because they have the following properties:</p>

<ol>
  <li><strong>Periodic:</strong> A complex exponential basis function has a single associated frequency. Therefore, by using complex exponentials as basis functions, we can represent a signal in terms of composite frequency components. By computing the projection of the signal onto these basis functions, we can effectively transform a signal from the time/spatial domain into the frequency domain.</li>
  <li><strong>Orthogonal:</strong> Having orthogonal basis functions leads to the derivation of elegant and simple formulas to calculate the transform coefficients when projecting the signal onto the basis functions.</li>
</ol>

<p>To begin our adventure into Fourier analysis, let us start with a question. Suppose we have a periodic real valued function <script type="math/tex">f\left( t \right)</script> that has period <script type="math/tex">P</script> such that <script type="math/tex">f\left( t \right) = f\left( {t + P} \right)</script>. Is it possible to find a set of coefficients <script type="math/tex">{c_k}</script> such that we can represent the signal for <script type="math/tex">N \to \infty</script> as:</p>

<script type="math/tex; mode=display">f\left( t \right) = \sum\limits_{n =  - N}^N {{c_n}{e^{i\frac{{2\pi }}{P}nt}}}</script>

<p>To unpack this question, let us first examine the contents of the above equation. The complex exponential can be expanded as <script type="math/tex">{e^{i\frac{{2\pi }}{P}nt}} = \cos \left( {\frac{{2\pi }}{P}nt} \right) + i\sin \left( {\frac{{2\pi }}{P}nt} \right)</script>. Therefore, we see that the period of each complex exponential is <script type="math/tex">\frac{P}{n}</script>. A couple interesting things to note: 1) when <script type="math/tex">n = 0</script> the complex exponential reduces down to <script type="math/tex">{e^{i\frac{{2\pi }}{P}0t}} = 1</script> which is the constant function. As such, we can interpret the coefficient <script type="math/tex">{c_0}</script> as specifying the average value of the function. 2) the smallest nonzero frequency we have in the sum is when <script type="math/tex">n =  \pm 1</script> and the frequency is <script type="math/tex">\pm \frac{1}{P}</script>. The reason why we don’t use a frequency smaller than <script type="math/tex">\pm \frac{1}{P}</script> is that our signal is known to have a base frequency of <script type="math/tex">\frac{1}{P}</script>, therefore, we don’t need any smaller frequencies to describe the signal. 3) The reason why we have negative values of <script type="math/tex">n</script> is so that the <script type="math/tex">{c_k}</script> values can be complex conjugates for <script type="math/tex">\pm k</script> so that the sum produces a real (potentially phase shifted) signal.</p>

<p>With this said, the central question becomes: how can we find the values for the coefficients <script type="math/tex">{c_k}</script>? This is where we can exploit the orthogonality of the complex exponentials. We can compute the projection of the signal onto the <script type="math/tex">{k^{th}}</script> basis vector by multiplying by the <script type="math/tex">{k^{th}}</script> basis vector and integrating over the period <script type="math/tex">P</script> in the following way:</p>

<script type="math/tex; mode=display">\begin{array}{l}f\left( t \right) = \sum\limits_{n =  - N}^N {{c_n}{e^{i\frac{{2\pi }}{P}nt}}} \\ \Rightarrow \int\limits_0^P {{e^{ - i\frac{{2\pi }}{P}kt}}f\left( t \right)dt}  = \int\limits_0^P {{e^{ - i\frac{{2\pi }}{P}kt}}\sum\limits_{n =  - N}^N {{c_n}{e^{i\frac{{2\pi }}{P}nt}}} dt} \\ \Rightarrow \int\limits_0^P {{e^{ - i\frac{{2\pi }}{P}kt}}f\left( t \right)dt}  = \sum\limits_{n =  - N}^N {\int\limits_0^P {{c_n}{e^{ - i\frac{{2\pi }}{P}kt}}{e^{i\frac{{2\pi }}{P}nt}}dt} } \\ \Rightarrow \int\limits_0^P {{e^{ - i\frac{{2\pi }}{P}kt}}f\left( t \right)dt}  = P{c_k}\\ \Rightarrow {c_k} = \frac{1}{P}\int\limits_0^P {{e^{ - i\frac{{2\pi }}{P}kt}}f\left( t \right)dt} \end{array}</script>

<p>We call this equation for the coefficient the “analysis” equation. The key to this result is the orthogonality of the basis vectors which tells us that</p>

<script type="math/tex; mode=display">\int\limits_0^P {{e^{ - i\frac{{2\pi }}{P}kt}}{e^{i\frac{{2\pi }}{P}nt}}dt}  = \left\{ {\begin{array}{*{20}{c}}{0\,\,\,k \ne n\,}\\{P\,\,k = n\,}\end{array}} \right.</script>

<p>Hence, we now have a formula for the coefficients of this series. We call this series the Fourier series of <script type="math/tex">f\left( t \right)</script>.</p>

<p>We can write the series representation for <script type="math/tex">f\left( t \right)</script> as an equation known as the “synthesis” equation such that <script type="math/tex">f\left( t \right) = \sum\limits_{n =  - N}^N {{c_n}{e^{i\frac{{2\pi }}{P}nt}}}</script>.</p>

<p>A limitation of the Fourier series is that it assumes that <script type="math/tex">f\left( t \right)</script> is periodic with period <script type="math/tex">P</script>. How could we extend the concept of the Fourier series to handle nonperiodic functions? One approach is to continue assuming that <script type="math/tex">f\left( t \right)</script> is periodic, but let the period get very large such that <script type="math/tex">P \to \infty</script>. If we take the derivation that we developed for the coefficients of the Fourier series (written below with a shifted integral bound) and write out the synthesis equation for the Fourier series we get</p>

<script type="math/tex; mode=display">f\left( t \right) = \sum\limits_{n =  - N}^N {\left( {\frac{1}{P}\int\limits_{ - \frac{P}{2}}^{\frac{P}{2}} {{e^{ - i\frac{{2\pi }}{P}nt}}f\left( t \right)dt} } \right){e^{i\frac{{2\pi }}{P}nt}}}</script>

<p>We can define the frequency of the complex exponential as a function <script type="math/tex">\omega \left( n \right) = \frac{n}{P}</script>. We see that the difference between any two successive frequencies is <script type="math/tex">\Delta \omega  = \omega \left( {n + 1} \right) - \omega \left( n \right) = \frac{{n + 1}}{P} - \frac{n}{P} = \frac{1}{P}</script>. Substituting into the expression, we see</p>

<script type="math/tex; mode=display">f\left( t \right) = \sum\limits_{n =  - N}^N {\left( {\Delta \omega \int\limits_{ - \frac{P}{2}}^{\frac{P}{2}} {{e^{ - i2\pi \omega t}}f\left( t \right)dt} } \right){e^{i2\pi \omega t}}}</script>

<p>Now, if we: 1) let <script type="math/tex">P \to \infty</script>, so <script type="math/tex">\Delta \omega  \to d\omega</script>, 2) let <script type="math/tex">N \to \infty</script>, and 3) swap the sum for an integral over <script type="math/tex">\omega</script>, then we get the following</p>

<script type="math/tex; mode=display">f\left( t \right) = \int\limits_{ - \infty }^\infty  {\left( {\int\limits_{ - \infty }^\infty  {{e^{ - i2\pi \omega t}}f\left( t \right)dt} } \right){e^{i2\pi \omega t}}d\omega }</script>

<p>The inner integral <script type="math/tex">F\left( \omega  \right) = \int\limits_{ - \infty }^\infty  {{e^{ - i2\pi \omega t}}f\left( t \right)dt}</script> gives a function over <script type="math/tex">\omega</script> and this function is the Fourier transform of <script type="math/tex">f\left( t \right)</script>. The outer integral can be interpreted as the inverse Fourier transform of <script type="math/tex">F\left( \omega  \right)</script>.</p>

<p>An important question to understand is: what is the bound of <script type="math/tex">\int\limits_{ - \infty }^\infty  {{e^{ - i2\pi \omega t}}f\left( t \right)dt}</script>? We can safely assume that every real-life signal will have finite energy which means that<script type="math/tex">\int\limits_{ - \infty }^\infty  {\left\lvert  {f\left( t \right)} \right\rvert dt}  = E</script>. Now using the fact that <script type="math/tex">{e^{ - i2\pi \omega t}}</script> is periodic with amplitude 1, we see that <script type="math/tex">\int\limits_{ - \infty }^\infty  {\left\lvert  {{e^{ - i2\pi \omega t}}f\left( t \right)} \right\rvert dt}  \le \int\limits_{ - \infty }^\infty  {\left\lvert  {f\left( t \right)} \right\rvert dt}  = E</script>.</p>

<p>Since any periodic signal can be written in terms of frequency components that are integer multiples of the base frequency of the signal, we only need a countably infinite number of complex exponentials to represent the periodic signal. This is the basis of the Fourier series. On the other hand, for infinite time signals that are aperiodic, we need to use all frequencies on the real number line to represent the signal. This leads to the Fourier integral transform where our integral produces a coefficient for each real frequency <script type="math/tex">\omega</script>.</p>

<p>To develop intuition for the Fourier transform, we can think about the difference between the Fourier series and the Fourier transform, and the analogous relations between PMFs and PDFs from probability theory. For the Fourier series, we had a coefficient for each discrete frequency <script type="math/tex">\frac{n}{P}</script>, with some of these frequencies having nonzero energy. This is analogous to the probability being located on discrete positions in the PMF.</p>

<p>Disclaimer: my knowledge of measure theory is minimal. Therefore, I have constructed this section mostly based on handwaving intuition. Please take it with a grain of salt.</p>

<p>Suppose that we want to represent the probability of landing anywhere on the real number line between <script type="math/tex">\left( {0,1} \right]</script>. Let us try to construct a random variable that can represent this distribution. Suppose that we have a random variable <script type="math/tex">X</script> associated with a uniform discrete distribution over the domain <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}{\frac{1}{N}}&{\frac{2}{N}}& \cdots &{\frac{N}{N}}\end{array}} \right] %]]></script>. The PMF associated with this variable is <script type="math/tex">P\left( {X = \frac{k}{N}} \right) = \frac{1}{N}</script> for any <script type="math/tex">k \in \left\{ {1,2, \ldots ,N} \right\}</script>. Consider what happens as we let <script type="math/tex">N \to \infty</script>, we see that the PMF goes to zero for every point <script type="math/tex">\mathop {\lim }\limits_{N \to \infty } P\left( {X = \frac{k}{N}} \right) = \mathop {\lim }\limits_{N \to \infty } \frac{1}{N} = 0</script>. This poses a problem if we want the PMF to describe the entire real number line because all probabilities go to zero.</p>

<p>However, we have an even bigger problem. The fundamental problem is that we are trying to describe an interval on the real number line between <script type="math/tex">\left( {0,1} \right]</script> using a countably infinite set of points <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}{\frac{1}{N}}&{\frac{2}{N}}& \cdots &{\frac{N}{N}}\end{array}} \right] %]]></script>. As a result of measure theory, we know that the total measure of all the points in <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}{\frac{1}{N}}&{\frac{2}{N}}& \cdots &{\frac{N}{N}}\end{array}} \right] %]]></script> will be smaller than any interval of the real number line. Therefore, even if we let <script type="math/tex">N \to \infty</script>, the domain <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}{\frac{1}{N}}&{\frac{2}{N}}& \cdots &{\frac{N}{N}}\end{array}} \right] %]]></script> cannot describe the complete interval <script type="math/tex">\left( {0,1} \right]</script>.</p>

<p>Therefore, we need a new approach to represent probabilities on the real number line. One approach is to define a primitive measure, such that we can divide the real number line into a countable collection of these primitive measures. Then we can assign probability values to these primitive measures such that they sum to 1. A suitable primitive measure to represent an interval on the real number line is an infinitesimal interval since we can clearly represent any interval as a countable collection of smaller intervals.</p>

<p>This pattern of defining a primitive measure that we can use to use to represent the domain of our random variable as a countable set is key as it enables us to assign a nonzero probability to some of the primitive measures. This process can be generalized for different dimension spaces. For example: for a domain of discrete points, a primitive measure is a single point, for a domain of intervals of the real line, a primitive measure is an infinitesimal interval, for a domain of areas, a primitive measure is an infinitesimal area, for a domain of volumes, a primitive measure is an infinitesimal volume, etc..</p>

<p>Suppose that we represent the interval <script type="math/tex">\left( {0,1} \right]</script> as an ordered collection of intervals given as <script type="math/tex">\left\{ {\left( {0,\frac{1}{N}} \right],\left( {0,\frac{2}{N}} \right], \ldots ,\left( {0,\frac{N}{N}} \right]} \right\}</script>. Now suppose that each of these intervals has an associated probability value such that <script type="math/tex">F\left( {\frac{k}{N}} \right)</script> is the probability associated with interval <script type="math/tex">\left( {0,\frac{k}{N}} \right]</script>. Then we can write the sum of these probabilities as <script type="math/tex">\sum\limits_{n = 1}^N {F\left( {\frac{n}{N}} \right)}  = 1</script>. For a uniform distribution, the associated probability is <script type="math/tex">F\left( {\frac{k}{N}} \right) = \frac{1}{N}</script>. However, notice that as <script type="math/tex">N \to \infty</script>, we have <script type="math/tex">F\left( {\frac{k}{N}} \right) \to 0</script>. Is there a nicer form that we can use to represent the probability that does not converge to zero? Here comes the key piece of innovation: suppose that we define another function that describes the probability density of the interval such that <script type="math/tex">f\left( {\frac{k}{N}} \right)\frac{1}{N} = F\left( {\frac{k}{N}} \right)</script> where <script type="math/tex">\frac{1}{N}</script> is the length of the interval. In the case of the uniform distribution, we see that as <script type="math/tex">N \to \infty</script>, <script type="math/tex">f\left( {\frac{k}{N}} \right) = 1</script>. Hence, by dealing with the probability density, as <script type="math/tex">N \to \infty</script> the density will not converge to zero.</p>

<p>Suppose in the general case that our associated probability <script type="math/tex">F\left( {\frac{k}{N}} \right)</script> is not constrained to be uniform. In this case,  <script type="math/tex">F\left( {\frac{k}{N}} \right)</script> can be any positive value such that the probabilities sum to 1, given by  <script type="math/tex">\sum\limits_{n = 1}^N {F\left( {\frac{n}{N}} \right)}  = 1</script>. Now substitute in our expression for the density as <script type="math/tex">\sum\limits_{n = 1}^N {f\left( {\frac{n}{N}} \right)\frac{1}{N}}  = 1</script>. Taking the limit, we see that <script type="math/tex">\mathop {\lim }\limits_{N \to \infty } \sum\limits_{n = 1}^N {f\left( {\frac{n}{N}} \right)\frac{1}{N}}  = \int\limits_0^1 {f\left( x \right)dx}  = 1</script>. Here we have substituted the limiting sum with a Riemann integral.</p>

<p>Therefore, an effective way to represent an arbitrary probability distribution is using the probability density which describes the probability per unit length. An intuitive way to think about the integral <script type="math/tex">\int\limits_0^1 {f\left( x \right)dx}</script>, is as follows: 1) this integral represents a countably infinite sum of primitive measures <script type="math/tex">dx</script> that have been scaled by <script type="math/tex">f\left( x \right)</script>. The interval from 0 to 1 is completely described by all the <script type="math/tex">dx</script> in the sum. 2) the function <script type="math/tex">f\left( x \right)</script> can be interpreted as the density of some quantity such as probability. The product <script type="math/tex">f\left( x \right)dx</script> computes the probability over an infinitesimal interval.</p>

<p>Returning to the Fourier transform, since we need all real frequencies <script type="math/tex">\omega</script> to describe an infinite time aperiodic signal, we need to consider the signal synthesis equation <script type="math/tex">f\left( t \right) = \int\limits_{ - \infty }^\infty  {\hat f\left( \omega  \right){e^{i2\pi \omega t}}d\omega }</script> as being an integral over the domain of <script type="math/tex">\omega</script>, where the primitive measure is an infinitesimal frequency interval of length <script type="math/tex">d\omega</script>.</p>

<p>Now that we have seen the Fourier series for signals that are periodic, and the Fourier transform for signals that are aperiodic, what about signals that are discretely sampled? One way of modeling the process of sampling is to multiply a signal by an impulse train. An impulse train is defined as <script type="math/tex">{\Psi _T}\left( t \right) = T\sum\limits_{n - \infty }^\infty  {\delta \left( {t - nT} \right)}</script> where <script type="math/tex">T</script> is the sampling interval.</p>

<p>To be able to use the Fourier series, we need to have a periodic signal. We can force our signal to be periodic by simply repeating it. Let us assume that our signal has length <script type="math/tex">P</script> and hence, by repeating the signal, the repeated signal has period <script type="math/tex">P</script>. We will assume that the sampling interval is a multiple of the period such that<script type="math/tex">P = NT</script>. We can write the Fourier series coefficients as follows</p>

<script type="math/tex; mode=display">\begin{array}{l}{c_k} = \frac{1}{P}\int\limits_{ - \varepsilon }^{P - \varepsilon } {{e^{ - i\frac{{2\pi }}{P}kt}}T\sum\limits_{n - \infty }^\infty  {\delta \left( {t - nT} \right)} f\left( t \right)dt} \\ = \frac{T}{P}\sum\limits_{n = 0}^{N - 1} {{e^{ - i\frac{{2\pi }}{P}knT}}f\left( {nT} \right)} \end{array}</script>

<p>Since <script type="math/tex">P = NT</script>, we can rewrite the coefficient equation as: <script type="math/tex">{c_k} = \frac{1}{N}\sum\limits_{n = 0}^{N - 1} {{e^{ - i2\pi \frac{{kn}}{N}}}f\left( {nT} \right)}</script>.</p>

<p>If we define <script type="math/tex">{\hat c_k} = N{c_k}</script> such that <script type="math/tex">{\hat c_k} = \sum\limits_{n = 0}^{N - 1} {{e^{ - i2\pi \frac{{kn}}{N}}}f\left( {nT} \right)}</script>, then we can write the equations for all coefficients as:</p>

<script type="math/tex; mode=display">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}{{{\hat c}_0}}\\ \vdots \\{{{\hat c}_{N - 1}}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}{{e^{ - i2\pi \frac{{\left( 0 \right)\left( 0 \right)}}{N}}}}& \cdots &{{e^{ - i2\pi \frac{{\left( 0 \right)\left( {N - 1} \right)}}{N}}}}\\ \vdots & \ddots & \vdots \\{{e^{ - i2\pi \frac{{\left( {N - 1} \right)\left( 0 \right)}}{N}}}}& \cdots &{{e^{ - i2\pi \frac{{\left( {N - 1} \right)\left( {N - 1} \right)}}{N}}}}\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{f\left( {0T} \right)}\\ \vdots \\{f\left( {\left( {N - 1} \right)T} \right)}\end{array}} \right] %]]></script>

<p>If we let <script type="math/tex">\omega  = {e^{ - i\frac{{2\pi }}{N}}}</script>, then we can write the system of equations as:</p>

<script type="math/tex; mode=display">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}{{{\hat c}_0}}\\ \vdots \\{{{\hat c}_{N - 1}}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}{{\omega ^{\left( 0 \right)\left( 0 \right)}}}& \cdots &{{\omega ^{\left( 0 \right)\left( {N - 1} \right)}}}\\ \vdots & \ddots & \vdots \\{{\omega ^{\left( {N - 1} \right)\left( 0 \right)}}}& \cdots &{{\omega ^{\left( {N - 1} \right)\left( {N - 1} \right)}}}\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{f\left( {0T} \right)}\\ \vdots \\{f\left( {\left( {N - 1} \right)T} \right)}\end{array}} \right] %]]></script>

<p>The matrix <script type="math/tex">% <![CDATA[
{\boldsymbol{F}_N} = \left[ {\begin{array}{*{20}{c}}{{\omega ^{\left( 0 \right)\left( 0 \right)}}}& \cdots &{{\omega ^{\left( 0 \right)\left( {N - 1} \right)}}}\\ \vdots & \ddots & \vdots \\{{\omega ^{\left( {N - 1} \right)\left( 0 \right)}}}& \cdots &{{\omega ^{\left( {N - 1} \right)\left( {N - 1} \right)}}}\end{array}} \right] %]]></script> is called the N-point DFT matrix and will be unitary if scaled by <script type="math/tex">\frac{1}{{\sqrt N }}</script>, such that <script type="math/tex">{\left( {\frac{1}{{\sqrt N }}{\boldsymbol{F}_N}} \right)^{ - 1}} = \frac{1}{{\sqrt N }}\boldsymbol{F}_N^H</script>. We can interpret this matrix as performing a change of basis from “time” domain to “frequency” domain.</p>

<p><a name="matrix-factorizations"></a></p>

<p><br /></p>

<hr />
<h2 id="matrix-factorizations">Matrix Factorizations</h2>
<hr />

<p>In this section we will discuss some of the important matrix factorizations.</p>

<p><a name="cr"></a></p>

<p><br /></p>

<hr />
<h4 id="cr">CR</h4>
<hr />

<p>Suppose we have a 3 x 3 matrix <script type="math/tex">\boldsymbol{A}</script> defined as</p>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{A} = \left[ {\begin{array}{*{20}{c}}1&4&5\\3&2&5\\2&1&3\end{array}} \right] %]]></script>

<p>We can write <script type="math/tex">\boldsymbol{A}</script> as the product of two matrices <script type="math/tex">\boldsymbol{C}</script> and <script type="math/tex">\boldsymbol{R}</script> such that</p>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{A} = \boldsymbol{CR} = \left[ {\begin{array}{*{20}{c}}1&4\\3&2\\2&1\end{array}} \right]\left[ {\begin{array}{*{20}{c}}1&0&1\\0&1&1\end{array}} \right] %]]></script>

<p>Where <script type="math/tex">\boldsymbol{C}</script> is constructed to contain the linearly independent columns of <script type="math/tex">\boldsymbol{A}</script>, and <script type="math/tex">\boldsymbol{R}</script> contains the coefficients for the linear combinations of these columns to produce <script type="math/tex">\boldsymbol{A}</script>. This decomposition provides two fundamental and important interpretations of matrix multiplication:</p>

<ol>
  <li>Each column of <script type="math/tex">\boldsymbol{A}</script> can be viewed as a linear combination of the columns of <script type="math/tex">\boldsymbol{C}</script> where the combination coefficients are given by the columns of <script type="math/tex">\boldsymbol{R}</script></li>
  <li>Each row of <script type="math/tex">\boldsymbol{A}</script> can be viewed as a linear combination of the rows of <script type="math/tex">\boldsymbol{R}</script> where the combination coefficients are given by the rows of <script type="math/tex">\boldsymbol{C}</script></li>
</ol>

<p>We can define the column rank and row rank of <script type="math/tex">\boldsymbol{A}</script> to be the number of linearly independent columns and rows respectively. From the decomposition <script type="math/tex">\boldsymbol{A} = \boldsymbol{CR}</script> we can show that the column rank must equal the row rank of any matrix <script type="math/tex">\boldsymbol{A}</script> as follows:</p>

<ol>
  <li>Suppose that <script type="math/tex">\boldsymbol{A}</script> has <script type="math/tex">r</script> linearly independent columns, then <script type="math/tex">\boldsymbol{C}</script> has <script type="math/tex">r</script> columns and by the definition of its construction they are independent</li>
  <li>Every column of <script type="math/tex">\boldsymbol{A}</script> can be written as a combination of the <script type="math/tex">r</script> columns of <script type="math/tex">\boldsymbol{C}</script> since <script type="math/tex">\boldsymbol{A} = \boldsymbol{CR}</script></li>
  <li>The <script type="math/tex">r</script> rows of <script type="math/tex">\boldsymbol{R}</script> must be independent since they contain the <script type="math/tex">r</script> by <script type="math/tex">r</script> identity matrix (this is because the <script type="math/tex">r</script> columns of <script type="math/tex">\boldsymbol{C}</script> are also columns of <script type="math/tex">\boldsymbol{A}</script> by construction, and hence, there must be a corresponding <script type="math/tex">r</script> by <script type="math/tex">r</script> identity submatrix in <script type="math/tex">\boldsymbol{R}</script>. Since <script type="math/tex">\boldsymbol{R}</script> has <script type="math/tex">r</script> rows and contains a <script type="math/tex">r</script> by <script type="math/tex">r</script> identity matrix, each row in <script type="math/tex">\boldsymbol{R}</script> contains a column which has value 1 for that row and 0 for all other rows)</li>
  <li>Every row of <script type="math/tex">\boldsymbol{A}</script> is a combination of the <script type="math/tex">r</script> rows of <script type="math/tex">\boldsymbol{R}</script> since <script type="math/tex">\boldsymbol{A} = \boldsymbol{CR}</script>, therefore the row rank of <script type="math/tex">\boldsymbol{A}</script> must be less than or equal to the column rank of <script type="math/tex">\boldsymbol{A}</script></li>
  <li>Now repeat steps 1-4 with <script type="math/tex">{\boldsymbol{A}^T}</script>, we get the result that the column rank of <script type="math/tex">\boldsymbol{A}</script> is less than or equal to the row rank of <script type="math/tex">\boldsymbol{A}</script></li>
  <li>By combining the results from step 4 and step 5, we see that the row rank must equal the column rank</li>
</ol>

<p><a name="lu"></a></p>

<p><br /></p>

<hr />
<h4 id="lu">LU</h4>
<hr />

<p>What is Lower Triangular – Upper Triangular (LU) factorization? The LU factorization states that we can factor any matrix <script type="math/tex">\boldsymbol{A}</script> with proper row and/or column permutations as <script type="math/tex">\boldsymbol{A} = \boldsymbol{LU}</script> where <script type="math/tex">\boldsymbol{L}</script> is a lower triangular matrix and <script type="math/tex">\boldsymbol{U}</script> is an upper triangular matrix. To develop intuition for this factorization, recall that we can transform a matrix <script type="math/tex">\boldsymbol{A}</script> into an upper triangular matrix by applying elementary row operations that consist of: 1) permuting two rows or columns, and 2) adding a multiple of one row to another row. If the matrix requires permutations, we can factor all the permutations into a matrix <script type="math/tex">\boldsymbol{P}</script> for row permutations, and a matrix <script type="math/tex">\boldsymbol{Q}</script> for column permutations such that <script type="math/tex">\boldsymbol{PAQ} = \boldsymbol{LU}</script>. By using these permutation matrices, we can ensure that the “add a multiple of one row to another row” operation only needs to add a multiple of an upper row of <script type="math/tex">\boldsymbol{A}</script> to a lower row of <script type="math/tex">\boldsymbol{A}</script>. This ensures that the matrix required to represent this operation is lower triangular. Hence, we can write the transformation of <script type="math/tex">\boldsymbol{A}</script> into an upper triangular matrix as <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}\boldsymbol{PAQ} = \boldsymbol{U}</script> where <script type="math/tex">{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}</script> is the product of lower triangular matrices each describing an “add a multiple of one row to another row” operation. Therefore, to show that <script type="math/tex">\boldsymbol{PAQ} = \boldsymbol{LU}</script>, we must show that <script type="math/tex">{\left( {{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}} \right)^{ - 1}}</script> exists and is lower triangular. Consider the operation <script type="math/tex">{\boldsymbol{E}_1}</script>, which adds a multiple of one row to another row. The inverse of this operation is simply to subtract the same multiple of one row from the other row. Therefore, the inverse of <script type="math/tex">{\boldsymbol{E}_1}</script> exists and furthermore is lower triangular since to compute the inverse of <script type="math/tex">{\boldsymbol{E}_1}</script>, we simply take the off diagonal component of <script type="math/tex">{\boldsymbol{E}_1}</script> (which is below the diagonal) and invert the sign of the element (i.e. change from add to subtract). Hence, the inverse <script type="math/tex">{\left( {{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}} \right)^{ - 1}}</script> exists and is the product of lower triangular matrices. It can be shown that the product of two lower triangular matrices is also lower triangular by examining the elements of the resultant matrix and showing that the elements above the diagonal must all be 0. Therefore, the matrix <script type="math/tex">\boldsymbol{L} = {\left( {{\boldsymbol{E}_M} \cdots {\boldsymbol{E}_1}} \right)^{ - 1}}</script> exists and is lower triangular. Hence, we have the factorization <script type="math/tex">\boldsymbol{PAQ} = \boldsymbol{LU}</script>. In the special case that <script type="math/tex">\boldsymbol{A}</script> already has proper row and/or column permutations, then <script type="math/tex">\boldsymbol{P} = \boldsymbol{Q = I}</script> and <script type="math/tex">\boldsymbol{A} = \boldsymbol{LU}</script>.</p>

<p>How many operations are required to transform an <script type="math/tex">n</script> by <script type="math/tex">n</script> matrix <script type="math/tex">\boldsymbol{A}</script> into an upper triangular matrix <script type="math/tex">\boldsymbol{U}</script>? In the worst case, for each of the rows, we must perform a multiplication and a subtraction. Therefore, in the first step, we multiply the first row and add a copy of it to all other rows. This results in approximately <script type="math/tex">{n^2}</script> steps. As we repeat this process, we see that the number of operations is roughly <script type="math/tex">{n^2} + {\left( {n - 1} \right)^2} +  \cdots  + {1^2} \approx \frac{{{n^3}}}{3}</script>.</p>

<p>Suppose we are trying to solve the system <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{b}</script>. One way to approach this problem is to factor the matrix as <script type="math/tex">\boldsymbol{A} = \boldsymbol{LU}</script>, where <script type="math/tex">\boldsymbol{L}</script> is a lower triangular matrix, and <script type="math/tex">\boldsymbol{U}</script> is an upper triangular matrix. The basic idea behind this factorization is the same as Gaussian elimination. We apply a series of lower triangular elementary operations to the matrix <script type="math/tex">\boldsymbol{A}</script> to form the upper triangular matrix <script type="math/tex">\boldsymbol{U}</script>. Upon determining the LU factorization, we can solve the system <script type="math/tex">\boldsymbol{Ax} = \boldsymbol{LUx} = \boldsymbol{b}</script> by first solving the lower triangular system <script type="math/tex">\boldsymbol{Ly} = \boldsymbol{b}</script> and then solving the upper triangular system <script type="math/tex">\boldsymbol{Ux} = \boldsymbol{y}</script>.</p>

<p><a name="qr"></a></p>

<p><br /></p>

<hr />
<h4 id="qr">QR</h4>
<hr />

<p>Another important factorization is the QR factorization which states that any matrix <script type="math/tex">\boldsymbol{A}</script> can be decomposed as <script type="math/tex">\boldsymbol{A} = \boldsymbol{QR}</script> where <script type="math/tex">\boldsymbol{Q}</script> is an orthogonal matrix and <script type="math/tex">\boldsymbol{R}</script> is an upper triangular matrix. The Gram-Schmidt process is a way of computing the QR factorization, and proceeds by iteratively computing the component for the current vector that is orthogonal to each of the previously processed vectors. Using this process, an orthonormal basis can be built, and the resulting steps can be captured in the QR factorization.</p>

<p>A matrix with orthonormal columns is represented as <script type="math/tex">{\boldsymbol{Q}^T}\boldsymbol{Q} = \boldsymbol{I}</script> . If <script type="math/tex">\boldsymbol{Q}</script> is square, then  <script type="math/tex">{\boldsymbol{Q}^T}\boldsymbol{Q} = \boldsymbol{I}</script> implies that <script type="math/tex">{\boldsymbol{Q}^{ - 1}} = {\boldsymbol{Q}^T}</script>. Suppose that <script type="math/tex">\boldsymbol{Q}</script> has orthonormal columns. If we want to project a vector onto its column space, then we can use the projection formula: <script type="math/tex">\boldsymbol{P = Q}{\left( {{\boldsymbol{Q}^T}\boldsymbol{Q}} \right)^{ - 1}}{\boldsymbol{Q}^T} = \boldsymbol{Q}{\boldsymbol{Q}^T}</script>. If <script type="math/tex">\boldsymbol{Q}</script> is square then <script type="math/tex">\boldsymbol{Q}{\boldsymbol{Q}^T} = \boldsymbol{I}</script>. For least squares, the normal equations for an orthonormal matrix reduces down to <script type="math/tex">\boldsymbol{\hat x} = {\left( {{\boldsymbol{Q}^T}\boldsymbol{Q}} \right)^{ - 1}}{\boldsymbol{Q}^T}\boldsymbol{b = }{\boldsymbol{Q}^T}\boldsymbol{b}</script>.</p>

<p>Given an <script type="math/tex">m\,\, \times \,\,n</script> matrix <script type="math/tex">\boldsymbol{A}</script> with rank <script type="math/tex">r</script>, how do we find the factorization <script type="math/tex">\boldsymbol{A} = \boldsymbol{QR}</script>? One method is to use Gram-Schmidt. The goal of Gram-Schmidt is to produce an orthonormal basis. The algorithm works as follows. Note that for simplicity we are assuming that the first <script type="math/tex">r</script> columns of <script type="math/tex">\boldsymbol{A}</script> are independent. We denote the column of <script type="math/tex">\boldsymbol{A}</script> as <script type="math/tex">% <![CDATA[
\boldsymbol{A} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{a}_1}}& \cdots &{{\boldsymbol{a}_n}}\end{array}} \right] %]]></script>.</p>
<ol>
  <li>Pick the first column <script type="math/tex">{\boldsymbol{a}_1}</script> and compute the normalized basis vector <script type="math/tex">{\boldsymbol{q}_1} = \frac{{{\boldsymbol{a}_1}}}{{\left\| {{\boldsymbol{a}_1}} \right\|}}</script>.</li>
  <li>Select the next column <script type="math/tex">{\boldsymbol{a}_2}</script> and compute the projection onto <script type="math/tex">{\boldsymbol{q}_1}</script>, written as <script type="math/tex">{\boldsymbol{p}_2} = {\boldsymbol{q}_1}\boldsymbol{q}_1^T{\boldsymbol{a}_2}</script>. Subtract the projection from <script type="math/tex">{\boldsymbol{a}_2}</script> and normalize to get the next orthogonal basis vector <script type="math/tex">{\boldsymbol{q}_2} = \frac{{{\boldsymbol{a}_2} - {\boldsymbol{p}_2}}}{{\left\| {{\boldsymbol{a}_2} - {\boldsymbol{p}_2}} \right\|}}</script>.</li>
  <li>Repeat this process for all <script type="math/tex">r</script> independent columns to establish an orthonormal basis <script type="math/tex">\left\{ {{\boldsymbol{q}_1}, \ldots ,{\boldsymbol{q}_r}} \right\}</script> for the column space of <script type="math/tex">\boldsymbol{A}</script>.</li>
</ol>

<p>Therefore, for a matrix  with rank <script type="math/tex">r</script>, the result of the Gram-Schmidt process will be an orthonormal basis for the column space <script type="math/tex">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}{{\boldsymbol{q}_1}}& \cdots &{{\boldsymbol{q}_r}}\end{array}} \right] %]]></script>. Since each column in <script type="math/tex">\boldsymbol{A}</script> is trivially in the column space. We can write each column as a linear combination <script type="math/tex">{\boldsymbol{a}_j} = {c_1}{\boldsymbol{q}_1} +  \cdots  + {c_r}{\boldsymbol{q}_r}</script>. Using orthonormality, we can determine each of the coefficients as <script type="math/tex">{c_k} = \boldsymbol{q}_k^T{\boldsymbol{a}_j}</script>. Substituting these coefficients back into the combination we get</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{array}{l}{\boldsymbol{a}_j} = {c_1}{\boldsymbol{q}_1} +  \cdots  + {c_r}{\boldsymbol{q}_r} = {\boldsymbol{q}_1}\boldsymbol{q}_1^T{\boldsymbol{a}_j} +  \cdots  + {\boldsymbol{q}_r}\boldsymbol{q}_r^T{\boldsymbol{a}_j}\\ = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{q}_1}}& \cdots &{{\boldsymbol{q}_r}}\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{\boldsymbol{q}_1^T{\boldsymbol{a}_j}}\\ \vdots \\{\boldsymbol{q}_r^T{\boldsymbol{a}_j}}\end{array}} \right]\end{array} %]]></script>

<p>Therefore, we can represent the complete matrix as</p>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{A = }\left[ {\begin{array}{*{20}{c}}{{\boldsymbol{a}_1}}& \cdots &{{\boldsymbol{a}_n}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{q}_1}}& \cdots &{{\boldsymbol{q}_r}}\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{\boldsymbol{q}_1^T{\boldsymbol{a}_1}}& \cdots &{\boldsymbol{q}_1^T{\boldsymbol{a}_n}}\\ \vdots & \ddots & \vdots \\{\boldsymbol{q}_r^T{\boldsymbol{a}_1}}& \cdots &{\boldsymbol{q}_r^T{\boldsymbol{a}_n}}\end{array}} \right] %]]></script>

<p>As a result of using Gram-Schmidt to construct the orthonormal basis, we have <script type="math/tex">\boldsymbol{q}_k^T{\boldsymbol{a}_j} = 0</script> when <script type="math/tex">k > j</script>. Therefore, we can simplify the second matrix in the product as</p>

<script type="math/tex; mode=display">% <![CDATA[
\left[ {\begin{array}{*{20}{c}}{\boldsymbol{q}_1^T{\boldsymbol{a}_1}}& \cdots &{\boldsymbol{q}_1^T{\boldsymbol{a}_n}}\\ \vdots & \ddots & \vdots \\{\boldsymbol{q}_r^T{\boldsymbol{a}_1}}& \cdots &{\boldsymbol{q}_r^T{\boldsymbol{a}_n}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}{\boldsymbol{q}_1^T{\boldsymbol{a}_1}}& \cdots &{\boldsymbol{q}_1^T{\boldsymbol{a}_n}}\\ \vdots & \ddots & \vdots \\0& \cdots &{\boldsymbol{q}_r^T{\boldsymbol{a}_n}}\end{array}} \right] %]]></script>

<p>And this matrix is upper triangular. Note that this matrix will only be invertible if <script type="math/tex">\boldsymbol{A}</script> has full column rank and <script type="math/tex">n = r</script>. Hence, setting <script type="math/tex">% <![CDATA[
\boldsymbol{Q} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{q}_1}}& \cdots &{{\boldsymbol{q}_r}}\end{array}} \right] %]]></script>, and <script type="math/tex">% <![CDATA[
\boldsymbol{R} = \left[ {\begin{array}{*{20}{c}}{\boldsymbol{q}_1^T{\boldsymbol{a}_1}}& \cdots &{\boldsymbol{q}_1^T{\boldsymbol{a}_n}}\\ \vdots & \ddots & \vdots \\0& \cdots &{\boldsymbol{q}_r^T{\boldsymbol{a}_n}}\end{array}} \right] %]]></script> we see that any matrix can be factored as <script type="math/tex">\boldsymbol{A} = \boldsymbol{QR}</script> where <script type="math/tex">\boldsymbol{Q}</script> is an orthogonal matrix of size <script type="math/tex">m\,\, \times \,\,r</script> and <script type="math/tex">\boldsymbol{R}</script> is an upper triangular matrix or size <script type="math/tex">r\,\, \times \,\,n</script>.</p>

<p>Some of the applications of QR factorization include:</p>
<ul>
  <li><strong>Solving Least Squares:</strong> The least squares solution <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{Ax} = {\boldsymbol{A}^T}\boldsymbol{b}</script> can be written using QR factorization as <script type="math/tex">{\left( {\boldsymbol{QR}} \right)^T}\boldsymbol{QRx} = {\left( {\boldsymbol{QR}} \right)^T}\boldsymbol{b} \Rightarrow {\boldsymbol{R}^T}\boldsymbol{Rx = }{\boldsymbol{R}^T}\boldsymbol{Qb}</script>. If <script type="math/tex">\boldsymbol{A}</script> has full column rank, then <script type="math/tex">\boldsymbol{R}</script> will be invertible and we can simplify the least squares equation to <script type="math/tex">\boldsymbol{Rx = Qb}</script>.</li>
  <li><strong>Pseudoinverse:</strong> Closely connected to least squares, the pseudoinverse of a matrix <script type="math/tex">\boldsymbol{A}</script> can be simplified using QR factorization as <script type="math/tex">{\left( {{\boldsymbol{A}^T}\boldsymbol{A}} \right)^{ - 1}}{\boldsymbol{A}^T} = {\left( {{{\left( {\boldsymbol{QR}} \right)}^T}\boldsymbol{QR}} \right)^{ - 1}}{\boldsymbol{R}^T}{\boldsymbol{Q}^T}\boldsymbol{ = }{\left( {{\boldsymbol{R}^T}\boldsymbol{R}} \right)^{ - 1}}{\boldsymbol{R}^T}{\boldsymbol{Q}^T}</script>. If <script type="math/tex">\boldsymbol{A}</script> has full column rank, then <script type="math/tex">\boldsymbol{R}</script> will be invertible and we can simplify this further to <script type="math/tex">{\left( {{\boldsymbol{A}^T}\boldsymbol{A}} \right)^{ - 1}}{\boldsymbol{A}^T} = {\boldsymbol{R}^{ - 1}}{\boldsymbol{Q}^T}</script>.</li>
  <li><strong>Projection onto the Column Space:</strong> Projection onto the column space of a matrix <script type="math/tex">\boldsymbol{A}</script> can be calculated using <script type="math/tex">\boldsymbol{P = A}{\left( {{\boldsymbol{A}^T}\boldsymbol{A}} \right)^{ - 1}}{\boldsymbol{A}^T} = \boldsymbol{QR}{\left( {{{\left( {\boldsymbol{QR}} \right)}^T}\boldsymbol{QR}} \right)^{ - 1}}{\boldsymbol{R}^T}{\boldsymbol{Q}^T}</script>. If <script type="math/tex">\boldsymbol{A}</script> has full column rank, then <script type="math/tex">\boldsymbol{R}</script> will be invertible and we can simplify this further to <script type="math/tex">\boldsymbol{P = Q}{\boldsymbol{Q}^T}</script>.</li>
</ul>

<p><a name="svd"></a></p>

<p><br /></p>

<hr />
<h4 id="svd">SVD</h4>
<hr />

<p>The singular value decomposition tells us that any <script type="math/tex">m</script> by <script type="math/tex">n</script> matrix <script type="math/tex">\boldsymbol{A}</script> with rank <script type="math/tex">r</script> can be factored as <script type="math/tex">\boldsymbol{A = U}\Sigma {\boldsymbol{V}^T}</script> where <script type="math/tex">\boldsymbol{U}</script> and <script type="math/tex">\boldsymbol{V}</script> are orthogonal matrices and <script type="math/tex">\Sigma</script> is a diagonal matrix.</p>

<p>By definition, a rank <script type="math/tex">r</script> matrix has a column space of dimension <script type="math/tex">r</script> which means that we need <script type="math/tex">r</script> vectors to describe a basis for the column space. Since the column rank equals the row rank, it follows that the row space also has dimension <script type="math/tex">r</script>. Since the dimensionality of these subspaces is the same, it makes intuitive sense that there should be a mapping between vectors in these subspaces. The process of finding such a mapping is the goal of the SVD.</p>

<p>The fundamental question that is answered by the SVD is the following: can we find an orthonormal basis for the row space <script type="math/tex">{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_r}</script>, such that mapping these vectors to the column space <script type="math/tex">% <![CDATA[
\boldsymbol{A}\left[ {\begin{array}{*{20}{c}}{{\boldsymbol{v}_1}}& \cdots &{{\boldsymbol{v}_r}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}{{\sigma _1}{\boldsymbol{u}_1}}& \cdots &{{\sigma _r}{\boldsymbol{u}_r}}\end{array}} \right] %]]></script> produces an orthonormal basis for the column space <script type="math/tex">{\boldsymbol{u}_1}, \ldots ,{\boldsymbol{u}_r}</script>.</p>

<p>Assuming that this factorization exists, then using matrices, we can write the mapping described above as</p>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{A}\left[ {\begin{array}{*{20}{c}}{{\boldsymbol{v}_1}}& \cdots &{{\boldsymbol{v}_r}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}{{\sigma _1}{\boldsymbol{u}_1}}& \cdots &{{\sigma _r}{\boldsymbol{u}_r}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{u}_1}}& \cdots &{{\boldsymbol{u}_r}}\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{{\sigma _1}}&{}&{}\\{}& \ddots &{}\\{}&{}&{{\sigma _r}}\end{array}} \right] %]]></script>

<p>We can go further than this by including the left and right nullspace into this equation. Given that the rank is <script type="math/tex">r</script>, the dimension of the nullspace will be <script type="math/tex">n - r</script> which means that we can describe a basis for the nullspace using <script type="math/tex">n - r</script> vectors. We can easily find orthonormal vectors which are orthogonal to <script type="math/tex">{\boldsymbol{v}_1}, \ldots ,{\boldsymbol{v}_r}</script> by using a process such as Gram-Schmidt. Let us call these orthonormal basis vectors for the nullspace <script type="math/tex">{\boldsymbol{v}_{r + 1}}, \ldots ,{\boldsymbol{v}_n}</script>. Then we can write</p>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{A}\left[ {\begin{array}{*{20}{c}}{{\boldsymbol{v}_1}}& \cdots &{{\boldsymbol{v}_r}}& \cdots &{{\boldsymbol{v}_n}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{u}_1}}& \cdots &{{\boldsymbol{u}_r}}\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{{\sigma _1}}&{}&{}&0& \cdots &0\\{}& \ddots &{}&0& \cdots &0\\{}&{}&{{\sigma _r}}&0& \cdots &0\end{array}} \right] %]]></script>

<p>Likewise, we can add vectors for the left nullspace. Given that the rank is <script type="math/tex">r</script>, the dimension of the left nullspace will be <script type="math/tex">m - r</script> . In a similar process to that described above, we can find an orthonormal basis for the left nullspace <script type="math/tex">{\boldsymbol{u}_{r + 1}}, \ldots ,{\boldsymbol{u}_m}</script>. Then we can write</p>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{A}\left[ {\begin{array}{*{20}{c}}{{\boldsymbol{v}_1}}& \cdots &{{\boldsymbol{v}_r}}& \cdots &{{\boldsymbol{v}_n}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{u}_1}}& \cdots &{{\boldsymbol{u}_r}}& \cdots &{{\boldsymbol{u}_m}}\end{array}} \right]\left[ {\begin{array}{*{20}{c}}{{\sigma _1}}&{}&{}&0& \cdots &0\\{}& \ddots &{}&0& \cdots &0\\{}&{}&{{\sigma _r}}&0& \cdots &0\\0&0&0&0& \cdots &0\\ \vdots & \vdots & \vdots & \vdots & \ddots &0\\0&0&0&0&0&0\end{array}} \right] %]]></script>

<p>We can now define:</p>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{V} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{v}_1}}& \cdots &{{\boldsymbol{v}_r}}& \cdots &{{\boldsymbol{v}_n}}\end{array}} \right] %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\boldsymbol{U} = \left[ {\begin{array}{*{20}{c}}{{\boldsymbol{u}_1}}& \cdots &{{\boldsymbol{u}_r}}& \cdots &{{\boldsymbol{u}_m}}\end{array}} \right] %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\Sigma  = \left[ {\begin{array}{*{20}{c}}{{\sigma _1}}&{}&{}&0& \cdots &0\\{}& \ddots &{}&0& \cdots &0\\{}&{}&{{\sigma _r}}&0& \cdots &0\\0&0&0&0& \cdots &0\\ \vdots & \vdots & \vdots & \vdots & \ddots &0\\0&0&0&0&0&0\end{array}} \right] %]]></script>

<p>Then our expression becomes <script type="math/tex">\boldsymbol{AV = U}\Sigma</script>. Given that <script type="math/tex">\boldsymbol{V}</script> is composed of orthonormal columns we know that <script type="math/tex">{\boldsymbol{V}^T}\boldsymbol{V} = \boldsymbol{I}</script>. Multiplying both sides of this equation by <script type="math/tex">{\boldsymbol{V}^T}</script> gives</p>

<script type="math/tex; mode=display">\begin{array}{l}{\boldsymbol{V}^T}\boldsymbol{V}{\boldsymbol{V}^T} = \boldsymbol{I}{\boldsymbol{V}^T} = {\boldsymbol{V}^T}\\ \Rightarrow {\boldsymbol{V}^T}\left( {\boldsymbol{V}{\boldsymbol{V}^T}} \right) = {\boldsymbol{V}^T}\\ \Rightarrow \boldsymbol{V}{\boldsymbol{V}^T} = \boldsymbol{I}\end{array}</script>

<p>Therefore, we see that <script type="math/tex">{\boldsymbol{V}^T} = {\boldsymbol{V}^{ - 1}}</script>. Hence, we can write the above factorization as <script type="math/tex">\boldsymbol{A = U}\Sigma {\boldsymbol{V}^T}</script>. Before diving into how to find these matrices, let us first describe what each of them represent:</p>

<ul>
  <li><script type="math/tex">\boldsymbol{A}</script> - This is the matrix we are trying to factor. <script type="math/tex">\boldsymbol{A}</script> can be any <script type="math/tex">m</script> by <script type="math/tex">n</script> matrix with rank <script type="math/tex">r</script>.</li>
  <li><script type="math/tex">\boldsymbol{V}</script> - This is an orthonormal matrix of size <script type="math/tex">n</script> by <script type="math/tex">n</script>. The first <script type="math/tex">r</script> columns represent an orthonormal basis for the row space of <script type="math/tex">\boldsymbol{A}</script>. To see why, consider rearranging the factorization as <script type="math/tex">\boldsymbol{A = U}\Sigma {\boldsymbol{V}^T} = \left( {\boldsymbol{U}\Sigma } \right){\boldsymbol{V}^T}</script>. From our interpretations of matrix multiplication, we can interpret the rows of the product <script type="math/tex">\left( {\boldsymbol{U}\Sigma } \right){\boldsymbol{V}^T}</script> as being linear combinations of the first <script type="math/tex">r</script>rows of <script type="math/tex">{\boldsymbol{V}^T}</script>. Note that only the first <script type="math/tex">r</script> rows of <script type="math/tex">{\boldsymbol{V}^T}</script> contribute to the final product since the last <script type="math/tex">n - r</script> rows of <script type="math/tex">\Sigma</script> are filled with zeros. Therefore, the rows of <script type="math/tex">\boldsymbol{A}</script> are linear combinations of the first <script type="math/tex">r</script>rows of <script type="math/tex">{\boldsymbol{V}^T}</script>, or equivalently, the first <script type="math/tex">r</script> columns of <script type="math/tex">\boldsymbol{V}</script>. Hence, the first <script type="math/tex">r</script> columns represent an orthonormal basis for the column space of <script type="math/tex">\boldsymbol{A}</script>. The last <script type="math/tex">n - r</script> columns of <script type="math/tex">\boldsymbol{V}</script> represent an orthonormal basis for the nullspace of <script type="math/tex">\boldsymbol{A}</script>. To see why, we notice that the last <script type="math/tex">n - r</script> columns of <script type="math/tex">\Sigma</script> are filled with zeros, which implies that <script type="math/tex">% <![CDATA[
\boldsymbol{A}\left[ {\begin{array}{*{20}{c}}{{\boldsymbol{v}_{r + 1}}}& \cdots &{{\boldsymbol{v}_n}}\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}\boldsymbol{0}& \cdots &\boldsymbol{0}\end{array}} \right] %]]></script>. Therefore, we see that <script type="math/tex">{\boldsymbol{v}_{r + 1}}, \ldots ,{\boldsymbol{v}_n}</script> are all in the nullspace of <script type="math/tex">\boldsymbol{A}</script>, and since they are orthonormal they form a basis for the nullspace.</li>
  <li><script type="math/tex">\boldsymbol{U}</script>- This is an orthonormal matrix of size <script type="math/tex">m</script> by <script type="math/tex">m</script>. The first <script type="math/tex">r</script> columns represent an orthonormal basis for the column space of <script type="math/tex">\boldsymbol{A}</script>. The last <script type="math/tex">m - r</script> columns of <script type="math/tex">\boldsymbol{U}</script> represent an orthonormal basis for the left nullspace of <script type="math/tex">\boldsymbol{A}</script>.</li>
  <li><script type="math/tex">\Sigma</script> - This is an <script type="math/tex">m</script> by <script type="math/tex">n</script> diagonal matrix where the first <script type="math/tex">r</script> diagonal entries of the matrix are nonzero and called singular values and the rest of the entries are 0.</li>
</ul>

<p>At this point, the question becomes: how do we find the SVD factorization for a matrix <script type="math/tex">\boldsymbol{A = U}\Sigma {\boldsymbol{V}^T}</script>? Well, we can start by assuming that the factorization exists. Then multiplying on the left by <script type="math/tex">{\boldsymbol{A}^T}</script> we get:</p>

<script type="math/tex; mode=display">{\boldsymbol{A}^T}\boldsymbol{A = }{\left( {\boldsymbol{U}\Sigma {\boldsymbol{V}^T}} \right)^T}\boldsymbol{U}\Sigma {\boldsymbol{V}^T} = \boldsymbol{V}{\Sigma ^T}\Sigma {\boldsymbol{V}^T}</script>

<p>Therefore, we see that <script type="math/tex">\boldsymbol{V}</script> must be the eigenvector matrix for <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script>, and <script type="math/tex">{\Sigma ^T}\Sigma</script> must be the eigenvalue matrix of <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script>. Given that <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> is a symmetric matrix, the spectral theorem tells us that there exists an orthonormal basis for its eigenvectors. Hence deriving <script type="math/tex">\boldsymbol{V}</script> from the eigenvectors of <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script> ensures that it is orthonormal.</p>

<p>To get <script type="math/tex">\Sigma</script>, we notice that <script type="math/tex">{\Sigma ^T}\Sigma</script> is the product of two diagonal matrices which will also be diagonal. Therefore, the entries along the diagonal of <script type="math/tex">\Sigma</script> are simply the square roots of the corresponding diagonal entries of <script type="math/tex">{\Sigma ^T}\Sigma</script> which are the eigenvalues of <script type="math/tex">{\boldsymbol{A}^T}\boldsymbol{A}</script>.</p>

<p>Finally, to get the matrix <script type="math/tex">\boldsymbol{U}</script> we can apply the same technique, but in the opposite order. Multiplying on the right by <script type="math/tex">{\boldsymbol{A}^T}</script> gives:</p>

<script type="math/tex; mode=display">\boldsymbol{A}{\boldsymbol{A}^T}\boldsymbol{ = U}\Sigma {\boldsymbol{V}^T}{\left( {\boldsymbol{U}\Sigma {\boldsymbol{V}^T}} \right)^T} = \boldsymbol{U}{\Sigma ^T}\Sigma {\boldsymbol{U}^T}</script>

<p>Therefore, we see that <script type="math/tex">\boldsymbol{U}</script> must be the eigenvector matrix for<script type="math/tex">\boldsymbol{A}{\boldsymbol{A}^T}</script>, and <script type="math/tex">{\Sigma ^T}\Sigma</script> must be the eigenvalue matrix of <script type="math/tex">\boldsymbol{A}{\boldsymbol{A}^T}</script>. Similar to above, as a product of the spectral theorem, we see that deriving <script type="math/tex">\boldsymbol{U}</script> from the eigenvectors of <script type="math/tex">\boldsymbol{A}{\boldsymbol{A}^T}</script> ensures that it is orthonormal.</p>

<p>Therefore, we have shown how to derive the SVD for a general matrix <script type="math/tex">\boldsymbol{A}</script>. The beauty of the SVD is that it selects the best basis (i.e. orthonormal) for the four subspaces associated with <script type="math/tex">\boldsymbol{A}</script>, and describes a scaling mapping between the row and column space basis. The SVD enables us to describe the action of any matrix as three steps: 1) rotation, 2) scaling, 3) rotation.</p>



  </div>

  <!-- Footer -->
  <footer>
    &copy; Copyright 2021 Chris Nielsen.
    
    
  </footer>

  <!-- Core JavaScript Files -->
  <script src="/assets/js/jquery.min.js" type="text/javascript"></script>
  <script src="/assets/js/popper.min.js" type="text/javascript"></script>
  <script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="/assets/js/mdb.min.js" type="text/javascript"></script>
  <script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D" crossorigin="anonymous"></script>
  <script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
  <script src="/assets/js/common.js"></script>

  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });
  </script>

  <!-- Code Syntax Highlighting -->
  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
  <script src="/assets/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  <!-- Script Used for Randomizing the Projects Order -->
  <!-- <script type="text/javascript">
    $.fn.shuffleChildren = function() {
      $.each(this.get(), function(index, el) {
        var $el = $(el);
        var $find = $el.children();

        $find.sort(function() {
          return 0.5 - Math.random();
        });

        $el.empty();
        $find.appendTo($el);
      });
    };
    $("#projects").shuffleChildren();
  </script> -->

  <!-- Project Cards Layout -->
  <script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });
  </script>

  <!-- Enable Tooltips -->
  <script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })
  </script>

  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');
  </script>
</body>
</html>
